diff --git CMakeLists.txt CMakeLists.txt
index 2f5b93e..fc8161d 100644
--- CMakeLists.txt
+++ CMakeLists.txt
@@ -25,6 +25,27 @@ target_link_libraries(
 
 set(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_DIR})
 
+file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/tmp)
+
+# add_executable(dram-test)
+# target_sources(
+#   dram-test
+#   PRIVATE test.cpp
+# )
+
+# target_link_libraries(
+#   dram-test
+#   PRIVATE llm_system
+# )
+
+# set_target_properties(
+#   dram-test
+#   PROPERTIES
+#   OUTPUT_NAME dram-test
+#   RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+# )
+
+
 configure_file(dram_config_HBM3_80GB.yaml ${EXECUTABLE_OUTPUT_PATH}/dram_config_HBM3_80GB.yaml COPYONLY)
 configure_file(dram_config_HBM3E_192GB.yaml ${EXECUTABLE_OUTPUT_PATH}/dram_config_HBM3E_192GB.yaml COPYONLY)
 configure_file(config.yaml ${EXECUTABLE_OUTPUT_PATH}/config.yaml COPYONLY)
\ No newline at end of file
diff --git config.yaml config.yaml
index 91eb69d..8463392 100644
--- config.yaml
+++ config.yaml
@@ -1,47 +1,37 @@
 model:
-  model_name: deepseekV3
+  model_name: mixtral
 
 system:
-  gpu_gen: B100 # e.g. A100, H100, B100, B200 (currently only support 4 generation)
-  nvlink_gen: 5
-  infiniband_gen: 800
+  gpu_gen: H100
   num_node: 1
-  num_device: 8
+  num_device: 4
   processor_type: GPU
+  offload:
+    collects_gpu_compute: on
+    offload_expert_to_cpu_memory: off
+    offload_expert_to_ssd: off
+    ssd_energy_scale: 1.0
+    offload_bandwidth: 5 # GPU <-> CPU-memory/SSD NVLink-C2C generation 4: NVLink4.0 and 5: NVLink5.0
   distribution:
     expert_tensor_degree: 1
-    none_expert_tensor_degree: 1
+    none_expert_tensor_degree: 4
   optimization:
-    parallel_execution: off
-    hetero_subbatch: off
-    disagg_system: off
-    use_low_unit_moe_only: off
-    use_ramulator: off
-    compressed_kv: on
-    use_absorb: on
-    use_flash_mla: on
-    use_flash_attention: on
-    reuse_kv_cache: on
-    kv_cache_reuse_rate: 0.0
+    compressed_kv: off
+    use_absorb: off
     prefill_mode: off
     decode_mode: on
 
 serving:
-  max_batch_size: 32
+  max_batch_size: 1
   max_process_token: 0
 
 simulation:
   data: synthesis
   input_len: 1024
-  output_len: 10
-  precision_byte: 1
-  iter: 5
+  output_len: 2
+  precision_byte: 2
+  iter: 2
   injection_rate: 0
-  exit_out_of_memory: off
-  mem_cap_limit: off # if memory usage exceed memory capacity, batchsize modifies
 
 log:
-  print_log: on
-  export_gantt: off
-  output_directory: ../data/
-  gantt_directory: ./gantt
\ No newline at end of file
+  output_directory: ./tmp
diff --git eval/CMakeLists.txt eval/CMakeLists.txt
index 0bf62ed..83a6835 100644
--- eval/CMakeLists.txt
+++ eval/CMakeLists.txt
@@ -1,10 +1,92 @@
+add_executable(latency-test latency.cpp)
+add_executable(sub-batch-test sub_batch_test.cpp)
+add_executable(latency-export export_latency_trace.cpp)
+add_executable(stat-export export_stat.cpp)
+add_executable(stat-export-hetero export_stat_hetero.cpp)
+add_executable(ablation_study ablation_study.cpp)
 add_executable(run test.cpp)
+add_executable(edap edap.cpp)
+add_executable(sum_gen_split sum_gen_split.cpp)
+
+target_link_libraries(
+    latency-test
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    sum_gen_split
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    sub-batch-test
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    latency-export
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    ablation_study
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    stat-export
+    PRIVATE llm_system
+)
+
+target_link_libraries(
+    stat-export-hetero
+    PRIVATE llm_system
+)
 
 target_link_libraries(
     run
     PRIVATE llm_system
 )
 
+target_link_libraries(
+    edap
+    PRIVATE llm_system
+)
+
+
+set_target_properties(latency-test PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+set_target_properties(sum_gen_split PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+
+set_target_properties(sub-batch-test PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+set_target_properties(latency-export PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+set_target_properties(ablation_study PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+set_target_properties(stat-export PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+set_target_properties(stat-export-hetero PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
 set_target_properties(run PROPERTIES
     RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
 )
+
+set_target_properties(edap PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
\ No newline at end of file
diff --git eval/ablation_study.cpp eval/ablation_study.cpp
new file mode 100644
index 0000000..64519b7
--- /dev/null
+++ eval/ablation_study.cpp
@@ -0,0 +1,163 @@
+#include <algorithm>
+#include <iostream>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  // 0: data_name,  1: processor_type, 2: device_num;
+
+  std::string model_name = arg[0];
+  std::string data_name = arg[1];
+  std::string processor_type = arg[2];
+  int num_device = std::stoi(arg[3]);
+  std::string output_path = arg[4];
+  // int max_batch_size = 128;
+  //  int iter = std::stoi(arg[6]);
+  int max_batch_size = std::stoi(arg[5]);
+  int input_len = std::stoi(arg[6]);
+  int output_len = std::stoi(arg[7]);
+  int max_process_token = std::stoi(arg[8]);
+  int request_per_second = std::stoi(arg[9]);
+  int iter = std::stoi(arg[10]);
+  int num_node = std::stoi(arg[11]);
+  int e_tp_dg = std::stoi(arg[12]);
+  int parallel_execution = std::stoi(arg[13]);
+  int hetero_subbatch = std::stoi(arg[14]); 
+
+
+  SystemConfig system_config;
+  system_config.num_node = num_node;
+  system_config.num_device = num_device;
+
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+  system_config.hetero_subbatch = false;
+  system_config.disagg_system = true;
+  system_config.use_ramulator = true;
+
+  int split = 0;
+  if (!system_config.disagg_system) {
+    split = 1;
+  }
+
+  if (!processor_type.compare("GPU")) {
+    system_config.processor_type = {ProcessorType::GPU};
+  } else if (!processor_type.compare("LOGIC")) {
+    system_config.processor_type = {ProcessorType::LOGIC};
+  } else if (!processor_type.compare("GPU+LOGIC")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+    system_config.logic_op_b = 16;
+    system_config.memory_capacity = 2 * system_config.memory_capacity;
+    system_config.num_logic_cube = 10;
+
+    if (parallel_execution) {
+      system_config.parallel_execution = true;
+    } else {
+      system_config.parallel_execution = false;
+    }
+    if (hetero_subbatch) {
+      system_config.parallel_execution = true;
+      system_config.hetero_subbatch = true;
+    }
+  } else if (!processor_type.compare("GPU+PIM")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::PIM};
+    system_config.parallel_execution = false;
+  } else {
+    fail("Not supported device type");
+  }
+
+  if (request_per_second == 0) {
+    system_config.use_inject_rate = false;
+  } else {
+    system_config.use_inject_rate = true;
+    system_config.request_per_second = request_per_second;
+  }
+
+  std::string expert_file_path;
+
+  ModelConfig model_config;
+
+  if (!model_name.compare("mixtral")) {
+    model_config = mixtral;
+  } else if (!model_name.compare("openMoE")) {
+    model_config = openMoE;
+  } else if (!model_name.compare("llama7bMoE")) {
+    model_config = llama7bMoE;
+  } else if (!model_name.compare("grok1")) {
+    model_config = grok1;
+  } else if (!model_name.compare("glam")) {
+    model_config = glam;
+  } else {
+    fail("Model is not supported");
+  }
+
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = e_tp_dg;
+  model_config.dataset = data_name;
+  // model_config.max_seq_len = 512;
+
+  if (!data_name.compare("synthesis")) {
+    expert_file_path = "none";
+    model_config.input_len = input_len;
+    model_config.output_len = output_len;
+  } else {
+    expert_file_path =
+        "../expert_data/experts_" + model_name + "_" + data_name + ".csv";
+  }
+
+  // long max_batch_size = 128;
+  if (max_process_token == 0) {
+    max_process_token = 8192 * 4;
+  }
+  Scheduler::Ptr scheduler =
+      Scheduler::Create(system_config, model_config, expert_file_path,
+                        max_batch_size, 8192 * 32, max_process_token);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  Model model(model_config, cluster, scheduler);
+
+  bool out_of_memory = cluster->checkMemorySize();
+  cluster->set_dependency();
+
+  // std::vector<Stat> stat_list;
+  int total_iter = iter;
+  
+  std::ofstream csv;
+  std::string file_name =
+      output_path + "/" + model_name + "_" + data_name + "_" +
+      std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+      processor_type + "_N" + std::to_string(num_node) + "_D" +
+      std::to_string(num_device) + "_maxbatch" +
+      std::to_string(max_batch_size) + "_maxprocess" +
+      std::to_string(max_process_token) + "_injectionrate" +
+      std::to_string(request_per_second) + "_iter" +
+      std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+      "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+      std::to_string(split) + "_hetero_subbatch" +
+      std::to_string(hetero_subbatch) + ".csv";
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  scheduler->getActualArrivalTime(total_iter);
+  cluster->runIteration(total_iter, file_name);
+
+  // TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  // top0->print_timeboard();
+  return 0;
+}
diff --git eval/edap.cpp eval/edap.cpp
new file mode 100644
index 0000000..56ed373
--- /dev/null
+++ eval/edap.cpp
@@ -0,0 +1,191 @@
+#include <algorithm>
+#include <iostream>
+
+#include "dram/dram_interface.h"
+#include "dram/memory_object.h"
+#include "dram/mmap_controller.h"
+#include "dram/power.h"
+#include "hardware/device.h"
+#include "hardware/stat.h"
+#include "module/tensor.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  std::string output_path = arg[0];
+  // std::string output_path = "test";
+
+  // Area overhead //
+  area_um2 dram_die = 106.92 * 1000.0 * 1000.0;
+  area_um2 logic_die = 106.92 * 1000.0 * 1000.0;
+  // area_um2 logic_die = 121.00 * 1000.0 * 1000.0;
+
+  double dram_scale = 46;
+
+  energy_nJ mac_energy = 0.45 / 1000;  // per operation
+  area_um2 fp16_mac_logic = 139.2536;
+  area_um2 fp16_mac_dram = 133.6257 * dram_scale;
+
+  double pitch = 22;  // um
+  area_um2 tsv_area = pitch * pitch * 4096 * 4;
+
+  area_um2 scratch_pad_logic = 702.706;
+  area_um2 scratch_pad_dram = 702.706 * dram_scale;
+
+  area_um2 controller_logic = 613.0;
+  area_um2 controller_dram = 613.0 * dram_scale;
+
+  area_um2 compute_area_per_pe_logic = fp16_mac_logic * 16;
+  area_um2 compute_area_per_pe_dram = fp16_mac_dram * 16;
+
+  int num_pseudo_channel = 32;
+  int num_rank = 2;
+  int num_bankgroup = 4;
+  int num_bank = 4;
+
+  // Area overhead end//
+
+  std::vector<energy_nJ> energy_list;
+  std::vector<time_ns> delay_list;
+  std::vector<area_um2> area_list;
+
+  SystemConfig config;
+  int m = 16384;
+  int k = 4096;
+
+  MemoryConfig memory_config(1);
+
+  Device::Ptr device = Device::Create(config, 0, nullptr);
+  device->mmap_controller = MMapController::Create(memory_config);
+  Tensor::Ptr tensor = Tensor::Create("dummy", {m, k}, "act", device, device->model_config.precision_byte);
+
+  std::vector<int> op_b = {1, 2, 4, 8, 16, 32};
+
+  DRAMInterface::Ptr interface = DRAMInterface::Create("./dram_config.yaml", 1);
+
+  // ALL-BANK PIM, X16 memory bandwidth //
+  energy_nJ energy = 0;
+  interface->setPIMHWConfig(ProcessorType::PIM, 16);
+
+  DRAMRequest::Ptr dram_request = DRAMRequest::Create(DRAMRequestType::kGEMV);
+  dram_request->AddOperand(tensor->getMemoryObject(), PIMOperandType::kSrc);
+
+  double start_time = interface->time;
+  std::list<DRAMRequest::Ptr> request;
+  request.push_back(dram_request);
+  interface->HandleRequest(request, 0);
+  ExecStatus exec_status = interface->getExecStatus();
+
+  energy += exec_status.act_count * pimEnergy.kACT_energy_j_;
+  energy += exec_status.read_count * pimEnergy.kREAD_energy_j_;
+  energy += exec_status.write_count * pimEnergy.kWRITE_energy_j_;
+  energy += exec_status.all_act_count * pimEnergy.kALL_ACT_energy_j_;
+  energy += exec_status.all_read_count * pimEnergy.kALL_READ_energy_j_;
+  energy += exec_status.all_write_count * pimEnergy.kALL_WRITE_energy_j_;
+  energy *= memory_config.num_channel;
+
+  time_ns time = exec_status.memory_duration;
+
+  for (int n : op_b) {
+    energy_list.push_back(energy + 1.0 * m * k * n * mac_energy);
+    delay_list.push_back(time * n);
+    area_list.push_back(
+        (dram_die +
+         ((compute_area_per_pe_dram + controller_dram + scratch_pad_dram * n) *
+          memory_config.num_bankgroup * memory_config.num_bank *
+          memory_config.num_channel / 4)) /
+        dram_die);
+  }
+
+  // ALL-BANKGroup PIM, X4 memory bandwidth, Op/b8 //
+  energy = 0;
+  interface->setPIMHWConfig(ProcessorType::PIM, 4);
+
+  dram_request = DRAMRequest::Create(DRAMRequestType::kGEMV);
+  dram_request->AddOperand(tensor->getMemoryObject(), PIMOperandType::kSrc);
+
+  start_time = interface->time;
+  request.resize(0);
+  request.push_back(dram_request);
+  interface->HandleRequest(request, 0);
+  exec_status = interface->getExecStatus();
+
+  energy += exec_status.act_count * pimBankgroupEnergy.kACT_energy_j_;
+  energy += exec_status.read_count * pimBankgroupEnergy.kREAD_energy_j_;
+  energy += exec_status.write_count * pimBankgroupEnergy.kWRITE_energy_j_;
+  energy += exec_status.all_act_count * pimBankgroupEnergy.kALL_ACT_energy_j_;
+  energy += exec_status.all_read_count * pimBankgroupEnergy.kALL_READ_energy_j_;
+  energy +=
+      exec_status.all_write_count * pimBankgroupEnergy.kALL_WRITE_energy_j_;
+  energy *= memory_config.num_channel;
+
+  time = exec_status.memory_duration;
+
+  for (int n : op_b) {
+    energy_list.push_back(energy + 1.0 * m * k * n * mac_energy);
+    delay_list.push_back(time * std::max(1, n / 8));
+    area_list.push_back((dram_die + (compute_area_per_pe_dram * 8 +
+                                     controller_dram + scratch_pad_dram * n) *
+                                        memory_config.num_bankgroup *
+                                        memory_config.num_channel / 4) /
+                        dram_die);
+  }
+
+  // LOGIC, X4 memory bandwidth //
+  energy = 0;
+  interface->setPIMHWConfig(ProcessorType::LOGIC, 4);
+
+  dram_request = DRAMRequest::Create(DRAMRequestType::kGEMV);
+  dram_request->AddOperand(tensor->getMemoryObject(), PIMOperandType::kSrc);
+
+  start_time = interface->time;
+  request.resize(0);
+  request.push_back(dram_request);
+  interface->HandleRequest(request, 0);
+  exec_status = interface->getExecStatus();
+
+  energy += exec_status.act_count * logicEnergy.kACT_energy_j_;
+  energy += exec_status.read_count * logicEnergy.kREAD_energy_j_;
+  energy += exec_status.write_count * logicEnergy.kWRITE_energy_j_;
+  energy += exec_status.all_act_count * logicEnergy.kALL_ACT_energy_j_;
+  energy += exec_status.all_read_count * logicEnergy.kALL_READ_energy_j_;
+  energy += exec_status.all_write_count * logicEnergy.kALL_WRITE_energy_j_;
+  energy *= memory_config.num_channel;
+
+  time = exec_status.memory_duration;
+
+  for (int n : op_b) {
+    energy_list.push_back(energy + 1.0 * m * k * n * mac_energy);
+    delay_list.push_back(time * std::max(1, n / 8));
+    area_list.push_back((logic_die + tsv_area +
+                         (compute_area_per_pe_logic * 8 + controller_logic +
+                          scratch_pad_logic * n) *
+                             4 * memory_config.num_channel) /
+                        logic_die);
+  }
+
+  std::ofstream csv;
+  std::string file_name = "unit_test.csv";
+
+  csv.open(output_path + "/" + file_name);
+  csv << "device,op_b,energy,delay,area" << std::endl;
+
+  std::vector<std::string> device_list;
+  device_list = {"PIM", "BG", "LOGIC"};
+  int i = 0;
+  for (auto device : device_list) {
+    for (auto opb : op_b) {
+      csv << device << "," << std::to_string(opb) << "," << energy_list[i]
+          << "," << delay_list[i] << "," << area_list[i] << std::endl;
+      i++;
+    }
+  }
+
+  csv.close();
+
+  return 0;
+}
diff --git eval/export_latency_trace.cpp eval/export_latency_trace.cpp
new file mode 100644
index 0000000..af0c0de
--- /dev/null
+++ eval/export_latency_trace.cpp
@@ -0,0 +1,106 @@
+#include <algorithm>
+#include <iostream>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  // 0: data_name,  1: processor_type, 2: device_num;
+
+  std::string model_name = arg[0];
+  std::string data_name = arg[1];
+  std::string processor_type = arg[2];
+  int num_device = std::stoi(arg[3]);
+  std::string output_path = arg[4];
+  int iter = std::stoi(arg[5]);
+
+  SystemConfig system_config;
+  system_config.num_node = 1;
+  system_config.num_device = num_device;
+
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+  system_config.communication_hiding = false;
+  system_config.hetero_subbatch = false;
+
+  system_config.disagg_system = true;
+
+  system_config.use_ramulator = true;
+
+  if (!processor_type.compare("GPU")) {
+    system_config.processor_type = {ProcessorType::GPU};
+  } else if (!processor_type.compare("LOGIC")) {
+    system_config.processor_type = {ProcessorType::LOGIC};
+  } else if (!processor_type.compare("GPU+LOGIC")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+    system_config.parallel_execution = true;
+  }
+
+  std::string expert_file_path =
+      "../expert_data/experts_" + model_name + "_" + data_name + ".csv";
+
+  ModelConfig model_config;
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = system_config.num_device;
+
+  model_config.num_layers = 32;
+  model_config.expert_freq = 4;
+
+  long max_batch_size = 128;
+
+  Scheduler::Ptr scheduler = Scheduler::Create(
+      system_config, model_config, expert_file_path, max_batch_size, 4096);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  Model model(model_config, cluster, scheduler);
+
+  cluster->checkMemorySize();
+  cluster->set_dependency();
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  std::vector<Stat> stat_list;
+  int total_iter = iter;
+  int p50 = total_iter / 2;
+  int p99 = total_iter * 0.99;
+  int p99dot9 = total_iter * 0.999;
+  stat_list = cluster->runIteration(total_iter);
+
+  TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  // TopModuleGraph::Ptr top1 = cluster->get_device(1)->top_module_graph;
+  // top0->print_timeboard();
+  // top1->print_timeboard();
+
+  std::ofstream csv;
+  std::string file_name = model_name + "_" + data_name + "_" + processor_type +
+                          std::to_string(num_device) + "_" +
+                          std::to_string(max_batch_size) + "_iter" +
+                          std::to_string(total_iter) + ".csv";
+  csv.open(output_path + "/" + file_name);
+  csv << "latency,mixed,batchsize,numtoken,seqlen" << std::endl;
+  for (auto temp : stat_list) {
+    csv << std::to_string(temp.latency) << ","
+        << std::to_string(temp.is_mixed) << ","
+        << std::to_string(temp.batchsize) << ","
+        << std::to_string(temp.process_token) << ","
+        << std::to_string(temp.average_seq_len) << std::endl;
+  }
+  csv.close();
+
+  return 0;
+}
diff --git eval/export_stat.cpp eval/export_stat.cpp
new file mode 100644
index 0000000..642c521
--- /dev/null
+++ eval/export_stat.cpp
@@ -0,0 +1,226 @@
+#include <algorithm>
+#include <iostream>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  // 0: data_name,  1: processor_type, 2: device_num;
+
+  std::string model_name = arg[0];
+  std::string data_name = arg[1];
+  std::string processor_type = arg[2];
+  int num_device = std::stoi(arg[3]);
+  std::string output_path = arg[4];
+  // int max_batch_size = 128;
+  //  int iter = std::stoi(arg[6]);
+  int max_batch_size = std::stoi(arg[5]);
+  int input_len = std::stoi(arg[6]);
+  int output_len = std::stoi(arg[7]);
+  int max_process_token = std::stoi(arg[8]);
+  int request_per_second = std::stoi(arg[9]);
+  int iter = std::stoi(arg[10]);
+  int num_node = std::stoi(arg[11]);
+  int e_tp_dg = std::stoi(arg[12]);
+  int parallel_execution = std::stoi(arg[13]);
+  int hetero_subbatch = std::stoi(arg[14]); 
+  int is_prefill = std::stoi(arg[15]); 
+
+
+  SystemConfig system_config;
+  system_config.num_node = num_node;
+  system_config.num_device = num_device;
+
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+  system_config.hetero_subbatch = false;
+  system_config.disagg_system = false;
+
+
+  system_config.use_ramulator = false;
+
+  system_config.use_flash_mla = false;
+  system_config.use_flash_attention = false;
+
+  system_config.reuse_kv_cache = false;
+  system_config.kv_cache_reuse_rate = 0.0;
+
+  if(is_prefill){
+    system_config.prefill_mode = true;
+    system_config.decode_mode = false;
+  }
+  else{
+    system_config.prefill_mode = false;
+    system_config.decode_mode = true;
+  }
+
+  assertTrue((system_config.prefill_mode == false) || (system_config.decode_mode == false), 
+            "prefill mode and decode mode is incompatible");
+  if(system_config.prefill_mode){
+    std::cout << "[Prefill Mode] Output Length is modified into 1" << std::endl;
+  }
+
+  if(system_config.decode_mode){
+    std::cout << "[Decode Mode] Current Length of sequences is modified into input_len" << std::endl;
+  }
+
+  int split = 0;
+  if (!system_config.disagg_system) {
+    split = 1;
+  }
+
+  if (!processor_type.compare("GPU")) {
+    system_config.processor_type = {ProcessorType::GPU};
+  } else if (!processor_type.compare("LOGIC")) {
+    system_config.processor_type = {ProcessorType::LOGIC};
+  } else if (!processor_type.compare("GPU+LOGIC")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+    if (parallel_execution) {
+      system_config.parallel_execution = true;
+    } else {
+      system_config.parallel_execution = false;
+    }
+    if (hetero_subbatch) {
+      system_config.parallel_execution = true;
+      system_config.hetero_subbatch = true;
+    }
+  } else if (!processor_type.compare("GPU+PIM")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::PIM};
+    system_config.parallel_execution = false;
+  } else {
+    fail("Not supported device type");
+  }
+
+  if (request_per_second == 0) {
+    system_config.use_inject_rate = false;
+  } else {
+    system_config.use_inject_rate = true;
+    system_config.request_per_second = request_per_second;
+  }
+
+  std::string expert_file_path;
+
+  ModelConfig model_config;
+
+  if (!model_name.compare("mixtral")) {
+    model_config = mixtral;
+  } else if (!model_name.compare("openMoE")) {
+    model_config = openMoE;
+  } else if (!model_name.compare("llama7bMoE")) {
+    model_config = llama7bMoE;
+  } else if (!model_name.compare("grok1")) {
+    model_config = grok1;
+  } else if (!model_name.compare("glam")) {
+    model_config = glam;
+  } else if (!model_name.compare("deepseekR1")) {
+    model_config = deepseekR1;
+  } else {
+    fail("Model is not supported");
+  }
+
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = e_tp_dg;
+  model_config.dataset = data_name;
+  if(system_config.prefill_mode){
+    model_config.use_absorb = false;
+    model_config.compressed_kv = true;
+    assertTrue(max_batch_size == 1, "For prefill mode, max batch size must be 1");
+  }
+  if(system_config.decode_mode){
+    model_config.use_absorb = true;
+    model_config.compressed_kv = true;
+  }
+  // model_config.max_seq_len = 512;
+
+  if (!data_name.compare("synthesis")) {
+    expert_file_path = "none";
+    model_config.input_len = input_len;
+    model_config.output_len = output_len;
+  } else {
+    expert_file_path =
+        "../expert_data/experts_" + model_name + "_" + data_name + ".csv";
+  }
+
+  // long max_batch_size = 128;
+  if (max_process_token == 0) {
+    max_process_token = 8192 * 4;
+  }
+  Scheduler::Ptr scheduler =
+      Scheduler::Create(system_config, model_config, expert_file_path,
+                        max_batch_size, 8192 * 32, max_process_token);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  Model model(model_config, cluster, scheduler);
+
+  bool out_of_memory = cluster->checkMemorySize();
+  cluster->set_dependency();
+
+  // std::vector<Stat> stat_list;
+  int total_iter = iter;
+  
+  std::ofstream csv;
+    std::string file_name;
+  if(system_config.prefill_mode){
+      file_name = output_path + "/" + model_name + "_" + data_name + "_" +
+      std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+      processor_type + "_N" + std::to_string(num_node) + "_D" +
+      std::to_string(num_device) + "_maxbatch" +
+      std::to_string(max_batch_size) + "_maxprocess" +
+      std::to_string(max_process_token) + "_injectionrate" +
+      std::to_string(request_per_second) + "_iter" +
+      std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+      "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+      std::to_string(split) + "_hetero_subbatch" +
+      std::to_string(hetero_subbatch) + "_prefill.csv";
+
+  }else if(system_config.decode_mode){
+    file_name = output_path + "/" + model_name + "_" + data_name + "_" +
+    std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+    processor_type + "_N" + std::to_string(num_node) + "_D" +
+    std::to_string(num_device) + "_maxbatch" +
+    std::to_string(max_batch_size) + "_maxprocess" +
+    std::to_string(max_process_token) + "_injectionrate" +
+    std::to_string(request_per_second) + "_iter" +
+    std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+    "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+    std::to_string(split) + "_hetero_subbatch" +
+    std::to_string(hetero_subbatch) + "_decode.csv";
+  }
+  else{
+    file_name = output_path + "/" + model_name + "_" + data_name + "_" +
+    std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+    processor_type + "_N" + std::to_string(num_node) + "_D" +
+    std::to_string(num_device) + "_maxbatch" +
+    std::to_string(max_batch_size) + "_maxprocess" +
+    std::to_string(max_process_token) + "_injectionrate" +
+    std::to_string(request_per_second) + "_iter" +
+    std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+    "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+    std::to_string(split) + "_hetero_subbatch" +
+    std::to_string(hetero_subbatch) + ".csv";
+  }
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  scheduler->getActualArrivalTime(total_iter);
+  cluster->runIteration(total_iter, file_name);
+
+  // TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  // top0->print_timeboard();
+  return 0;
+}
diff --git eval/export_stat_hetero.cpp eval/export_stat_hetero.cpp
new file mode 100644
index 0000000..e491063
--- /dev/null
+++ eval/export_stat_hetero.cpp
@@ -0,0 +1,159 @@
+#include <algorithm>
+#include <iostream>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  // 0: data_name,  1: processor_type, 2: device_num;
+
+  std::string model_name = arg[0];
+  std::string data_name = arg[1];
+  std::string processor_type = arg[2];
+  int num_device = std::stoi(arg[3]);
+  std::string output_path = arg[4];
+  // int max_batch_size = 128;
+  //  int iter = std::stoi(arg[6]);
+  int max_batch_size = std::stoi(arg[5]);
+  int input_len = std::stoi(arg[6]);
+  int output_len = std::stoi(arg[7]);
+  int max_process_token = std::stoi(arg[8]);
+  int request_per_second = std::stoi(arg[9]);
+  int iter = std::stoi(arg[10]);
+  int num_node = std::stoi(arg[11]);
+  int e_tp_dg = std::stoi(arg[12]);
+  int parallel_execution = std::stoi(arg[13]);
+  int hetero_subbatch = std::stoi(arg[14]); 
+
+
+  SystemConfig system_config;
+  system_config.num_node = num_node;
+  system_config.num_device = num_device;
+
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+  system_config.hetero_subbatch = false;
+  system_config.disagg_system = true;
+  system_config.use_ramulator = true;
+
+  int split = 0;
+  if (!system_config.disagg_system) {
+    split = 1;
+  }
+
+  if (!processor_type.compare("GPU")) {
+    system_config.processor_type = {ProcessorType::GPU};
+  } else if (!processor_type.compare("LOGIC")) {
+    system_config.processor_type = {ProcessorType::LOGIC};
+  } else if (!processor_type.compare("GPU+LOGIC")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+    if (parallel_execution) {
+      system_config.parallel_execution = true;
+    } else {
+      system_config.parallel_execution = false;
+    }
+    if (hetero_subbatch) {
+      system_config.parallel_execution = true;
+      system_config.hetero_subbatch = true;
+    }
+  } else if (!processor_type.compare("GPU+PIM")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::PIM};
+    system_config.parallel_execution = false;
+  } else {
+    fail("Not supported device type");
+  }
+
+  if (request_per_second == 0) {
+    system_config.use_inject_rate = false;
+  } else {
+    system_config.use_inject_rate = true;
+    system_config.request_per_second = request_per_second;
+  }
+
+  std::string expert_file_path;
+
+  ModelConfig model_config;
+
+  if (!model_name.compare("mixtral")) {
+    model_config = mixtral;
+  } else if (!model_name.compare("openMoE")) {
+    model_config = openMoE;
+  } else if (!model_name.compare("llama7bMoE")) {
+    model_config = llama7bMoE;
+  } else if (!model_name.compare("grok1")) {
+    model_config = grok1;
+  } else if (!model_name.compare("glam")) {
+    model_config = glam;
+  } else {
+    fail("Model is not supported");
+  }
+
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = e_tp_dg;
+  model_config.dataset = data_name;
+  // model_config.max_seq_len = 512;
+
+  if (!data_name.compare("synthesis")) {
+    expert_file_path = "none";
+    model_config.input_len = input_len;
+    model_config.output_len = output_len;
+  } else {
+    expert_file_path =
+        "../expert_data/experts_" + model_name + "_" + data_name + ".csv";
+  }
+
+  // long max_batch_size = 128;
+  if (max_process_token == 0) {
+    max_process_token = 8192 * 4;
+  }
+  Scheduler::Ptr scheduler =
+      Scheduler::Create(system_config, model_config, expert_file_path,
+                        max_batch_size, 8192 * 32, max_process_token);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  Model model(model_config, cluster, scheduler);
+
+  bool out_of_memory = cluster->checkHeteroMemorySize();
+  cluster->set_dependency();
+
+  // std::vector<Stat> stat_list;
+  int total_iter = iter;
+  
+  std::ofstream csv;
+  std::string file_name =
+      output_path + "/" + model_name + "_" + data_name + "_" +
+      std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+      processor_type + "_N" + std::to_string(num_node) + "_D" +
+      std::to_string(num_device) + "_maxbatch" +
+      std::to_string(max_batch_size) + "_maxprocess" +
+      std::to_string(max_process_token) + "_injectionrate" +
+      std::to_string(request_per_second) + "_iter" +
+      std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+      "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+      std::to_string(split) + "_hetero_subbatch" +
+      std::to_string(hetero_subbatch) + ".csv";
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  scheduler->getActualArrivalTime(total_iter);
+  cluster->runIteration(total_iter, file_name);
+
+  // TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  // top0->print_timeboard();
+  return 0;
+}
diff --git eval/latency.cpp eval/latency.cpp
new file mode 100644
index 0000000..4c0022b
--- /dev/null
+++ eval/latency.cpp
@@ -0,0 +1,91 @@
+#include <algorithm>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main() {
+  SystemConfig system_config;
+  system_config.num_node = 1;
+  system_config.num_device = 4;
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+
+  system_config.disagg_system = true;
+
+  // system_config.use_ramulator = false;
+  system_config.use_ramulator = true;
+
+  ModelConfig& model_config = mixtral;
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = system_config.num_device;
+  model_config.num_layers = 1;
+
+  // model_config.expert_freq = 1;
+  model_config.input_len = 2048;
+  model_config.output_len = 512;
+
+  long max_batch_size = 16;
+
+  Scheduler::Ptr scheduler = Scheduler::Create(
+      system_config, model_config, "none",
+      max_batch_size, 4096);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  scheduler->fillSequenceQueue();
+  scheduler->fillRunningQueue();
+  
+  Model model(model_config, cluster, scheduler);
+
+  cluster->set_dependency();
+
+  cluster->checkMemorySize();
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  std::vector<Stat> stat_list;
+  int total_iter = 2;
+  stat_list = cluster->runIteration(total_iter);
+
+  total_iter = stat_list.size();
+  int p50 = total_iter / 2;
+  int p99 = total_iter * 0.99;
+  int p99dot9 = total_iter * 0.999;
+
+  std::vector<time_ns> time;
+  for (auto& temp : stat_list) {
+    if (temp.iter_info) {
+      time.push_back(temp.latency);
+    }
+  }
+
+  sort(time.begin(), time.end());
+  // std::cout << "p50: " << time.at(p50) << ", p99: " << time.at(p99)
+  //           << ", p99.9: " << time.at(p99dot9) << std::endl;
+
+  TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  TopModuleGraph::Ptr top1 = cluster->get_device(1)->top_module_graph;
+  TopModuleGraph::Ptr top2 = cluster->get_device(2)->top_module_graph;
+  TopModuleGraph::Ptr top3 = cluster->get_device(3)->top_module_graph;
+
+  top0->print_timeboard();
+  top1->print_timeboard();
+  top2->print_timeboard();
+  top3->print_timeboard();
+
+  cluster->getTotalEnergy();
+  // top1->print_timeboard();
+
+  return 0;
+}
diff --git eval/sub_batch_test.cpp eval/sub_batch_test.cpp
new file mode 100644
index 0000000..6a8cba4
--- /dev/null
+++ eval/sub_batch_test.cpp
@@ -0,0 +1,258 @@
+#include <algorithm>
+
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+#define MAX(a, b) a > b ? a : b
+
+using namespace llm_system;
+
+int main() {
+  SystemConfig system_config;
+  system_config.num_node = 1;
+  system_config.num_device = 2;
+  system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+  system_config.hetero_subbatch = true;
+  system_config.use_ramulator = false;
+
+  ModelConfig model_config;
+  model_config.ne_tp_dg = 2;
+  model_config.e_tp_dg = 1;
+  model_config.num_layers = 2;
+  model_config.expert_freq = 1;
+
+  long max_batch_size = 32;
+
+  Scheduler::Ptr scheduler1 = Scheduler::Create(
+      system_config, model_config, "../expert_data/experts_mixtral_GSM.csv",
+      max_batch_size / 2, 4096);
+  Scheduler::Ptr scheduler2 = Scheduler::Create(
+      system_config, model_config, "../expert_data/experts_mixtral_GSM.csv",
+      max_batch_size / 2, 4096);
+
+  Cluster::Ptr cluster1 = Cluster::Create(system_config, scheduler1);
+  Cluster::Ptr cluster2 = Cluster::Create(system_config, scheduler2);
+
+  Model model1(model_config, cluster1, scheduler1);
+  Model model2(model_config, cluster2, scheduler2);
+
+  cluster1->checkMemorySize();
+  cluster2->checkMemorySize();
+  
+  cluster1->set_dependency();
+  cluster2->set_dependency();
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  int total_iter = 60;
+  int p50 = total_iter / 2;
+  int p99 = total_iter * 0.99;
+  int p99dot9 = total_iter * 0.999;
+  // time = cluster->runIteration(total_iter);
+  
+  scheduler1->fillSequenceQueue();
+  scheduler1->fillRunningQueue();
+
+  scheduler2->fillSequenceQueue();
+  scheduler2->fillRunningQueue();
+
+  // hitting
+  // scheduler1->hittingQueue(5000);
+  // scheduler2->hittingQueue(5000);
+  
+  time_ns total_time = 0;
+
+  std::vector<time_ns> time_list;
+
+  for (int i = 0; i < total_iter; i++) {
+    auto metadata1 = scheduler1->setMetadata();
+    auto metadata2 = scheduler2->setMetadata();
+    cluster1->run(metadata1);
+    cluster2->run(metadata2);
+
+    TimeBoard& timeboard1 =
+        cluster1->get_device(0)->top_module_graph->timeboard;
+    TimeBoard& timeboard2 =
+        cluster2->get_device(0)->top_module_graph->timeboard;
+
+    std::vector<TimeStamp*> QKV_gen_1; // GPU
+    std::vector<TimeStamp*> AttnSum_1; // GPU
+    std::vector<TimeStamp*> AttnGen_1; // PIM or Logic
+    std::vector<TimeStamp*> O_proj_1; // GPU
+    std::vector<TimeStamp*> FFN_1; // GPU
+    std::vector<TimeStamp*> ExpertFFN_1; // PIM or Logic
+
+    std::vector<TimeStamp*> QKV_gen_2; // GPU
+    std::vector<TimeStamp*> AttnSum_2; // GPU
+    std::vector<TimeStamp*> AttnGen_2; // PIM or Logic
+    std::vector<TimeStamp*> O_proj_2; // GPU
+    std::vector<TimeStamp*> FFN_2; // GPU
+    std::vector<TimeStamp*> ExpertFFN_2; // PIM or Logic
+
+    timeboard1.find_stamp("attn_qkv_proj", QKV_gen_1);
+    timeboard1.find_stamp("AttentionSum", AttnSum_1);
+    timeboard1.find_stamp("AttentionGen", AttnGen_1);
+    timeboard1.find_stamp("attn_o_proj", O_proj_1);
+    timeboard1.find_stamp("expertFFN", ExpertFFN_1);
+    timeboard1.find_stamp("feedforward", FFN_1);
+
+    timeboard2.find_stamp("attn_qkv_proj", QKV_gen_2);
+    timeboard2.find_stamp("AttentionSum", AttnSum_2);
+    timeboard2.find_stamp("AttentionGen", AttnGen_2);
+    timeboard2.find_stamp("attn_o_proj", O_proj_2);
+    timeboard2.find_stamp("expertFFN", ExpertFFN_2);
+    timeboard1.find_stamp("feedforward", FFN_2);
+
+    time_ns stage_time = 0;
+    int layer_idx_1 = 0;
+    int layer_idx_2 = 0;
+    
+    int cur_1 = 1;
+    int cur_2 = 0;
+
+    time_ns pipe_1;
+    time_ns pipe_2;
+
+    int num_layer = model_config.num_layers;
+    int expert_freq = model_config.expert_freq;
+    stage_time += QKV_gen_1[0]->get_duration() + AttnSum_1[0]->get_duration();
+    while(layer_idx_1 < num_layer || layer_idx_2 < num_layer){
+      // first layer must be MoE layer
+      // batch-1
+      pipe_1 = 0;
+      pipe_2 = 0;
+      if(layer_idx_1 % expert_freq == 0 && layer_idx_1 < num_layer){ // is MoE Layer
+        if(cur_1 == 0){
+          pipe_1 = QKV_gen_1[layer_idx_1]->get_duration() + AttnSum_1[layer_idx_1]->get_duration();
+        }
+        else if(cur_1 == 1){
+          pipe_1 = AttnGen_1[layer_idx_1]->get_duration();
+        }
+        else if(cur_1 == 2){
+          pipe_1 = O_proj_1[layer_idx_1]->get_duration();
+        }
+        else {
+          int expert_ffn_idx = layer_idx_1 / expert_freq;
+          pipe_1 = ExpertFFN_1[expert_ffn_idx]->get_duration();
+        }
+      }
+      else if (layer_idx_1 < num_layer){ // is Non-MoE layer
+        if(cur_1 == 0){
+          pipe_1 = QKV_gen_1[layer_idx_1]->get_duration() + AttnSum_1[layer_idx_1]->get_duration();
+        }
+        else if(cur_1 == 1){
+          pipe_1 = AttnGen_1[layer_idx_1]->get_duration();
+        }
+        else {
+          int ffn_idx = layer_idx_1 - (layer_idx_1 / expert_freq) - 1;
+          if(layer_idx_1 != num_layer - 1){
+            pipe_1 = O_proj_1[layer_idx_1]->get_duration()
+                    + FFN_1[ffn_idx]->get_duration()
+                    + QKV_gen_1[layer_idx_1 + 1]->get_duration() + AttnSum_1[layer_idx_1 + 1]->get_duration();
+          }
+          else{
+            pipe_1 = O_proj_1[layer_idx_1]->get_duration()
+                    + FFN_1[ffn_idx]->get_duration();
+          }
+          layer_idx_1++;
+          cur_1 = 0;
+        }
+      }
+      // batch-2
+      if(layer_idx_2 % expert_freq == 0){ // is MoE Layer
+        if(cur_2 == 0){
+          pipe_2 = QKV_gen_2[layer_idx_2]->get_duration() + AttnSum_2[layer_idx_2]->get_duration();
+        }
+        else if(cur_2 == 1){
+          pipe_2 = AttnGen_2[layer_idx_2]->get_duration();
+        }
+        else if(cur_2 == 2){
+          pipe_2 = O_proj_2[layer_idx_2]->get_duration();
+        }
+        else {
+          int expert_ffn_idx = layer_idx_2 / expert_freq;
+          pipe_2 = ExpertFFN_2[expert_ffn_idx]->get_duration();
+        }
+      }
+      else{ // is Non-MoE layer
+        if(cur_2 == 0){
+          pipe_2 = QKV_gen_2[layer_idx_2]->get_duration() + AttnSum_2[layer_idx_2]->get_duration();
+        }
+        else if(cur_2 == 1){
+          pipe_2 = AttnGen_2[layer_idx_2]->get_duration();
+        }
+        else {
+          int ffn_idx = layer_idx_2 - (layer_idx_2 / expert_freq) - 1;
+          if(layer_idx_2 != num_layer - 1){
+            pipe_2 = O_proj_2[layer_idx_2]->get_duration()
+                    + FFN_2[ffn_idx]->get_duration()
+                    + QKV_gen_2[layer_idx_2 + 1]->get_duration() + AttnSum_2[layer_idx_2 + 1]->get_duration();
+          }
+          else{
+            pipe_2 = O_proj_2[layer_idx_2]->get_duration()
+                    + FFN_2[ffn_idx]->get_duration();
+          }
+          layer_idx_2++;
+          cur_2 = 0;
+        }
+      }
+      stage_time += MAX(pipe_1, pipe_2);
+
+      cur_1++;
+      cur_2++;
+      if(cur_1 == 4){
+        layer_idx_1++;
+        cur_1 = 0;
+      }
+      if(cur_2 == 4){
+        layer_idx_2++;
+        cur_2 = 0;
+      }
+    }
+    time_list.push_back(stage_time);
+    total_time += stage_time;
+    std::cout << "Iter: " << std::to_string(i)
+              << "\t| time: " << std::setprecision(4) << stage_time / 1000 << "us\t|";
+
+    // check time
+    scheduler1->updateScheduler();
+    scheduler1->fillSequenceQueue();
+    scheduler1->fillRunningQueue();
+
+    scheduler2->updateScheduler();
+    scheduler2->fillSequenceQueue();
+    scheduler2->fillRunningQueue();
+
+    // if(i != total_iter - 1){
+    //   timeboard1.reset_timeboard();
+    //   timeboard2.reset_timeboard();
+    // }
+  } 
+  std::cout << "Total: " << std::to_string(total_time) << std::endl;
+  // batch
+  // auto metadata1 = scheduler1->setMetadata();
+  // cluster1->run(metadata1);
+  
+  // scheduler1->updateScheduler();
+  // scheduler1->fillSequenceQueue();
+  // scheduler1->fillRunningQueue();
+  
+  cluster1->get_device(0)->top_module_graph->print_timeboard();
+  cluster1->get_device(1)->top_module_graph->print_timeboard();
+  // cluster1->get_device(2)->top_module_graph->print_timeboard();
+  // cluster1->get_device(3)->top_module_graph->print_timeboard();
+
+  cluster2->get_device(0)->top_module_graph->print_timeboard();
+  cluster2->get_device(1)->top_module_graph->print_timeboard();
+  // cluster2->get_device(2)->top_module_graph->print_timeboard();
+  // cluster2->get_device(3)->top_module_graph->print_timeboard();
+
+  // sort(time_list.begin(), time_list.end());
+  // std::cout << "p50: " << time_list.at(p50) << ", p99: " << time_list.at(p99)
+  //           << ", p99.9: " << time_list.at(p99dot9) << std::endl;
+  return 0;
+}
+
diff --git eval/sum_gen_split.cpp eval/sum_gen_split.cpp
new file mode 100644
index 0000000..7e243cd
--- /dev/null
+++ eval/sum_gen_split.cpp
@@ -0,0 +1,144 @@
+#include <algorithm>
+#include <iostream>
+
+#include "hardware/stat.h"
+#include "model/model.h"
+#include "model/util.h"
+#include "module/layer.h"
+#include "module/module_graph.h"
+
+using namespace llm_system;
+
+int main(int argc, char *argv[]) {
+  std::vector<std::string> arg;
+  for (int i = 1; i < argc; i++) {
+    arg.push_back(argv[i]);
+  }
+  // 0: data_name,  1: processor_type, 2: device_num;
+
+  std::string model_name = arg[0];
+  std::string data_name = arg[1];
+  std::string processor_type = arg[2];
+  int num_device = std::stoi(arg[3]);
+  std::string output_path = arg[4];
+  int max_batch_size = std::stoi(arg[5]);
+  int input_len = std::stoi(arg[6]);
+  int output_len = std::stoi(arg[7]);
+  int max_process_token = std::stoi(arg[8]);
+  int request_per_second = std::stoi(arg[9]);
+  int iter = std::stoi(arg[10]);
+  int num_node = std::stoi(arg[11]);
+  int e_tp_dg = std::stoi(arg[12]);
+  int parallel_execution = std::stoi(arg[13]);
+
+  SystemConfig system_config;
+  system_config.num_node = num_node;
+  system_config.num_device = num_device;
+
+  // system_config.processor_type = {ProcessorType::GPU};
+  system_config.high_processor_type = ProcessorType::GPU;
+  system_config.low_processor_type = ProcessorType::LOGIC;
+
+  system_config.parallel_execution = false;
+  system_config.hetero_subbatch = false;
+  system_config.disagg_system = false;
+  system_config.use_ramulator = true;
+
+  if (!processor_type.compare("GPU")) {
+    system_config.processor_type = {ProcessorType::GPU};
+  } else if (!processor_type.compare("LOGIC")) {
+    system_config.processor_type = {ProcessorType::LOGIC};
+  } else if (!processor_type.compare("GPU+LOGIC")) {
+    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+    if (parallel_execution) {
+      system_config.parallel_execution = true;
+    } else {
+      system_config.parallel_execution = false;
+    }
+  }
+
+  if (request_per_second == 0) {
+    system_config.use_inject_rate = false;
+  } else {
+    system_config.use_inject_rate = true;
+    system_config.request_per_second = request_per_second;
+  }
+
+  int split = 0;
+  if (!system_config.disagg_system) {
+    split = 1;
+  }
+  std::string expert_file_path;
+
+  ModelConfig model_config;
+
+  if (!model_name.compare("mixtral")) {
+    model_config = mixtral;
+  } else if (!model_name.compare("openMoE")) {
+    model_config = openMoE;
+  } else if (!model_name.compare("llama7bMoE")) {
+    model_config = llama7bMoE;
+  } else if (!model_name.compare("grok1")) {
+    model_config = grok1;
+  } else if (!model_name.compare("glam")) {
+    model_config = glam;
+  } else {
+    fail("Model is not supported");
+  }
+
+  model_config.ne_tp_dg = system_config.num_device;
+  model_config.e_tp_dg = e_tp_dg;
+  model_config.dataset = data_name;
+  // model_config.max_seq_len = 512;
+
+  if (!data_name.compare("synthesis")) {
+    expert_file_path = "none";
+    model_config.input_len = input_len;
+    model_config.output_len = output_len;
+  } else {
+    expert_file_path =
+        "../expert_data/experts_" + model_name + "_" + data_name + ".csv";
+  }
+
+  // long max_batch_size = 128;
+  if (max_process_token == 0) {
+    max_process_token = 8192 * 4;
+  }
+  Scheduler::Ptr scheduler =
+      Scheduler::Create(system_config, model_config, expert_file_path,
+                        max_batch_size, 8192 * 32, max_process_token);
+
+  Cluster::Ptr cluster = Cluster::Create(system_config, scheduler);
+
+  Model model(model_config, cluster, scheduler);
+
+  bool out_of_memory = cluster->checkMemorySize();
+  cluster->set_dependency();
+
+  // std::vector<Stat> stat_list;
+  int total_iter = iter;
+
+  std::ofstream csv;
+  std::string file_name =
+      output_path + "/" + model_name + "_" + data_name + "_" +
+      std::to_string(input_len) + "_" + std::to_string(output_len) + "_" +
+      processor_type + "_N" + std::to_string(num_node) + "_D" +
+      std::to_string(num_device) + "_maxbatch" +
+      std::to_string(max_batch_size) + "_maxprocess" +
+      std::to_string(max_process_token) + "_injectionrate" +
+      std::to_string(request_per_second) + "_iter" +
+      std::to_string(total_iter) + "_e_tp" + std::to_string(e_tp_dg) +
+      "_parallel_execution" + std::to_string(parallel_execution) + "_split" +
+      std::to_string(split) + ".csv";
+
+  std::cout << "-----------------------------------" << std::endl;
+  std::cout << "-------------start-----------------" << std::endl;
+  std::cout << "-----------------------------------" << std::endl;
+
+  scheduler->getActualArrivalTime(total_iter);
+  cluster->runIteration(total_iter, file_name);
+
+  // TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  // top0->print_timeboard();
+  return 0;
+}
diff --git eval/test.cpp eval/test.cpp
index 0e25244..8053391 100644
--- eval/test.cpp
+++ eval/test.cpp
@@ -55,68 +55,27 @@ int main(int argc, char *argv[]) {
     fail("No GPU generation information");
   }
 
-  // NVLink Config // 
-  if(config["system"]["nvlink_gen"].as<int>() == 4){
-    system_config.device_ict_bandwidth = 450.0 * 1000 * 1000 * 1000; // B/s NVLink 4th Gen (H100)
-    system_config.device_ict_latency = 0.8 * 1000; // ns
-  }
-  else if(config["system"]["nvlink_gen"].as<int>() == 5){
-    system_config.device_ict_bandwidth = 900.0 * 1000 * 1000 * 1000; // B/s NVLink 5th Gen (B100, B200)
-    system_config.device_ict_latency = 0.8 * 1000; // ns
-  }else{
-    fail("Not support NVLink generation");
-  }
+  // inter-GPU NVLink Config // 
+  system_config.device_ict_bandwidth = 450.0 * 1000 * 1000 * 1000; // B/s NVLink 4th Gen (DGX H100)
+  system_config.device_ict_latency = 0.8 * 1000; // ns
 
   // InfiniBand Config // 
-  if(config["system"]["infiniband_gen"].as<int>() == 400){
-    system_config.node_ict_bandwidth = 50.0 * 1000 * 1000 * 1000; // B/s Infiniband NDR
-    system_config.node_ict_latency = 0.13 * 1000; // ns
-  }
-  else if(config["system"]["infiniband_gen"].as<int>() == 800){
-    system_config.node_ict_bandwidth = 100.0 * 1000 * 1000 * 1000; // B/s InfiniBand XDR
-    system_config.node_ict_latency = 0.13 * 1000; // ns
-  }
-  else if(config["system"]["infiniband_gen"].as<int>() == 3600){
-    system_config.node_ict_bandwidth = 450.0 * 1000 * 1000 * 1000; // B/s NVLink 4th Gen
-    system_config.node_ict_latency = 0.8 * 1000; // ns
-  }
-  else if(config["system"]["infiniband_gen"].as<int>() == 7200){
-    system_config.node_ict_bandwidth = 900.0 * 1000 * 1000 * 1000; // B/s NVLink 5th Gen
-    system_config.node_ict_latency = 0.8 * 1000; // ns
-  }
-  else{
-    fail("Not support InfiniBand generation");
-  }
-  
+  system_config.node_ict_bandwidth = 50.0 * 1000 * 1000 * 1000; // B/s Infiniband NDR
+  system_config.node_ict_latency = 0.13 * 1000; // ns
+
   system_config.num_node = num_node;
   system_config.num_device = num_device;
 
-
-  system_config.high_processor_type = ProcessorType::GPU;
-  system_config.low_processor_type = ProcessorType::LOGIC;
-
-
-  system_config.parallel_execution =
-      config["system"]["optimization"]["parallel_execution"].as<bool>();
-  system_config.hetero_subbatch =
-      config["system"]["optimization"]["hetero_subbatch"].as<bool>();
-  system_config.disagg_system =
-      config["system"]["optimization"]["disagg_system"].as<bool>(); 
-  system_config.use_low_unit_moe_only =
-      config["system"]["optimization"]["use_low_unit_moe_only"].as<bool>();      
-  system_config.use_ramulator =
-      config["system"]["optimization"]["use_ramulator"].as<bool>();
-
-  system_config.use_flash_mla =
-      config["system"]["optimization"]["use_flash_mla"].as<bool>();
-  system_config.use_flash_attention =
-      config["system"]["optimization"]["use_flash_attention"].as<bool>();
-
-  // kv cache reuse
-  system_config.reuse_kv_cache =
-      config["system"]["optimization"]["reuse_kv_cache"].as<bool>();
-  system_config.kv_cache_reuse_rate =
-      config["system"]["optimization"]["kv_cache_reuse_rate"].as<double>();
+  // default settings
+  system_config.parallel_execution = false;
+  system_config.hetero_subbatch = false;
+  system_config.disagg_system = false;
+  system_config.use_low_unit_moe_only = false;
+  system_config.use_ramulator = false;
+  system_config.use_flash_mla = false;
+  system_config.use_flash_attention = false;
+  system_config.reuse_kv_cache = false;
+  system_config.kv_cache_reuse_rate = 0.0;
   
   // prefill mode or decode mode
   system_config.prefill_mode =
@@ -137,24 +96,24 @@ int main(int argc, char *argv[]) {
     std::cout << "[Decode Mode] Current Length of sequences is modified into input_len" << std::endl;
   }
 
+  //// In this branch, we use only GPU-related code, neither LOGIC nor PIM
   if (!processor_type.compare("GPU")) {
     system_config.processor_type = {ProcessorType::GPU};
     system_config.high_processor_type = ProcessorType::GPU;
     system_config.low_processor_type = ProcessorType::GPU;
-  } else if (!processor_type.compare("LOGIC")) {
-    system_config.processor_type = {ProcessorType::LOGIC};
-    system_config.high_processor_type = ProcessorType::LOGIC;
-    system_config.low_processor_type = ProcessorType::LOGIC;
-  } else if (!processor_type.compare("GPU+LOGIC")) {
-    system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
-    system_config.high_processor_type = ProcessorType::GPU;
-    system_config.low_processor_type = ProcessorType::LOGIC;
-    // system_config.parallel_execution = true;
-  } else if (!processor_type.compare("GPU+PIM")) {
-    system_config.processor_type = {ProcessorType::GPU, ProcessorType::PIM};
-    system_config.high_processor_type = ProcessorType::GPU;
-    system_config.low_processor_type = ProcessorType::PIM;
-  }
+  } //else if (!processor_type.compare("LOGIC")) {
+  //  system_config.processor_type = {ProcessorType::LOGIC};
+  //  system_config.high_processor_type = ProcessorType::LOGIC;
+  //  system_config.low_processor_type = ProcessorType::LOGIC;
+  //} else if (!processor_type.compare("GPU+LOGIC")) {
+  //  system_config.processor_type = {ProcessorType::GPU, ProcessorType::LOGIC};
+  //  system_config.high_processor_type = ProcessorType::GPU;
+  //  system_config.low_processor_type = ProcessorType::LOGIC;
+  //} else if (!processor_type.compare("GPU+PIM")) {
+  //  system_config.processor_type = {ProcessorType::GPU, ProcessorType::PIM};
+  //  system_config.high_processor_type = ProcessorType::GPU;
+  //  system_config.low_processor_type = ProcessorType::PIM;
+  //}
 
   std::string expert_file_path;
 
@@ -166,12 +125,16 @@ int main(int argc, char *argv[]) {
     model_config = openMoE;
   } else if (!model_name.compare("llama7bMoE")) {
     model_config = llama7bMoE;
+  } else if (!model_name.compare("llama3_70B")) {
+    model_config = llama3_70B;
   } else if (!model_name.compare("llama3_405B")) {
     model_config = llama3_405B;
+  } else if (!model_name.compare("llama4_maverick")) {
+    model_config = llama4_maverick;
   } else if (!model_name.compare("grok1")) {
     model_config = grok1;
-  } else if (!model_name.compare("deepseekV3")) {
-    model_config = deepseekV3;
+  } else if (!model_name.compare("deepseekR1")) {
+    model_config = deepseekR1;
   } else {
     fail("No model configuration of " + model_name);
   }
@@ -191,8 +154,8 @@ int main(int argc, char *argv[]) {
     system_config.compute_peak_flops *= 2; // system_config has FP16 peak FLOPS information
   }
 
-  system_config.exit_out_of_memory = config["simulation"]["exit_out_of_memory"].as<bool>();
-  system_config.mem_cap_limit = config["simulation"]["mem_cap_limit"].as<bool>();
+  system_config.exit_out_of_memory = false;
+  system_config.mem_cap_limit = false;
 
   model_config.dataset = data_name;
 
@@ -222,7 +185,6 @@ int main(int argc, char *argv[]) {
 
   Model model(model_config, cluster, scheduler);
 
-  bool out_of_memory = cluster->checkMemorySize();
   cluster->set_dependency();
 
   std::cout << "-----------------------------------" << std::endl;
@@ -237,109 +199,39 @@ int main(int argc, char *argv[]) {
 
   int precision_bytes = model_config.precision_byte;
 
-  std::string file_name;
-  if(system_config.prefill_mode){
-    if(system_config.use_ramulator){
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + "_ramul_prefill.csv";
-    }
-    else{
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + "_prefill.csv";
-    }
+  std::string offloading;
+  if(config["system"]["offload"]["offload_expert_to_cpu_memory"].as<bool>()){
+    offloading="CPU_memory";
+  } else if(config["system"]["offload"]["offload_expert_to_ssd"].as<bool>()){
+    offloading="SSD";
+  } else{
+    offloading="baseline"; // no offload
   }
-  else if(system_config.decode_mode){
-    if(system_config.use_ramulator){
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + "_ramul_decode.csv";
-    }
-    else{
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + "_decode.csv";
-    }
-  }
-  else{
-    if(system_config.use_ramulator){
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + "_ramul.csv";
-    }
-    else{
-      file_name = output_path + "/" + model_name + "_" + data_name +
-                            "_" + std::to_string(input_len) + "_" +
-                            std::to_string(output_len) + "_" + processor_type +
-                            "_N" + std::to_string(num_node) + "_D" + 
-                            std::to_string(num_device) + "_TP" +
-                            std::to_string(ne_tp_dg) + "_DP" +
-                            std::to_string(ne_dp_dg) + "_maxbatch" +
-                            std::to_string(max_batch_size) + "_maxprocess" +
-                            std::to_string(max_process_token) + "_iter" +
-                            std::to_string(total_iter) + 
-                            "_precision_byte" + std::to_string(precision_bytes) + "_parallel_execution" + std::to_string(system_config.parallel_execution) + ".csv";
-    }
-  }
-  if (out_of_memory && config["simulation"]["exit_out_of_memory"].as<bool>()) {
-    std::cout << "Out of Memory: " << file_name << std::endl;
-    return 0;
+
+  std::string collect;
+  if(config["system"]["offload"]["collects_gpu_compute"].as<bool>()){
+    collect="GPU_compute";
+  } else{
+    collect="offloaded_weight_copy";
   }
+
+  int nvlink_gen = config["system"]["offload"]["offload_bandwidth"].as<int>();
+
+  std::string file_name;
+  file_name = output_path + "/" + model_name + "_" + data_name +
+                        "_N" + std::to_string(num_node) + "_D" + 
+                        std::to_string(num_device) + "_TP" +
+                        std::to_string(ne_tp_dg) + "_DP" +
+                        std::to_string(ne_dp_dg) + "_batchsize" +
+                        std::to_string(max_batch_size) + "_" + offloading + 
+                        "_NVLink" + std::to_string(nvlink_gen) + "_" + collect + ".csv";
+
+  
   scheduler->getActualArrivalTime(total_iter);
   stat_list = cluster->runIteration(total_iter, file_name);
-  // TopModuleGraph::Ptr top1 = cluster->get_device(8)->top_module_graph;
-  std::string gantt_file_path =
-      config["log"]["gantt_directory"].as<std::string>();
-
-  if(config["log"]["export_gantt"].as<bool>()){
-    cluster->exportGantt(gantt_file_path);
-  }
 
-  if(config["log"]["print_log"].as<bool>()){
-    TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
-    top0->print_timeboard();
-  }
+  //TopModuleGraph::Ptr top0 = cluster->get_device(0)->top_module_graph;
+  //top0->print_timeboard();
 
   return 0;
 }
diff --git figure_4_and_5.py figure_4_and_5.py
new file mode 100755
index 0000000..d95acda
--- /dev/null
+++ figure_4_and_5.py
@@ -0,0 +1,304 @@
+#!/usr/bin/env python3
+
+import os
+import sys
+import subprocess
+import yaml
+import pandas as pd
+
+BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
+CONFIG_FILE  = os.path.join(BASE_DIR, "build", "config.yaml")
+SIM_CMD      = [os.path.join(BASE_DIR, "build", "run")]
+
+# latency
+LATENCY_NON_MOE_DECODER   = "non_MoE_decoders"
+LATENCY_FC_COLS = [
+    "qkvgen", "q_down_proj", "kv_down_proj", "kr_proj",
+    "q_up_proj", "qr_proj", "kv_up_proj", "tr_k_up_proj",
+    "v_up_proj", "o_proj", "ffn", "shared_expert_ffn"
+]
+LATENCY_ATTEN = "atten_gen"
+LATENCY_MOE   = "expert_ffn"
+LATENCY_COMM  = "communication"
+
+# energy
+BASE = {
+    "fc_dram":         "fc_dram",
+    "fc_comp":         "fc_comp",
+    "ffn_dram":        "ffn_dram",
+    "ffn_comp":        "ffn_comp",
+    "shared_moe_hbm": "shared_moe_hbm",
+    "shared_moe_comp": "shared_moe_comp",
+    "attn_dram":       "attn_dram",
+    "attn_comp":       "attn_comp",
+    "moe_hbm":        "moe_hbm",
+    "moe_comp":        "moe_comp",
+}
+OFFLOAD = "moe_offload"
+
+# model & batch size
+EXPERIMENTS = {
+    "mixtral":    [1, 4, 16, 64, 256, 1024],
+    "deepseekR1": [4, 16, 64, 256, 1024],
+}
+
+# system background power (Watt)
+## 70: single H100 SXM 80GB GPU (includes HBM)
+## 57: 256GB of CPU memory (DDR5-7200 x8) per GPU
+## ignore the negligible background power of SSD (https://semiconductor.samsung.com/consumer-storage/internal-ssd/9100-pro/)
+BACKGROUND_POWER = 57 + 70
+
+DISCARD_COLS = [
+    "non_MoE_decoders", "MoE_decoders", "qkvgen", "q_down_proj", "kv_down_proj", "kr_proj",
+    "q_up_proj", "qr_proj", "kv_up_proj", "tr_k_up_proj",
+    "v_up_proj", "atten_sum", "atten_gen", "o_proj",
+    "ffn", "shared_expert_ffn", "communication", "fc_dram",
+    "fc_comp", "attn_dram", "attn_comp", "ffn_dram", "ffn_comp",
+    "shared_moe_hbm", "shared_moe_comp", "moe_hbm", "moe_comp"
+]
+
+def load_cfg():
+    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
+        return yaml.safe_load(f)
+
+def write_cfg(cfg):
+    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
+        yaml.dump(cfg, f, sort_keys=False, allow_unicode=True)
+
+def update_configuration(model_name: str, batch_size: int, input_sequence_length: int):
+    cfg = load_cfg()
+    cfg['model']['model_name'] = model_name
+    cfg['serving']['max_batch_size'] = batch_size
+    cfg['simulation']['input_len'] = input_sequence_length
+    cfg['simulation']['output_len'] = 2 # the first decode stage after the prefill stage
+    cfg['simulation']['iter'] = 2
+
+    if model_name == "mixtral":
+        cfg['system']['num_node'] = 1
+        cfg['system']['num_device'] = 4
+        cfg['system']['distribution']['expert_tensor_degree'] = 1
+        cfg['system']['distribution']['none_expert_tensor_degree'] = 4
+        cfg['system']['optimization']['compressed_kv'] = False
+        cfg['system']['optimization']['use_absorb'] = False
+    elif model_name == "deepseekR1":
+        cfg['system']['num_node'] = 4
+        cfg['system']['num_device'] = 8
+        cfg['system']['distribution']['expert_tensor_degree'] = 1
+        cfg['system']['distribution']['none_expert_tensor_degree'] = 8
+        cfg['system']['optimization']['compressed_kv'] = True
+        cfg['system']['optimization']['use_absorb'] = True
+
+    cfg['system']['offload']['ssd_energy_scale'] = 1.0
+    cfg['system']['offload']['offload_expert_to_cpu_memory'] = False
+    cfg['system']['offload']['offload_expert_to_ssd']       = False
+    write_cfg(cfg)
+
+
+def run_sim(target_gpu: bool, offload_cpu: bool, offload_ssd: bool, nvlink_gen: int = 4) -> str:
+    cfg = load_cfg()
+    cfg['system']['offload']['offload_expert_to_cpu_memory'] = offload_cpu
+    cfg['system']['offload']['offload_expert_to_ssd']       = offload_ssd
+    cfg['system']['offload']['collects_gpu_compute']       = target_gpu
+    cfg['system']['offload']['offload_bandwidth']       = nvlink_gen
+    write_cfg(cfg)
+
+    tag = "CPU_memory" if offload_cpu else "SSD" if offload_ssd else "baseline"
+    collect = "GPU_compute" if target_gpu else "offloaded_weight_copy"
+    print(f"\n--- Running [{tag}] ---")
+    simulation_working_dir = "./build"
+    proc = subprocess.run(SIM_CMD, cwd=simulation_working_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
+    if proc.returncode != 0:
+        print(proc.stdout, file=sys.stdout)
+        print(proc.stderr, file=sys.stderr)
+        sys.exit(f"ERROR: failed ({tag})")
+
+    # file name
+    m   = cfg['model']['model_name']
+    d   = cfg['simulation']['data']
+    nn  = cfg['system']['num_node']
+    nd  = cfg['system']['num_device']
+    ne_tp_dg = cfg["system"]["distribution"]["none_expert_tensor_degree"]
+    ne_dp_dg = int(nd * nn / ne_tp_dg)
+    bs  = cfg['serving']['max_batch_size']
+    od  = cfg['log']['output_directory']
+    fname = os.path.join(simulation_working_dir, od, f"{m}_{d}_N{nn}_D{nd}_TP{ne_tp_dg}_DP{ne_dp_dg}_batchsize{bs}_{tag}_NVLink{nvlink_gen}_{collect}.csv")
+    if not os.path.isfile(fname):
+        sys.exit(f"ERROR: cannot find generated CSV: {fname}")
+    print(f"→ OK: {fname}")
+
+    if not target_gpu:
+        zero_out_columns_in_csv(fname)
+    
+    return fname
+
+def zero_out_columns_in_csv(fname: str):
+    df = pd.read_csv(fname)
+    for col in DISCARD_COLS:
+        if col in df.columns:
+            df[col] = 0
+    df.to_csv(fname, index=False)
+
+def compute_energy_baseline(label: str, latency: float, df_base: pd.DataFrame, bs: int):
+    cfg = load_cfg()
+    num_tot_device = cfg['system']['num_node'] * cfg['system']['num_device']
+
+    r = df_base.loc[0]
+    # xx_off: (memory/storage access) off-chip memory access
+    # xx_on: (GPU-compute) on-chip memory & compute
+    fc_off   = (r[BASE["fc_dram"]] + r[BASE["ffn_dram"]] + r[BASE["shared_moe_hbm"]]) / 1e9
+    fc_on    = (r[BASE["fc_comp"]] + r[BASE["ffn_comp"]] + r[BASE["shared_moe_comp"]]) / 1e9
+    attn_off = r[BASE["attn_dram"]] / 1e9
+    attn_on  = r[BASE["attn_comp"]] / 1e9
+    moe_off  = r[BASE["moe_hbm"]] / 1e9
+    moe_on   = r[BASE["moe_comp"]] / 1e9
+    background   = num_tot_device * BACKGROUND_POWER * latency / 1e3
+    per_tok  = (fc_off + fc_on + attn_off + attn_on + moe_off + moe_on + background) / bs
+
+    print(f"\n>> {label} energy (J/single decode stage)")
+    print(f"   FC off-chip      : {fc_off:.6f}")
+    print(f"   FC on-chip       : {fc_on:.6f}")
+    print(f"   Attn off-chip    : {attn_off:.6f}")
+    print(f"   Attn on-chip     : {attn_on:.6f}")
+    print(f"   MoE off-chip     : {moe_off:.6f}")
+    print(f"   MoE on-chip      : {moe_on:.6f}")
+    print(f"   System background: {background:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token        : {per_tok:.6f} (÷ batch_size={bs})")
+
+def compute_energy_offload(label: str, latency: float, df_gpu: pd.DataFrame, df_offload: pd.DataFrame, bs: int):
+    cfg = load_cfg()
+    num_tot_device = cfg['system']['num_node'] * cfg['system']['num_device']
+
+    r, s = df_gpu.loc[0], df_offload.loc[0]
+    # xx_off: (memory/storage access) off-chip memory access
+    # xx_on: (GPU-compute) on-chip memory & compute
+    fc_off   = (r[BASE["fc_dram"]] + r[BASE["ffn_dram"]] + r[BASE["shared_moe_hbm"]]) / 1e9
+    fc_on    = (r[BASE["fc_comp"]] + r[BASE["ffn_comp"]] + r[BASE["shared_moe_comp"]]) / 1e9
+    attn_off = r[BASE["attn_dram"]] / 1e9
+    attn_on  = r[BASE["attn_comp"]] / 1e9
+    moe_off  = (r[BASE["moe_hbm"]] + s[OFFLOAD]) / 1e9
+    moe_on   = r[BASE["moe_comp"]] / 1e9
+    # system background energy
+    background   = num_tot_device * BACKGROUND_POWER * latency / 1e3
+    per_tok  = (fc_off + fc_on + attn_off + attn_on + moe_off + moe_on + background) / bs
+
+    print(f"\n>> {label}-offloaded (nvlink5.0) case energy (J/single decode stage)")
+    print(f"   FC off-chip      : {fc_off:.6f}")
+    print(f"   FC on-chip       : {fc_on:.6f}")
+    print(f"   Attn off-chip    : {attn_off:.6f}")
+    print(f"   Attn on-chip     : {attn_on:.6f}")
+    print(f"   MoE off-chip     : {moe_off:.6f}")
+    print(f"   MoE on-chip      : {moe_on:.6f}")
+    print(f"   System background: {background:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token        : {per_tok:.6f} (÷ batch_size={bs})")
+
+
+def compute_latency_baseline(label: str, df_base: pd.DataFrame) -> float:
+    r = df_base.loc[0]
+    # FC
+    fc_sum = sum(r[col] for col in LATENCY_FC_COLS)
+    fc_lat = fc_sum / 1e6
+    # Attention
+    attn_lat = r[LATENCY_ATTEN] / 1e6
+    # MoE
+    moe_lat  = r[LATENCY_MOE] / 1e6
+    # Communication
+    comm_lat = r[LATENCY_COMM] / 1e6
+    per_tok = fc_lat + attn_lat + moe_lat + comm_lat
+
+    print(f"\n>> {label}-latency (ms/token)")
+    print(f"   FC            : {fc_lat:.6f}")
+    print(f"   Attention     : {attn_lat:.6f}")
+    print(f"   MoE           : {moe_lat:.6f}")
+    print(f"   Communication : {comm_lat:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token     : {per_tok:.6f}")
+
+    return per_tok
+
+def compute_latency_offload(label: str, nvlink_gen: str, df_base: pd.DataFrame, df_cpu: pd.DataFrame, bs: int) -> float:
+    r, s = df_base.loc[0], df_cpu.loc[0]
+    # FC
+    fc_sum = sum(r[col] for col in LATENCY_FC_COLS)
+    fc_lat = fc_sum / 1e6
+    # Attention
+    attn_lat = r[LATENCY_ATTEN] / 1e6
+    # MoE
+    if bs == 1024 and nvlink_gen == "nvlink5.0":
+        # For this batch size and NVLink generation, compute latency exceeds gated-expert weight load latency for both Mixtral and DeepSeek-R1
+        moe_lat  = r[LATENCY_MOE] / 1e6
+    else:
+        # For others, gated-expert weight load latency exceeds compute latency
+        moe_lat  = (s[LATENCY_MOE] - (fc_sum + r[LATENCY_ATTEN] + r[LATENCY_COMM] - r[LATENCY_NON_MOE_DECODER])) / 1e6
+    # Communication
+    comm_lat = r[LATENCY_COMM] / 1e6
+    per_tok = fc_lat + attn_lat + moe_lat + comm_lat
+
+    print(f"\n>> {label} ({nvlink_gen}) latency (ms/token)")
+    print(f"   FC            : {fc_lat:.6f}")
+    print(f"   Attention     : {attn_lat:.6f}")
+    print(f"   MoE           : {moe_lat:.6f}")
+    print(f"   Communication : {comm_lat:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token     : {per_tok:.6f}")
+
+    return per_tok
+
+
+def main():
+    for model_name, bs_list in EXPERIMENTS.items():
+        for bs in bs_list:
+            print("\n" + "="*60)
+            print(f" Experiment: model={model_name}, batch_size={bs}")
+            print("="*60)
+
+            input_sequence_length = 1024
+            update_configuration(model_name, bs, input_sequence_length)
+
+            # -----------------------------------------------------------------------------
+            # Configuration flags:
+            #   target_gpu (bool):
+            #     • True  – measure GPU compute latency and energy
+            #     • False – measure latency and energy for offloaded‐weight copying
+            #   offload_cpu (bool), offload_ssd (bool):
+            #     • offload_cpu=False and offload_ssd=False
+            #         – baseline (no offloading)
+            #     • offload_cpu=True
+            #         – gated‐expert weights are offloaded to CPU memory
+            #     • offload_ssd=True
+            #         – gated‐expert weights are offloaded to SSD
+            # -----------------------------------------------------------------------------
+
+            f_baseline = run_sim(target_gpu=True, offload_cpu=False, offload_ssd=False)
+            f_cpu_offload_gpu_compute = run_sim(target_gpu=True, offload_cpu=True,  offload_ssd=False)
+            f_cpu_offload_weight_copy = run_sim(target_gpu=False, offload_cpu=True,  offload_ssd=False)
+            # below, GPU and CPU-memory/SSD are connected with NVLink4.0
+            f_ssd_offload_gpu_compute_nvlink4 = run_sim(target_gpu=True, offload_cpu=False, offload_ssd=True, nvlink_gen=4)
+            f_ssd_offload_weight_copy_nvlink4 = run_sim(target_gpu=False, offload_cpu=False, offload_ssd=True, nvlink_gen=4)
+            # below, GPU and CPU-memory/SSD are connected with NVLink5.0
+            f_ssd_offload_gpu_compute_nvlink5 = run_sim(target_gpu=True, offload_cpu=False, offload_ssd=True, nvlink_gen=5)            
+            f_ssd_offload_weight_copy_nvlink5 = run_sim(target_gpu=False, offload_cpu=False, offload_ssd=True, nvlink_gen=5)
+
+            df_baseline = pd.read_csv(f_baseline)
+            df_cpu_offload_gpu_compute  = pd.read_csv(f_cpu_offload_gpu_compute)
+            df_cpu_offload_weight_copy  = pd.read_csv(f_cpu_offload_weight_copy)
+            df_ssd_offload_gpu_compute_nvlink4  = pd.read_csv(f_ssd_offload_gpu_compute_nvlink4)
+            df_ssd_offload_weight_copy_nvlink4  = pd.read_csv(f_ssd_offload_weight_copy_nvlink4)
+            df_ssd_offload_gpu_compute_nvlink5  = pd.read_csv(f_ssd_offload_gpu_compute_nvlink5)
+            df_ssd_offload_weight_copy_nvlink5  = pd.read_csv(f_ssd_offload_weight_copy_nvlink5)
+
+            real_bs = load_cfg()['serving']['max_batch_size']
+
+            latency_baseline = compute_latency_baseline("Baseline", df_baseline)
+            compute_latency_offload("Offloaded-case", "nvlink4.0", df_ssd_offload_gpu_compute_nvlink4, df_ssd_offload_weight_copy_nvlink4, real_bs)
+            latency_offload = compute_latency_offload("Offloaded-case", "nvlink5.0", df_ssd_offload_gpu_compute_nvlink5, df_ssd_offload_weight_copy_nvlink5, real_bs) 
+
+            compute_energy_baseline("Baseline", latency_baseline, df_baseline, real_bs)
+            compute_energy_offload("SSD", latency_offload, df_ssd_offload_gpu_compute_nvlink4, df_ssd_offload_weight_copy_nvlink4, real_bs) # SSD offload
+            compute_energy_offload("CPU", latency_offload, df_cpu_offload_gpu_compute, df_cpu_offload_weight_copy, real_bs) # CPU-memory offload
+    
+    print("\nAll experiments done.")
+
+if __name__ == "__main__":
+    main()
diff --git figure_6.py figure_6.py
new file mode 100755
index 0000000..95019c1
--- /dev/null
+++ figure_6.py
@@ -0,0 +1,257 @@
+#!/usr/bin/env python3
+
+import os
+import sys
+import subprocess
+import yaml
+import pandas as pd
+
+BASE_DIR     = os.path.dirname(os.path.abspath(__file__))
+CONFIG_FILE  = os.path.join(BASE_DIR, "build", "config.yaml")
+SIM_CMD      = [os.path.join(BASE_DIR, "build", "run")]
+
+# latency
+LATENCY_FC_COLS = [
+    "qkvgen", "q_down_proj", "kv_down_proj", "kr_proj",
+    "q_up_proj", "qr_proj", "kv_up_proj", "tr_k_up_proj",
+    "v_up_proj", "o_proj", "ffn", "shared_expert_ffn"
+]
+LATENCY_ATTEN = "atten_gen"
+LATENCY_MOE   = "expert_ffn"
+LATENCY_COMM  = "communication"
+
+# energy
+BASE = {
+    "fc_dram":         "fc_dram",
+    "fc_comp":         "fc_comp",
+    "ffn_dram":        "ffn_dram",
+    "ffn_comp":        "ffn_comp",
+    "shared_moe_hbm":  "shared_moe_hbm",
+    "shared_moe_comp": "shared_moe_comp",
+    "attn_dram":       "attn_dram",
+    "attn_comp":       "attn_comp",
+    "moe_hbm":         "moe_hbm",
+    "moe_comp":        "moe_comp",
+}
+OFFLOAD = "moe_offload"
+
+# batch size
+BATCH_SIZES = [1, 2, 3, 4]
+
+# system background power (Watt)
+# single H100 SXM 80GB GPU (includes HBM)
+# ignore the negligible background power of SSD (https://semiconductor.samsung.com/consumer-storage/internal-ssd/9100-pro/)
+# don't use CPU memory in this experiment
+BACKGROUND_POWER = 70
+
+# SSD energy‐scale values to sweep
+SSD_SCALES = [0.1]#1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1]
+
+DISCARD_COLS = [
+    "non_MoE_decoders", "MoE_decoders", "qkvgen", "q_down_proj", "kv_down_proj", "kr_proj",
+    "q_up_proj", "qr_proj", "kv_up_proj", "tr_k_up_proj",
+    "v_up_proj", "atten_sum", "atten_gen", "o_proj",
+    "ffn", "shared_expert_ffn", "communication", "fc_dram",
+    "fc_comp", "attn_dram", "attn_comp", "ffn_dram", "ffn_comp",
+    "shared_moe_hbm", "shared_moe_comp", "moe_hbm", "moe_comp"
+]
+
+def load_cfg():
+    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
+        return yaml.safe_load(f)
+
+def write_cfg(cfg):
+    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
+        yaml.dump(cfg, f, sort_keys=False, allow_unicode=True)
+
+def update_configuration(model_name: str, batch_size: int, input_sequence_length: int):
+    cfg = load_cfg()
+    cfg['model']['model_name'] = model_name
+    cfg['serving']['max_batch_size'] = batch_size
+    cfg['simulation']['input_len'] = input_sequence_length
+    cfg['simulation']['output_len'] = 2 # the first decode stage after the prefill stage
+    cfg['simulation']['iter'] = 2
+
+    cfg['system']['num_node'] = 1
+    cfg['system']['num_device'] = 8
+    cfg['system']['distribution']['expert_tensor_degree'] = 1
+    cfg['system']['distribution']['none_expert_tensor_degree'] = 8
+    cfg['system']['optimization']['compressed_kv'] = False
+    cfg['system']['optimization']['use_absorb'] = False
+
+    # ensure offload flags default
+    cfg['system']['offload']['offload_expert_to_cpu_memory'] = False
+    cfg['system']['offload']['offload_expert_to_ssd']       = False
+    write_cfg(cfg)
+
+def set_ssd_scale(scale: float):
+    cfg = load_cfg()
+    cfg['system']['offload']['ssd_energy_scale'] = scale
+    write_cfg(cfg)
+
+def run_sim(target_gpu: bool, offload_cpu: bool, offload_ssd: bool, nvlink_gen: int = 5) -> str:
+    cfg = load_cfg()
+    cfg['system']['offload']['offload_expert_to_cpu_memory'] = offload_cpu
+    cfg['system']['offload']['offload_expert_to_ssd']       = offload_ssd
+    cfg['system']['offload']['collects_gpu_compute']        = target_gpu
+    cfg['system']['offload']['offload_bandwidth']           = nvlink_gen
+    write_cfg(cfg)
+
+    tag = "CPU_memory" if offload_cpu else "SSD" if offload_ssd else "baseline"
+    collect = "GPU_compute" if target_gpu else "offloaded_weight_copy"
+    print(f"\n--- Running [{tag}] ---")
+    simulation_working_dir = "./build"
+    proc = subprocess.run(SIM_CMD, cwd=simulation_working_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
+    if proc.returncode != 0:
+        print(proc.stdout, file=sys.stdout)
+        print(proc.stderr, file=sys.stderr)
+        sys.exit(f"ERROR: failed ({tag})")
+
+    # file name
+    m    = cfg['model']['model_name']
+    d    = cfg['simulation']['data']
+    nn   = cfg['system']['num_node']
+    nd   = cfg['system']['num_device']
+    ne_tp_dg = cfg["system"]["distribution"]["none_expert_tensor_degree"]
+    ne_dp_dg = int(nd * nn / ne_tp_dg)
+    bs   = cfg['serving']['max_batch_size']
+    od   = cfg['log']['output_directory']
+    fname = os.path.join(simulation_working_dir, od,
+        f"{m}_{d}_N{nn}_D{nd}_TP{ne_tp_dg}_DP{ne_dp_dg}_batchsize{bs}_{tag}_NVLink{nvlink_gen}_{collect}.csv")
+    if not os.path.isfile(fname):
+        sys.exit(f"ERROR: cannot find generated CSV: {fname}")
+    print(f"→ OK: {fname}")
+
+    if not target_gpu:
+        zero_out_columns_in_csv(fname)
+
+    return fname
+
+def zero_out_columns_in_csv(fname: str):
+    df = pd.read_csv(fname)
+    for col in DISCARD_COLS:
+        if col in df.columns:
+            df[col] = 0
+    df.to_csv(fname, index=False)
+
+def compute_energy_baseline(label: str, latency: float, df_base: pd.DataFrame, bs: int) -> float:
+    cfg = load_cfg()
+    num_tot_device = cfg['system']['num_node'] * cfg['system']['num_device']
+
+    r = df_base.loc[0]
+    # xx_off: (memory/storage access) off-chip memory access
+    # xx_on: (GPU-compute) on-chip memory & compute
+    fc_off   = (r[BASE["fc_dram"]] + r[BASE["ffn_dram"]] + r[BASE["shared_moe_hbm"]]) / 1e9
+    fc_on    = (r[BASE["fc_comp"]] + r[BASE["ffn_comp"]] + r[BASE["shared_moe_comp"]]) / 1e9
+    attn_off = r[BASE["attn_dram"]] / 1e9
+    attn_on  = r[BASE["attn_comp"]] / 1e9
+    moe_off  = r[BASE["moe_hbm"]] / 1e9
+    moe_on   = r[BASE["moe_comp"]] / 1e9
+    background   = num_tot_device * BACKGROUND_POWER * latency / 1e3
+    per_tok  = (fc_off + fc_on + attn_off + attn_on + moe_off + moe_on + background) / bs
+
+    print(f"\n>> {label} Energy (J/single decode stage)")
+    print(f"   FC off-chip      : {fc_off:.6f}")
+    print(f"   FC on-chip       : {fc_on:.6f}")
+    print(f"   Attn off-chip    : {attn_off:.6f}")
+    print(f"   Attn on-chip     : {attn_on:.6f}")
+    print(f"   MoE off-chip     : {moe_off:.6f}")
+    print(f"   MoE on-chip      : {moe_on:.6f}")
+    print(f"   System background: {background:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token        : {per_tok:.6f} (÷ batch_size={bs})")
+
+    return per_tok
+
+def compute_energy_offload(label: str, latency: float, df_gpu: pd.DataFrame, df_offload: pd.DataFrame, bs: int) -> float:
+    cfg = load_cfg()
+    num_tot_device = cfg['system']['num_node'] * cfg['system']['num_device']
+
+    r, s = df_gpu.loc[0], df_offload.loc[0]
+    # xx_off: (memory/storage access) off-chip memory access
+    # xx_on: (GPU-compute) on-chip memory & compute
+    fc_off   = (r[BASE["fc_dram"]] + r[BASE["ffn_dram"]] + r[BASE["shared_moe_hbm"]]) / 1e9
+    fc_on    = (r[BASE["fc_comp"]] + r[BASE["ffn_comp"]] + r[BASE["shared_moe_comp"]]) / 1e9
+    attn_off = r[BASE["attn_dram"]] / 1e9
+    attn_on  = r[BASE["attn_comp"]] / 1e9
+    moe_off  = (r[BASE["moe_hbm"]] + s[OFFLOAD]) / 1e9
+    moe_on   = r[BASE["moe_comp"]] / 1e9
+    background   = num_tot_device * BACKGROUND_POWER * latency / 1e3
+    per_tok  = (fc_off + fc_on + attn_off + attn_on + moe_off + moe_on + background) / bs
+
+    print(f"\n>> {label}-offloaded case Energy (J/single decode stage)")
+    print(f"   FC off-chip      : {fc_off:.6f}")
+    print(f"   FC on-chip       : {fc_on:.6f}")
+    print(f"   Attn off-chip    : {attn_off:.6f}")
+    print(f"   Attn on-chip     : {attn_on:.6f}")
+    print(f"   MoE off-chip     : {moe_off:.6f}")
+    print(f"   MoE on-chip      : {moe_on:.6f}")
+    print(f"   System background: {background:.6f}")
+    print(f"   ---------------------------")
+    print(f"   Per token        : {per_tok:.6f} (÷ batch_size={bs})")
+
+    return per_tok
+
+def compute_latency_baseline(df_base: pd.DataFrame) -> float:
+    r = df_base.loc[0]
+    # FC
+    fc_sum = sum(r[col] for col in LATENCY_FC_COLS)
+    fc_lat = fc_sum / 1e6
+    # Attention
+    attn_lat = r[LATENCY_ATTEN] / 1e6
+    # MoE
+    moe_lat  = r[LATENCY_MOE] / 1e6
+    # Communication
+    comm_lat = r[LATENCY_COMM] / 1e6
+    per_tok = fc_lat + attn_lat + moe_lat + comm_lat
+
+    return per_tok
+
+def compute_latency_offload(df_offload: pd.DataFrame) -> float:
+    s = df_offload.loc[0]
+    # gated-expert weight load latency exceeds compute latency
+    moe_lat  = s[LATENCY_MOE] / 1e6
+    per_tok = moe_lat
+
+    return per_tok
+
+def main():
+    for bs in BATCH_SIZES:
+        print("\n" + "="*60)
+        print(f"### Batch size = {bs} ###")
+        print("="*60)
+
+        input_sequence_length = 1024
+        # llama3_70B (baseline)
+        update_configuration("llama3_70B", bs, input_sequence_length)
+        real_bs = load_cfg()['serving']['max_batch_size']
+
+        f_baseline = run_sim(target_gpu=True, offload_cpu=False, offload_ssd=False)
+        df_baseline = pd.read_csv(f_baseline)
+        latency_llama3 = compute_latency_baseline(df_baseline)
+        energy_llama3 = compute_energy_baseline("llama3_70B Baseline", latency_llama3, df_baseline, real_bs)
+
+        # llama4_maverick (SSD-offloading)
+        print("-"*60)
+        update_configuration("llama4_maverick", bs, input_sequence_length)
+        f_ssd_gpu_compute = run_sim(target_gpu=True, offload_cpu=False, offload_ssd=True, nvlink_gen=5)
+        df_ssd_gpu = pd.read_csv(f_ssd_gpu_compute)
+        
+        for scale in SSD_SCALES:
+            print("-"*60)
+            set_ssd_scale(scale)
+            ssd_read_energy = 102.4 * scale
+            print(f"\n>>> ssd_energy_scale = {scale} ({ssd_read_energy:.1f}pJ/b)")
+
+            f_ssd_weight_copy = run_sim(target_gpu=False, offload_cpu=False, offload_ssd=True, nvlink_gen=5)
+            df_ssd_offload = pd.read_csv(f_ssd_weight_copy)
+            latency_llama4 = compute_latency_offload(df_ssd_offload)
+            energy_llama4 = compute_energy_offload("llama4_maverick SSD", latency_llama4, df_ssd_gpu, df_ssd_offload, real_bs)
+
+            ratio = energy_llama4 / energy_llama3 if energy_llama3 > 0 else 0.0
+            print(f"\n>>> Energy ratio (llama4_maverick / llama3_70B) at batch size {bs} = {ratio:.3f}\n")
+
+    print("\nAll experiments done.")
+
+if __name__ == "__main__":
+    main()
diff --git log/cmd_hbm3.log.ch0 log/cmd_hbm3.log.ch0
new file mode 100644
index 0000000..e69de29
diff --git script/edap.sh script/edap.sh
new file mode 100755
index 0000000..c6ec2bc
--- /dev/null
+++ script/edap.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+
+MAX_CORE=30
+
+
+OUTPUT_DIR="../data/edap"
+
+rm -rf run__.sh
+
+mkdir -p ${OUTPUT_DIR}
+
+echo "../build/edap ${OUTPUT_DIR}" >> run__.sh
+
+./run_multiprog.py --process ${MAX_CORE} --script run__.sh
+rm -rf run__.sh
diff --git script/pinball_config.yaml script/pinball_config.yaml
new file mode 100644
index 0000000..f338f59
--- /dev/null
+++ script/pinball_config.yaml
@@ -0,0 +1,31 @@
+Frontend:
+  impl: PIMController
+  clock_ratio: 1
+  path: example_inst.trace 
+
+MemorySystem:
+  impl: PIMDRAMSystem
+  clock_ratio: 1
+
+  DRAM:
+    impl: HBM3
+    org:
+      preset: HBM3_16Gb_2R
+      channel: 1
+      rank: 2
+    timing:
+      preset: HBM3_5.2Gbps
+
+  Controller:
+    impl: PIM_DRAM_controller
+    Scheduler:
+      impl: PIM_Scheduler
+    RefreshManager:
+      impl: AllBank
+#    plugins:
+#    - ControllerPlugin:
+#        impl: TraceRecorder
+#        path: log/cmd.log
+
+  AddrMapper:
+    impl: RoBaRaCoCh
diff --git script/run_multiprog.py script/run_multiprog.py
new file mode 100755
index 0000000..80b3dea
--- /dev/null
+++ script/run_multiprog.py
@@ -0,0 +1,109 @@
+#!/usr/bin/python3
+
+from optparse import OptionParser
+import sys, re, string, time, os
+from subprocess import Popen
+from datetime import datetime
+
+def getcmd(cmd_list, simscript):
+  try:
+    script = open(simscript,'r')
+  except IOError:
+    print ("cannot open " + options.simscript)
+    sys.exit()
+
+  for line in script:
+    line = re.sub('#.*', '', line)
+    if len(line) > 2:
+      line=line[:-1]
+      cmd_list.append(line)
+
+def check_va_randomize():
+  va_flag = "/proc/sys/kernel/randomize_va_space"
+  try:
+    f = open(va_flag,'rt')
+  except IOError:
+    print ("cannot open " + va_flag)
+
+  flag = re.split("\s*",f.readline())
+  if flag[0] != "0":
+    print ("Set %s 0"%(va_flag))
+    print ("cmd echo 0 > %s"%(va_flag))
+    print ("sudo sh -c \"echo 0 > /proc/sys/kernel/randomize_va_space\" ")
+#    sys.exit(1)
+    print ("for now, it is allowed to run yet")
+
+usage = "usage: %prog [options]"
+parser = OptionParser(usage)
+parser.add_option("--process", action = "store", type="int", default=4, dest="process", help="maximum active process, default = 4")
+parser.add_option("--script", action = "store", type="string", default="/dev/null", dest="simscript", help="cmd list")
+
+(options, args) = parser.parse_args()
+
+active_processes = []
+cmd_list = []
+
+check_va_randomize()
+getcmd(cmd_list, options.simscript)
+
+
+#create log dir if it is not exist
+host = os.uname()[1]
+now = str(datetime.now())
+t = re.split("\s*",now)
+t1 = re.split("\.",t[1])
+t1 = re.split(":",t1[0])
+if not os.path.exists("/scale/cal/home/jhpark/log/%s"%t[0]):
+  os.makedirs("/scale/cal/home/jhpark/log/%s"%t[0])
+#log = open(logfile, 'w')
+script = os.path.abspath(options.simscript)
+#log.write("Simulation Start: %s with %d processes\n"%(now, options.process))
+#log.write("Script: %s\n\n"%(script))
+#log.close()
+
+cmd_list.reverse()
+# first, check if we have enough processes
+# for process in active_processes:
+while True:
+  try:
+    for process in active_processes[:]:
+     if process.poll() != None:
+       active_processes.remove(process)
+       finish = "Process [pid = " + str(process.pid) + "] finished"
+       #log = open(logfile,'a')
+       #log.write(finish + "\n")
+       #log.close()
+       print (finish)
+
+    while len(active_processes) < options.process:
+      if len(cmd_list) != 0:
+        cmd=cmd_list.pop()
+        #log = open(logfile, 'a')
+        #log.write("started -- " + cmd + "\n")
+        #log.close()
+      else:
+        break
+      active_processes.append(Popen([cmd], shell=True))
+      print ("Process [pid = "+str(active_processes[-1].pid)+"] started -- " + cmd)
+
+    time.sleep(1)
+
+    if len(active_processes) == 0:
+      break
+
+  except KeyboardInterrupt:
+    os.killpg(os.getpgrp(), 9)
+    for process in active_processes:
+      kill = "Process [pid = " + str(process.pid) + "] killed"
+      #log = open(logfile, 'a')
+      #log.write(kill + "\n")
+      #log.close()
+      print (kill)
+      os.kill(process.pid, 9)
+    sys.exit()
+
+#log = open(logfile, 'a')
+end = str(datetime.now())
+#log.write("\nSimulation End: %s\n"%(end))
+#log.close()
+sys.exit()
diff --git script/sum_gen_split.sh script/sum_gen_split.sh
new file mode 100755
index 0000000..2c86110
--- /dev/null
+++ script/sum_gen_split.sh
@@ -0,0 +1,52 @@
+#!/bin/bash
+
+MAX_CORE=32
+
+
+OUTPUT_DIR="../data/sum_gen_split"
+
+rm -rf run__.sh
+
+mkdir -p ${OUTPUT_DIR}
+
+for MODEL in "mixtral"
+do
+  for DATA in "synthesis"
+  do
+    for HARDWARE in "GPU" 
+    do
+      for DEVICE_NUM in 2
+      do
+        for MAX_BATCHSIZE in 64
+        do
+          for MAX_PROCESS_TOKEN in 0
+          do
+            for INPUT_LEN in 1024
+            do
+              for OUTPUT_LEN in 16 64 256 1024 4096
+              do
+                for ITER in 5000
+                do
+                  for INJECTION_RATE in 0 1 2 4 8 16
+                  do
+                    for NUM_NODE in 1
+                    do
+                      for PARALLEL_EXECUTION in 0
+                      do
+                          echo "../build/sum_gen_split ${MODEL} ${DATA} ${HARDWARE} ${DEVICE_NUM} ${OUTPUT_DIR} ${MAX_BATCHSIZE} ${INPUT_LEN} ${OUTPUT_LEN} ${MAX_PROCESS_TOKEN} ${INJECTION_RATE} ${ITER} 1 1 ${PARALLEL_EXECUTION}" >> run__.sh
+                  
+                      done
+                    done
+                  done
+                done
+              done
+            done
+          done
+        done
+      done
+    done
+  done
+done
+
+./run_multiprog.py --process ${MAX_CORE} --script run__.sh
+rm -rf run__.sh
diff --git script/test_stat.sh script/test_stat.sh
new file mode 100755
index 0000000..156ecac
--- /dev/null
+++ script/test_stat.sh
@@ -0,0 +1,46 @@
+#!/bin/bash
+
+MAX_CORE=30
+
+
+OUTPUT_DIR="../data/latency"
+# OUTPUT_DIR="../data/latency/not_ramul"
+
+rm -rf run__.sh
+
+mkdir -p ${OUTPUT_DIR}
+
+for MODEL in mixtral
+do
+  for DATA in "synthesis"
+  do
+    for HARDWARE in "GPU"
+    do
+      for DEVICE_NUM in 4
+      do
+        for MAX_BATCHSIZE in 64
+        do
+          for MAX_PROCESS_TOKEN in 0
+          do
+            for INPUT_LEN in 4096
+            do
+              for OUTPUT_LEN in 512
+              do
+                for INJECTION_RATE in 0
+                do
+                  for ITER in 1000
+                  do
+                    echo "../build/stat-export ${MODEL} ${DATA} ${HARDWARE} ${DEVICE_NUM} ${OUTPUT_DIR} ${MAX_BATCHSIZE} ${INPUT_LEN} ${OUTPUT_LEN} ${MAX_PROCESS_TOKEN} ${INJECTION_RATE} ${ITER}" >> run__.sh
+                  done
+                done
+              done
+            done
+          done
+        done
+      done
+    done
+  done
+done
+
+./run_multiprog.py --process ${MAX_CORE} --script run__.sh
+rm -rf run__.sh
diff --git script/time_breakdown.sh script/time_breakdown.sh
new file mode 100755
index 0000000..731d471
--- /dev/null
+++ script/time_breakdown.sh
@@ -0,0 +1,30 @@
+#!/bin/bash
+
+MAX_CORE=30
+
+
+OUTPUT_DIR="../data/latency"
+
+rm -rf run__.sh
+
+mkdir -p ${OUTPUT_DIR}
+
+for MODEL in mixtral
+do
+  for DATA in GSM ShareGPT Alpaca HellaSwag
+  do
+    for HARDWARE in "GPU" "LOGIC" "GPU+LOGIC"
+    do
+      for DEVICE_NUM in 2 4
+      do
+        for ITER in 100
+        do
+          echo "../build/timebreakdown-export ${MODEL} ${DATA} ${HARDWARE} ${DEVICE_NUM} ${OUTPUT_DIR} ${ITER}" >> run__.sh
+        done
+      done
+    done
+  done
+done
+
+./run_multiprog.py --process ${MAX_CORE} --script run__.sh
+rm -rf run__.sh
diff --git src/dram/CMakeLists.txt src/dram/CMakeLists.txt
index 8d3f471..1161146 100644
--- src/dram/CMakeLists.txt
+++ src/dram/CMakeLists.txt
@@ -2,6 +2,7 @@ add_library(dram STATIC)
 
 add_subdirectory(pimkernel)
 add_subdirectory(ramulator2)
+#add_dependencies(dram ramulator)
 
 target_sources(
   dram PRIVATE
@@ -11,4 +12,29 @@ target_sources(
   pim_request.cpp
   memory_object.cpp
   mmap_controller.cpp
-)
\ No newline at end of file
+)
+
+# target_link_libraries(
+#   dram 
+#   PRIVATE ramulator
+#   PRIVATE pimkernel
+# )
+
+add_executable(dram-test)
+target_sources(
+  dram-test
+  PRIVATE test.cpp
+)
+
+target_link_libraries(
+  dram-test
+  PRIVATE llm_system
+)
+
+set_target_properties(
+  dram-test
+  PROPERTIES
+  OUTPUT_NAME dram-test
+  RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
diff --git src/dram/pimkernel/CMakeLists.txt src/dram/pimkernel/CMakeLists.txt
index 7690072..bd393b1 100644
--- src/dram/pimkernel/CMakeLists.txt
+++ src/dram/pimkernel/CMakeLists.txt
@@ -7,6 +7,20 @@ target_sources(
   Read.cpp
   Write.cpp
   GEMV.cpp
+  # pimkernel/Move.cpp
+  # pimkernel/Mult.cpp
+  # pimkernel/Add.cpp
+  # pimkernel/MAD.cpp
+  # pimkernel/PMult.cpp
+  # pimkernel/CMult.cpp
+  # pimkernel/CAdd.cpp
+  # pimkernel/CMAD.cpp
+  # pimkernel/Tensor.cpp
+  # pimkernel/Tensor_Square.cpp
+  # pimkernel/ModUp_Evkmult.cpp
+  # pimkernel/ModDownEpilogue.cpp
+  # pimkernel/PMult_Accum.cpp
+  # pimkernel/CMult_Accum.cpp
 )
 
 
diff --git src/dram/pimkernel/heean/Add.cpp src/dram/pimkernel/heean/Add.cpp
new file mode 100644
index 0000000..69ca7a8
--- /dev/null
+++ src/dram/pimkernel/heean/Add.cpp
@@ -0,0 +1,58 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void Add_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                DRAMRequest::PIM_Operand& operand,
+                const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 2, "Add operands are not valid");
+  assertTrue(dest.size() == 1, "Mult destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    // b = a1 + a2
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/CAdd.cpp src/dram/pimkernel/heean/CAdd.cpp
new file mode 100644
index 0000000..a8bea9b
--- /dev/null
+++ src/dram/pimkernel/heean/CAdd.cpp
@@ -0,0 +1,57 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void CAdd_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                 DRAMRequest::PIM_Operand& operand,
+                 const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 1, "CAdd operands are not valid");
+  assertTrue(dest.size() == 1, "CAdd destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // b = a1 + c
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kAdd,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/CMAD.cpp src/dram/pimkernel/heean/CMAD.cpp
new file mode 100644
index 0000000..fbe8e2a
--- /dev/null
+++ src/dram/pimkernel/heean/CMAD.cpp
@@ -0,0 +1,60 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void CMAD_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                 DRAMRequest::PIM_Operand& operand,
+                 const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int dnum = src.size();
+  int evk_size = dest.size();
+
+  int mde_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer;
+
+  assertTrue(src.size() == 2, "CMAD operands are not valid");
+  assertTrue(dest.size() == 1, "Mult destinations are not valid");
+
+  // loop for read Modup to RF
+  for (int chunk_idx = 0; chunk_idx < mde_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // b = a1 * c
+    // b += a2
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, mde_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, mde_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/CMult.cpp src/dram/pimkernel/heean/CMult.cpp
new file mode 100644
index 0000000..44f1a90
--- /dev/null
+++ src/dram/pimkernel/heean/CMult.cpp
@@ -0,0 +1,57 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void CMult_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                  DRAMRequest::PIM_Operand& operand,
+                  const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 1, "CMult operands are not valid");
+  assertTrue(dest.size() == 1, "CMult destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // b = a1 * c
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/CMult_Accum.cpp src/dram/pimkernel/heean/CMult_Accum.cpp
new file mode 100644
index 0000000..1d7a76d
--- /dev/null
+++ src/dram/pimkernel/heean/CMult_Accum.cpp
@@ -0,0 +1,84 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void CMult_Accum_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                        DRAMRequest::PIM_Operand& operand,
+                        const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_x = src.size() / 2;
+  int num_dest = dest.size();
+
+  int num_chunk = (src[0]->GetAddress()).num_chunks_;
+
+  // assertTrue(src.size() == 2, "PMult Accum operands are not valid");
+  assertTrue(src.size() % 2 == 0, "CMult Accum operands are not valid");
+  assertTrue(dest.size() == 2, "CMult Accum destinations are not valid");
+
+  // the number of chunks which can be buffered
+  int num_chunk_in_buffer = total_buffer / (num_dest);
+  int max_num_x_in_buffer = std::min(total_buffer - num_dest, num_x);
+  num_chunk_in_buffer = std::max(num_chunk_in_buffer, 1);
+
+  // std::next(buffer.begin(), std::min(max_size, buffer.size())
+  // loop for read Modup to RF
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    for (int x_idx = 0; x_idx < num_x; x_idx += max_num_x_in_buffer) {
+      for (int x_inner_idx = 0;
+           x_inner_idx < std::min(max_num_x_in_buffer, num_x - x_idx);
+           x_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd_b = src[2 * (x_idx + x_inner_idx)];
+
+          addr_vec[4] = opnd_b->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd_b->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kSrc, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+      for (int x_inner_idx = 0;
+           x_inner_idx < std::min(max_num_x_in_buffer, num_x - x_idx);
+           x_inner_idx++) {
+        // read a1
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd_a = src[2 * (x_idx + x_inner_idx) + 1];
+
+          addr_vec[4] = opnd_a->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd_a->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kSrc, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+    }
+    for (auto des_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = des_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = des_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kDest, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/MAD.cpp src/dram/pimkernel/heean/MAD.cpp
new file mode 100644
index 0000000..bbb9c86
--- /dev/null
+++ src/dram/pimkernel/heean/MAD.cpp
@@ -0,0 +1,59 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void MAD_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                DRAMRequest::PIM_Operand& operand,
+                const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 3, "MAD operands are not valid");
+  assertTrue(dest.size() == 1, "MAD destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    // b = a1 * a2
+    // b += a3
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/ModDownEpilogue.cpp src/dram/pimkernel/heean/ModDownEpilogue.cpp
new file mode 100644
index 0000000..4098d43
--- /dev/null
+++ src/dram/pimkernel/heean/ModDownEpilogue.cpp
@@ -0,0 +1,58 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void ModDownEpilogue_kernel(PIMRequest& pim_request,
+                            DRAMRequestType dramreq_type,
+                            DRAMRequest::PIM_Operand& operand,
+                            const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int dnum = src.size();
+  int evk_size = dest.size();
+
+  int mde_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer;
+
+  // loop for read Modup to RF
+  for (int chunk_idx = 0; chunk_idx < mde_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // b = a1 * c1 + a2 + a3
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, mde_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, mde_chunk - chunk_idx);
+           rf_inner_idx++) {
+        // b = a1 * c1 + a2 + a3
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/ModUp_Evkmult.cpp src/dram/pimkernel/heean/ModUp_Evkmult.cpp
new file mode 100644
index 0000000..732fed0
--- /dev/null
+++ src/dram/pimkernel/heean/ModUp_Evkmult.cpp
@@ -0,0 +1,100 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void ModUp_Evkmult_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                          DRAMRequest::PIM_Operand& operand,
+                          const PIMHWConfig pim_hw_config) {
+  // const int num_modup_buffer = 8;  // number of buffer, each holds 256-bit
+  // const int num_accum_buffer = 8;  // twice for total accum buffer
+  const int total_buffer = pim_hw_config.num_buffer;
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto precomputed = get_operand(operand, PIMOperandType::kPrecomputed);
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dnum = src.size();
+  int num_dest = dest.size();
+
+  int num_chunk = (src[0]->GetAddress()).num_chunks_;
+
+  // assertTrue(src.size() == 2, "ModUp_Evkmult operands are not valid");
+  // assertTrue(num_dnum * 3 < total_buffer,
+  //            "ModUp_Evkmult may results invalid value");
+  assertTrue(precomputed.size() == 2 * num_dnum,
+             "ModUp_Evkmult operands are not valid");
+  assertTrue(dest.size() == 2, "ModUp_Evkmult destinations are not valid");
+
+  // the number of chunks which can be buffered
+  int num_chunk_in_buffer = total_buffer / (num_dnum + num_dest);
+  int max_num_dnum_in_buffer = std::min(total_buffer - num_dest, num_dnum);
+  num_chunk_in_buffer = std::max(num_chunk_in_buffer, 1);
+
+  // loop for read Modup to RF
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    for (int dnum_idx = 0; dnum_idx < num_dnum;
+         dnum_idx += max_num_dnum_in_buffer) {
+      for (int dnum_inner_idx = 0;
+           dnum_inner_idx <
+           std::min(max_num_dnum_in_buffer, num_dnum - dnum_idx);
+           dnum_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd = src[dnum_idx + dnum_inner_idx];
+          addr_vec[4] = opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                            PIMOperandType::kModUp, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+    }
+    for (int dnum_idx = 0; dnum_idx < num_dnum;
+         dnum_idx += max_num_dnum_in_buffer) {
+      for (int dnum_inner_idx = 0;
+           dnum_inner_idx <
+           std::min(max_num_dnum_in_buffer, num_dnum - dnum_idx);
+           dnum_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd = precomputed[(dnum_idx + dnum_inner_idx) * 2];
+
+          addr_vec[4] = opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kEvk, addr_vec,
+                                            &pim_request, dramreq_type));
+
+          opnd = precomputed[dnum_inner_idx * 2 + 1];
+          addr_vec[4] = opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kEvk, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+    }
+    for (auto des_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = des_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = des_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kDest, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/Move.cpp src/dram/pimkernel/heean/Move.cpp
new file mode 100644
index 0000000..3790a97
--- /dev/null
+++ src/dram/pimkernel/heean/Move.cpp
@@ -0,0 +1,57 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void Move_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                 DRAMRequest::PIM_Operand& operand,
+                 const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 1, "Move operands are not valid");
+  assertTrue(dest.size() == 1, "Move destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/Mult.cpp src/dram/pimkernel/heean/Mult.cpp
new file mode 100644
index 0000000..dc22e19
--- /dev/null
+++ src/dram/pimkernel/heean/Mult.cpp
@@ -0,0 +1,58 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void Mult_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                 DRAMRequest::PIM_Operand& operand,
+                 const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / num_dest;
+
+  assertTrue(src.size() == 2, "Mult operands are not valid");
+  assertTrue(dest.size() == 1, "Mult destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    // b = a1 * a2
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/PMult.cpp src/dram/pimkernel/heean/PMult.cpp
new file mode 100644
index 0000000..430f6c9
--- /dev/null
+++ src/dram/pimkernel/heean/PMult.cpp
@@ -0,0 +1,75 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void PMult_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                  DRAMRequest::PIM_Operand& operand,
+                  const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto precomputed = get_operand(operand, PIMOperandType::kPrecomputed);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_precomputed = precomputed.size();
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  const int num_chunk_in_buffer = total_buffer / (num_precomputed + num_dest);
+
+  assertTrue(src.size() == 2, "PMult operands are not valid");
+  assertTrue(precomputed.size() == 1, "PMult operands are not valid");
+  assertTrue(dest.size() == 2, "PMult destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read precomputed
+    for (auto prc_opnd : precomputed) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = prc_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = prc_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(
+            PIMCommand(PIMCommandType::kDRAM2RF, PIMOperandType::kPrecomputed,
+                       addr_vec, &pim_request, dramreq_type));
+      }
+    }
+
+    // a = a1 * p
+    // b = b1 * p
+    for (auto src_opnd : src) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                          PIMOperandType::kSrc, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+
+    // write b to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/PMult_Accum.cpp src/dram/pimkernel/heean/PMult_Accum.cpp
new file mode 100644
index 0000000..6ed45e6
--- /dev/null
+++ src/dram/pimkernel/heean/PMult_Accum.cpp
@@ -0,0 +1,103 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void PMult_Accum_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                        DRAMRequest::PIM_Operand& operand,
+                        const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto precomputed = get_operand(operand, PIMOperandType::kPrecomputed);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_x = precomputed.size();
+  int num_dest = dest.size();
+
+  int num_chunk = (src[0]->GetAddress()).num_chunks_;
+
+  // assertTrue(src.size() == 2, "PMult Accum operands are not valid");
+  assertTrue(src.size() == 2 * precomputed.size(),
+             "PMult Accum operands are not valid");
+  assertTrue(dest.size() == 2, "PMult Accum destinations are not valid");
+
+  // the number of chunks which can be buffered
+  int num_chunk_in_buffer = total_buffer / (num_x + num_dest);
+  int max_num_x_in_buffer = std::min(total_buffer - num_dest, num_x);
+  num_chunk_in_buffer = std::max(num_chunk_in_buffer, 1);
+
+  // std::next(buffer.begin(), std::min(max_size, buffer.size())
+  // loop for read Modup to RF
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    for (int x_idx = 0; x_idx < num_x; x_idx += max_num_x_in_buffer) {
+      // read precomputed to RF
+      for (int x_inner_idx = 0;
+           x_inner_idx < std::min(max_num_x_in_buffer, num_x - x_idx);
+           x_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd = precomputed.at(x_idx + x_inner_idx);
+          addr_vec[4] = opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(
+              PIMCommand(PIMCommandType::kDRAM2RF, PIMOperandType::kPrecomputed,
+                         addr_vec, &pim_request, dramreq_type));
+        }
+      }
+      for (int x_inner_idx = 0;
+           x_inner_idx < std::min(max_num_x_in_buffer, num_x - x_idx);
+           x_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd_b = src.at((x_idx + x_inner_idx) * 2);
+
+          addr_vec[4] = opnd_b->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd_b->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kSrc, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+
+      for (int x_inner_idx = 0;
+           x_inner_idx < std::min(max_num_x_in_buffer, num_x - x_idx);
+           x_inner_idx++) {
+        for (int rf_inner_idx = 0;
+             rf_inner_idx <
+             std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+             rf_inner_idx++) {
+          Limb::Ptr opnd_a = src.at((x_idx + x_inner_idx) * 2 + 1);
+
+          addr_vec[4] = opnd_a->GetRowAddr(chunk_idx + rf_inner_idx);
+          addr_vec[5] = opnd_a->GetColAddr(chunk_idx + rf_inner_idx);
+          pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                            PIMOperandType::kSrc, addr_vec,
+                                            &pim_request, dramreq_type));
+        }
+      }
+    }
+
+    for (auto des_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = des_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = des_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kDest, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/Tensor.cpp src/dram/pimkernel/heean/Tensor.cpp
new file mode 100644
index 0000000..0b67c26
--- /dev/null
+++ src/dram/pimkernel/heean/Tensor.cpp
@@ -0,0 +1,134 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void Tensor_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                   DRAMRequest::PIM_Operand& operand,
+                   const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_src = src.size();
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  // a1, a2, b2, a, b, aux is buffered
+  // input limbs : (b1, a1, b2, a2)
+  const int num_chunk_in_buffer = total_buffer / (num_src + num_dest - 1);
+
+  assertTrue(src.size() == 4, "Tensor operands are not valid");
+  assertTrue(dest.size() == 3, "Tensor destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    // read a2
+    // read b2
+    // a = a1 * b2 (read b2 from DRAM)
+    // a += b1 * a2 (read b1 from DRAM)
+    // b = b1 * b2 (read b1 from DRAM)
+    // aux = a1 * a2 (no read from DRAM)
+    // write a, b, aux to DRAM
+
+    // read a1
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[1];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // read a2
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[3];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // read b2
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[2];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // a = a1 * b2 (read b2 from DRAM)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[2];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // a += b1 * a2 (read b1 from DRAM)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[0];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMAC,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // b = b1*b2 (read b1 from DRAM)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[0];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // aux = a1 * a2 (no read from DRAM, just dummy)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[0];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // write a, b, aux to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/pimkernel/heean/Tensor_Square.cpp src/dram/pimkernel/heean/Tensor_Square.cpp
new file mode 100644
index 0000000..620923e
--- /dev/null
+++ src/dram/pimkernel/heean/Tensor_Square.cpp
@@ -0,0 +1,95 @@
+#include <dram/dram_interface.h>
+#include <dram/pim_kernel.h>
+
+namespace llm_system {
+namespace PIM_KERNEL {
+
+void Tensor_Square_kernel(PIMRequest& pim_request, DRAMRequestType dramreq_type,
+                          DRAMRequest::PIM_Operand& operand,
+                          const PIMHWConfig pim_hw_config) {
+  const int total_buffer = pim_hw_config.num_buffer;
+
+  Ramulator::AddrVec_t addr_vec = {0, 0, 0, 0, 0, 0};
+
+  auto src = get_operand(operand, PIMOperandType::kSrc);
+  auto dest = get_operand(operand, PIMOperandType::kDest);
+
+  int num_src = src.size();
+  int num_dest = dest.size();
+
+  int num_chunk = (dest[0]->GetAddress()).num_chunks_;
+
+  // the number of chunks which can be buffered
+  // b1, a1 is buffered
+  // input limbs : (b1, a1)
+  const int num_chunk_in_buffer = total_buffer / (num_src + num_dest);
+
+  assertTrue(src.size() == 2, "Tensor_Square operands are not valid");
+  assertTrue(dest.size() == 3, "Tensor_Square destinations are not valid");
+
+  for (int chunk_idx = 0; chunk_idx < num_chunk;
+       chunk_idx += num_chunk_in_buffer) {
+    // read a1
+    // aux = a1 * a1 (read a1 from DRAM)
+    // read b1
+    // b = b1 * b1 (read b1 from DRAM)
+    // a = 2 * b1 (read b1 from DRAM)
+    // a *= a1 (no read from DRAM)
+    // write a, b, aux to DRAM
+
+    // read a1
+    // aux = a1 * a1 (read a1 from DRAM)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[1];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+    // read b1
+    // b = b1 * b1 (read b1 from DRAM)
+    // a = 2 * b1 (read b1 from DRAM)
+    // a *= a1 (no read from DRAM)
+    for (int rf_inner_idx = 0;
+         rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+         rf_inner_idx++) {
+      Limb::Ptr src_opnd = src[0];
+      addr_vec[4] = src_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+      addr_vec[5] = src_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kDRAM2RF,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+      pim_request.AddCommand(PIMCommand(PIMCommandType::kMult,
+                                        PIMOperandType::kSrc, addr_vec,
+                                        &pim_request, dramreq_type));
+    }
+
+    // write a, b, aux to DRAM
+    for (auto dest_opnd : dest) {
+      for (int rf_inner_idx = 0;
+           rf_inner_idx < std::min(num_chunk_in_buffer, num_chunk - chunk_idx);
+           rf_inner_idx++) {
+        addr_vec[4] = dest_opnd->GetRowAddr(chunk_idx + rf_inner_idx);
+        addr_vec[5] = dest_opnd->GetColAddr(chunk_idx + rf_inner_idx);
+        pim_request.AddCommand(PIMCommand(PIMCommandType::kRF2DRAM,
+                                          PIMOperandType::kRF, addr_vec,
+                                          &pim_request, dramreq_type));
+      }
+    }
+  }
+}
+
+}  // namespace PIM_KERNEL
+}  // namespace llm_system
\ No newline at end of file
diff --git src/dram/power.h src/dram/power.h
index 7d2f0f1..5710468 100644
--- src/dram/power.h
+++ src/dram/power.h
@@ -32,7 +32,7 @@ struct DramEnergy {
 // for example, energy for Read operation in HBM2E is 3.48 (= 0.44 + 1.01 + 1.23 + 0.5 + 0.3) pJ/b
 // and HBM's granularity is 32Byte (256bit), so we multiply 256 and divide it by 1000 to get nJ (= 3.48 * 256 / 1000 = 0.8912 nJ)
 
-static DramEnergy gpuEnergy{0.909, 0.891, 0.891, 0, 0, 0, 0.46 / 2 / 1000};
+static DramEnergy gpuEnergy{0.909, 0.891, 0.891, 0, 0, 0, 0.32 / 2 / 1000};  // 0.32 for bf16 GPU MAC, reference: Ten Lessons From Three Generations Shaped Google’s TPUv4i (ISCA'21)
 
 static DramEnergy logicEnergy{0.909,     0.464,     0.464,
                               0.909 * 8, 0.464 * 8, 0.464 * 8, 0.46 / 2 / 1000};  // X4
diff --git src/hardware/CMakeLists.txt src/hardware/CMakeLists.txt
index 47a22e6..497a6e4 100644
--- src/hardware/CMakeLists.txt
+++ src/hardware/CMakeLists.txt
@@ -1,5 +1,7 @@
 add_library(hardware OBJECT)
 
+target_link_libraries(hardware PUBLIC yaml-cpp::yaml-cpp)
+
 target_sources(
     hardware PRIVATE
 
@@ -14,4 +16,15 @@ target_sources(
     attention_sum_impl.cpp
     attention_gen_impl.cpp
     attention_mixed_impl.cpp
-)
\ No newline at end of file
+)
+
+add_executable(hardware-test test.cpp)
+
+target_link_libraries(
+    hardware-test
+    PRIVATE llm_system
+)
+
+set_target_properties(hardware-test PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
diff --git src/hardware/attention_gen_impl.cpp src/hardware/attention_gen_impl.cpp
index 678754c..2ae6ab8 100644
--- src/hardware/attention_gen_impl.cpp
+++ src/hardware/attention_gen_impl.cpp
@@ -1,10 +1,17 @@
 #include <memory>
 
+#include <algorithm>
+#include <cmath>
+#include <limits>
+#include <vector>
+#include <array>
+
 #include "common/type.h"
 #include "dram/dram_interface.h"
 #include "dram/dram_request.h"
 #include "hardware/layer_impl.h"
 #include "module/tensor.h"
+#include "hardware/tile.h"
 
 namespace llm_system {
 class DRAMRequest;
@@ -13,6 +20,7 @@ class Tensor;
 using Tensor_Ptr = std::shared_ptr<Tensor>;
 using DRAMRequest_Ptr = std::shared_ptr<DRAMRequest>;
 
+//// In this branch, we use only GPU-related code, neither LOGIC nor PIM
 ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
                                     std::vector<Tensor_Ptr> tensor,
                                     BatchedSequence::Ptr sequences_metadata,
@@ -64,6 +72,13 @@ ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
     k = head_dim;
     n = seq->current_len + seq->num_process_token;
 
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m * num_heads / num_kv_heads, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m * num_heads / num_kv_heads, n, k, input->precision_byte, tiles);
+
     for (int kv_idx = 0; kv_idx < num_kv_heads; kv_idx++) {
       flops = m * k * n * 2.0 * attention_group_size;
       total_flops += flops;
@@ -78,23 +93,12 @@ ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
       memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
       accumul_memory_duration += memory_duration;
     }
-    accumul_len += n;
-  }
 
-  if (use_ramulator) {
-    k_cache->setShape({accumul_len, head_dim * num_kv_heads});
-    ExecStatus temp;
-    temp =
-        issueRamulator(device, LayerType::ATTENTION_GEN, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, k_cache);
-    exec_status += temp;
-    accumul_memory_duration = temp.memory_duration;
-  }
-  else {
-    k_cache->setShape({accumul_len, head_dim * num_kv_heads});
-    ExecStatus temp;
-    temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, k_cache);
-    exec_status += temp;
+    exec_status.read_count += traffic[0] * num_kv_heads;
+    exec_status.l2_count += traffic[1] * num_kv_heads;
+    exec_status.l1_count += traffic[2] * num_kv_heads;
+
+    accumul_len += n;
   }
 
   exec_status.total_duration +=
@@ -113,9 +117,12 @@ ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
     flops = 7.0 * m * n * num_heads; // scale + mask + softmax
     total_flops += flops;
 
+    memory_size = (2.0 * m * n * num_heads) * input->precision_byte;
+
     compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
 
-    exec_status.total_duration += compute_duration;
+    exec_status.total_duration += std::max(compute_duration, memory_duration);
   }
 
   // Context //
@@ -129,6 +136,13 @@ ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
     k = seq->current_len + seq->num_process_token;
     n = head_dim;
 
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m * num_heads / num_kv_heads, n, k, input->precision_byte,
+                                        config.l1_cache_size,
+                                        config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m * num_heads / num_kv_heads, n, k, input->precision_byte, tiles);
+
     for (int kv_idx = 0; kv_idx < num_kv_heads; kv_idx++) {
       flops = m * k * n * 2.0 * attention_group_size;
       total_flops += flops;
@@ -143,39 +157,24 @@ ExecStatus AttentionGenExecutionGPU(Device_Ptr device,
       memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
       accumul_memory_duration += memory_duration;
     }
-    accumul_len += k;
-  }
 
-  if (use_ramulator) {
-    v_cache->setShape({accumul_len, head_dim * num_kv_heads});
-    ExecStatus temp;
-    temp =
-        issueRamulator(device, LayerType::ATTENTION_GEN, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, v_cache);
-    exec_status += temp;
-    accumul_memory_duration = temp.memory_duration;
-  }  
-  else {
-    v_cache->setShape({accumul_len, head_dim * num_kv_heads});
-    ExecStatus temp;
-    temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, v_cache);
-    exec_status += temp;
+    exec_status.read_count += traffic[0] * num_kv_heads;
+    exec_status.l2_count += traffic[1] * num_kv_heads;
+    exec_status.l1_count += traffic[2] * num_kv_heads;   
+
+    accumul_len += k;
   }
 
   exec_status.total_duration +=
       std::max(accumul_compute_duration, accumul_memory_duration);
 
-  exec_status.compute_util = 1000.0 * 1000.0 * 1000.0 * total_flops /
-                             compute_peak_flops / exec_status.total_duration;
-  exec_status.memory_util = 1000.0 * 1000.0 * 1000.0 * total_memory_size /
-                            memory_bandwidth / exec_status.total_duration;
-
   exec_status.flops = total_flops;
   exec_status.memory_size = total_memory_size;
 
   return exec_status;
 };
 
+//// not used
 ExecStatus AttentionGenExecutionLogic(Device_Ptr device,
                                       std::vector<Tensor_Ptr> tensor,
                                       BatchedSequence::Ptr sequences_metadata,
@@ -330,6 +329,7 @@ ExecStatus AttentionGenExecutionLogic(Device_Ptr device,
   return exec_status;
 };
 
+//// not used
 ExecStatus AttentionGenExecutionPIM(Device_Ptr device,
                                     std::vector<Tensor_Ptr> tensor,
                                     BatchedSequence::Ptr sequences_metadata,
@@ -486,6 +486,7 @@ ExecStatus AttentionGenExecutionPIM(Device_Ptr device,
   return exec_status;
 };
 
+//// In this branch, we use only GPU-related code, neither LOGIC nor PIM
 ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
   std::vector<Tensor_Ptr> tensor,
   BatchedSequence::Ptr sequences_metadata,
@@ -539,6 +540,174 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
   time_ns accumul_compute_duration = 0;
   time_ns accumul_memory_duration = 0;
 
+  // Score //
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    seq = seq_list.at(seq_idx);
+
+    m = seq->num_process_token;
+    k = head_dim + qk_rope_head_dim;
+    n = seq->current_len + seq->num_process_token;
+
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m, n, k, input->precision_byte, tiles);
+
+    for (int head_idx = 0; head_idx < num_heads; head_idx++) {
+      flops = m * k * n * 2.0;
+      total_flops += flops;
+
+      memory_size = (m * k + k * n + m * n) * input->precision_byte;
+      total_memory_size += memory_size;
+
+      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+      exec_status.compute_duration += compute_duration;
+      accumul_compute_duration += compute_duration;
+
+      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+      accumul_memory_duration += memory_duration;
+    }
+
+    exec_status.read_count += traffic[0] * num_heads;
+    exec_status.l2_count += traffic[1] * num_heads;
+    exec_status.l1_count += traffic[2] * num_heads;    
+    
+    accumul_len += n;
+  }
+
+  exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
+
+  // Softmax //
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    time_ns compute_duration = 0;
+    time_ns memory_duration = 0;
+
+    seq = seq_list.at(seq_idx);
+
+    m = seq->num_process_token;
+    n = seq->current_len + seq->num_process_token;
+
+    flops = 7.0 * m * n * num_heads; // scale + mask + softmax
+    total_flops += flops;
+
+    memory_size = (2.0 * m * n * num_heads) * input->precision_byte;
+
+    compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+
+    exec_status.total_duration += std::max(compute_duration, memory_duration);
+  }
+
+  // Context //
+  accumul_len = 0;
+  accumul_compute_duration = 0;
+  accumul_memory_duration = 0;
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    seq = seq_list.at(seq_idx);
+
+    m = seq->num_process_token;
+    k = seq->current_len + seq->num_process_token;
+    n = head_dim;
+
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m, n, k, input->precision_byte, tiles);
+
+    for (int head_idx = 0; head_idx < num_heads; head_idx++) {
+      flops = m * k * n * 2.0; 
+      total_flops += flops;
+
+      memory_size = (m * k + k * n + m * n) * input->precision_byte;
+      total_memory_size += memory_size;
+
+      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+      exec_status.compute_duration += compute_duration;
+      accumul_compute_duration += compute_duration;
+
+      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+      accumul_memory_duration += memory_duration;
+    }
+
+    exec_status.read_count += traffic[0] * num_heads;
+    exec_status.l2_count += traffic[1] * num_heads;
+    exec_status.l1_count += traffic[2] * num_heads;     
+
+    accumul_len += k;
+  }
+
+  exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
+
+  exec_status.flops = total_flops;
+  exec_status.memory_size = total_memory_size;
+  input->setShape(orig_shape); // restore orig shape of input
+
+  return exec_status;
+};
+
+//// not used
+ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
+  std::vector<Tensor_Ptr> tensor,
+  BatchedSequence::Ptr sequences_metadata,
+  LayerInfo layer_info, bool use_ramulator) {
+
+  Tensor_Ptr input = tensor.at(0);
+  Tensor_Ptr k_cache = tensor.at(0);
+  Tensor_Ptr v_cache = tensor.at(0);
+  bool compressed_kv = true;
+  if(tensor.size() > 1){ // not use compressed_kv
+    compressed_kv = false;
+    k_cache = tensor.at(1);
+    v_cache = tensor.at(2);
+  }
+
+  auto config = device->config;
+  hw_metric compute_peak_flops =
+    config.logic_memory_bandwidth * config.logic_op_b;
+  hw_metric memory_bandwidth = config.logic_memory_bandwidth;
+  if(input->precision_byte == 1){
+    compute_peak_flops *= 2;
+  }
+
+  int head_dim = layer_info.head_dim;
+  int num_heads = layer_info.num_heads;
+  int num_kv_heads = layer_info.num_kv_heads;
+  int qk_rope_head_dim = layer_info.qk_rope_head_dim;
+  int use_flash_mla = layer_info.use_flash_mla;
+
+  time_ns time = 0;
+
+  int m, n, k;
+  double flops, memory_size;
+  double total_flops = 0;
+  double total_memory_size = 0;
+
+  time_ns compute_duration;
+  time_ns memory_duration;
+  time_ns total_duration = 0;
+
+  ExecStatus exec_status;
+  if (sequences_metadata->get_gen_process_token() == 0) {
+    return exec_status;
+  }
+
+  // Scoring //
+  std::vector<Sequence::Ptr> seq_list = sequences_metadata->get_gen();
+  Sequence::Ptr seq;
+  std::vector<int> orig_shape = input->shape;
+
+  std::vector<int> shape = {1, head_dim};
+
+  int num_seq = seq_list.size();
+
+  int accumul_len = 0;
+  time_ns accumul_compute_duration = 0;
+  time_ns accumul_memory_duration = 0;
+
   if(use_flash_mla){
     for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
       seq = seq_list.at(seq_idx);
@@ -574,22 +743,22 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
 
       // read query
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
 
       // read key
       input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
 
       // read value
       input->setShape({accumul_len, head_dim * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
 
       // write output
       input->setShape({num_seq, num_heads * head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
 
       exec_status += temp;
@@ -599,19 +768,19 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
 
@@ -647,15 +816,15 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
     if(use_ramulator) {
       ExecStatus temp;
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
 
       k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, k_cache);                        
 
       input->setShape({num_heads, accumul_len});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
 
       exec_status += temp;
@@ -665,15 +834,15 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, k_cache);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, k_cache);
       exec_status += temp;
 
       input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
 
@@ -704,13 +873,13 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
 
         // read input
         input->setShape({m, n * num_heads});
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                 DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
         exec_status += temp;
         memory_duration += temp.memory_duration;
 
         // store output
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                 DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
         exec_status += temp;
         memory_duration += temp.memory_duration;
@@ -720,11 +889,11 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
 
         // read input
         input->setShape({m, n * num_heads});
-        temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+        temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
         exec_status += temp;
 
         // store output
-        temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
+        temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
         exec_status += temp;
       }
 
@@ -764,19 +933,19 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
       
       v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, v_cache);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::GPU,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
@@ -785,15 +954,15 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
       ExecStatus temp;
       
       input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, v_cache);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, v_cache);
       exec_status += temp;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
     exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
@@ -810,7 +979,8 @@ ExecStatus MultiLatentAttentionGenExecutionGPU(Device_Ptr device,
   return exec_status;
 };
 
-ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
+//// not used
+ExecStatus MultiLatentAttentionGenExecutionPIM(Device_Ptr device,
   std::vector<Tensor_Ptr> tensor,
   BatchedSequence::Ptr sequences_metadata,
   LayerInfo layer_info, bool use_ramulator) {
@@ -826,9 +996,8 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
   }
 
   auto config = device->config;
-  hw_metric compute_peak_flops =
-    config.logic_memory_bandwidth * config.logic_op_b;
-  hw_metric memory_bandwidth = config.logic_memory_bandwidth;
+  hw_metric compute_peak_flops = config.pim_memory_bandwidth * config.pim_op_b;
+  hw_metric memory_bandwidth = config.pim_memory_bandwidth;
   if(input->precision_byte == 1){
     compute_peak_flops *= 2;
   }
@@ -903,22 +1072,22 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
 
       // read query
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
 
       // read key
       input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
 
       // read value
       input->setShape({accumul_len, head_dim * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
 
       // write output
       input->setShape({num_seq, num_heads * head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
 
       exec_status += temp;
@@ -928,26 +1097,25 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
 
     exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
   }
   else{
-
     // Score //
     for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
       seq = seq_list.at(seq_idx);
@@ -976,15 +1144,15 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
     if(use_ramulator) {
       ExecStatus temp;
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
 
       k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, k_cache);                        
 
       input->setShape({num_heads, accumul_len});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
 
       exec_status += temp;
@@ -994,15 +1162,15 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, k_cache);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, k_cache);
       exec_status += temp;
 
       input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
 
@@ -1033,13 +1201,13 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
 
         // read input
         input->setShape({m, n * num_heads});
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                 DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
         exec_status += temp;
         memory_duration += temp.memory_duration;
 
         // store output
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                 DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
         exec_status += temp;
         memory_duration += temp.memory_duration;
@@ -1049,11 +1217,11 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
 
         // read input
         input->setShape({m, n * num_heads});
-        temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+        temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
         exec_status += temp;
 
         // store output
-        temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
+        temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
         exec_status += temp;
       }
 
@@ -1093,19 +1261,19 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
       ExecStatus temp;
 
       input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
       
       v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kRead, PIMOperandType::kDRAM, v_cache);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::LOGIC,
+      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
                             DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
       exec_status += temp;
       accumul_memory_duration += temp.memory_duration;
@@ -1114,15 +1282,15 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
       ExecStatus temp;
       
       input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
       exec_status += temp;
 
       v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kRead, v_cache);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, v_cache);
       exec_status += temp;
 
       input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::LOGIC, DRAMRequestType::kWrite, input);
+      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
       exec_status += temp;
     }
     exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
@@ -1139,33 +1307,24 @@ ExecStatus MultiLatentAttentionGenExecutionLogic(Device_Ptr device,
   return exec_status;
 };
 
-ExecStatus MultiLatentAttentionGenExecutionPIM(Device_Ptr device,
+//// In this branch, we use only GPU-related code, neither LOGIC nor PIM
+ExecStatus AbsorbMLAGenExecutionGPU(Device_Ptr device,
   std::vector<Tensor_Ptr> tensor,
   BatchedSequence::Ptr sequences_metadata,
   LayerInfo layer_info, bool use_ramulator) {
 
   Tensor_Ptr input = tensor.at(0);
-  Tensor_Ptr k_cache = tensor.at(0);
-  Tensor_Ptr v_cache = tensor.at(0);
-  bool compressed_kv = true;
-  if(tensor.size() > 1){ // not use compressed_kv
-    compressed_kv = false;
-    k_cache = tensor.at(1);
-    v_cache = tensor.at(2);
-  }
 
   auto config = device->config;
-  hw_metric compute_peak_flops = config.pim_memory_bandwidth * config.pim_op_b;
-  hw_metric memory_bandwidth = config.pim_memory_bandwidth;
-  if(input->precision_byte == 1){
-    compute_peak_flops *= 2;
-  }
+  hw_metric compute_peak_flops = config.compute_peak_flops;
+  hw_metric memory_bandwidth = config.memory_bandwidth;
 
   int head_dim = layer_info.head_dim;
   int num_heads = layer_info.num_heads;
   int num_kv_heads = layer_info.num_kv_heads;
   int qk_rope_head_dim = layer_info.qk_rope_head_dim;
-  int use_flash_mla = layer_info.use_flash_mla;
+  int kv_lora_rank = layer_info.kv_lora_rank;
+  bool use_flash_mla = layer_info.use_flash_mla;
 
   time_ns time = 0;
 
@@ -1183,689 +1342,165 @@ ExecStatus MultiLatentAttentionGenExecutionPIM(Device_Ptr device,
     return exec_status;
   }
 
-  // Scoring //
   std::vector<Sequence::Ptr> seq_list = sequences_metadata->get_gen();
   Sequence::Ptr seq;
-  std::vector<int> orig_shape = input->shape;
 
-  std::vector<int> shape = {1, head_dim};
+  std::vector<int> orig_shape = input->shape;
 
   int num_seq = seq_list.size();
 
-  int accumul_len = 0;
+  int accumul_len = 0; // num_seq x seqLen
   time_ns accumul_compute_duration = 0;
   time_ns accumul_memory_duration = 0;
 
-  if(use_flash_mla){
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = head_dim + qk_rope_head_dim;
-      n = seq->current_len + seq->num_process_token;
-
-      accumul_len += n;
-
-      flops = 2.0 * m * (head_dim + qk_rope_head_dim) * n * num_heads + // score
-              2.0 * m * (head_dim) * n * num_heads; // context
-      total_flops += flops;
-
-
-      memory_size = 1.0 * (m * (head_dim + qk_rope_head_dim) + // query
-                    1.0 * n * (head_dim + qk_rope_head_dim) + // key
-                    1.0 * n * head_dim + // value
-                    1.0 * m * head_dim) * // output 
-                    num_heads * input->precision_byte;
-      total_memory_size += memory_size;
-
-      accumul_compute_duration += flops / compute_peak_flops * 1000 * 1000 * 1000;
-      exec_status.compute_duration += accumul_compute_duration;
-
-      accumul_memory_duration +=
-          memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-      exec_status.memory_duration += accumul_memory_duration;
-    }
-
-    if(use_ramulator) {
-      ExecStatus temp;
-
-      // read query
-      input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-
-      // read key
-      input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
-
-      // read value
-      input->setShape({accumul_len, head_dim * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                        
+  // Scoring for NoPE//
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    seq = seq_list.at(seq_idx);
 
-      // write output
-      input->setShape({num_seq, num_heads * head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
+    m = seq->num_process_token;
+    k = kv_lora_rank;
+    n = seq->current_len + seq->num_process_token;
 
-      exec_status += temp;
-      accumul_memory_duration = temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m * num_heads, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m * num_heads, n, k, input->precision_byte, tiles);
 
-      input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-      exec_status += temp;
+    flops = 1.0 * m * k * n * 2.0 * num_heads;
+    total_flops += flops;
 
-      input->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-      exec_status += temp;
+    memory_size = (1.0 * m * k * num_heads + 1.0 * k * n + 1.0 * m * n * num_heads) * input->precision_byte;
+    total_memory_size += memory_size;
 
-      input->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-      exec_status += temp;
+    exec_status.read_count += traffic[0];
+    exec_status.l2_count += traffic[1];
+    exec_status.l1_count += traffic[2];
 
-      input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
+    compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    exec_status.compute_duration += compute_duration;
+    accumul_compute_duration += compute_duration;
 
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+    accumul_memory_duration += memory_duration;
+    accumul_len += n;
   }
-  else{
-    // Score //
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = head_dim + qk_rope_head_dim;
-      n = seq->current_len + seq->num_process_token;
-
-      for (int head_idx = 0; head_idx < num_heads; head_idx++) {
-        flops = m * k * n * 2.0;
-        total_flops += flops;
-
-        memory_size = (m * k + k * n + m * n) * input->precision_byte;
-        total_memory_size += memory_size;
-
-        compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-        exec_status.compute_duration += compute_duration;
-        accumul_compute_duration += compute_duration;
 
-        memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-        accumul_memory_duration += memory_duration;
-      }
-      accumul_len += n;
-    }
-
-    if(use_ramulator) {
-      ExecStatus temp;
-      input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
+  exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
 
-      k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, k_cache);                        
+  // Scoring for RoPE//
+  accumul_len = 0;
+  accumul_compute_duration = 0;
+  accumul_memory_duration = 0;
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    seq = seq_list.at(seq_idx);
 
-      input->setShape({num_heads, accumul_len});
-      temp += issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
+    m = seq->num_process_token;
+    k = qk_rope_head_dim;
+    n = seq->current_len + seq->num_process_token;
 
-      exec_status += temp;
-      accumul_memory_duration = temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m * num_heads, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m * num_heads, n, k, input->precision_byte, tiles);
 
-      input->setShape({num_seq * num_heads, head_dim + qk_rope_head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-      exec_status += temp;
+    flops = 1.0 * m * k * n * 2.0 * num_heads;
+    total_flops += flops;
 
-      k_cache->setShape({accumul_len, (head_dim + qk_rope_head_dim) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, k_cache);
-      exec_status += temp;
+    memory_size = (1.0 * m * k * num_heads + 1.0 * k * n + 1.0 * m * n * num_heads) * input->precision_byte;
+    total_memory_size += memory_size;
 
-      input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
+    exec_status.read_count += traffic[0];
+    exec_status.l2_count += traffic[1];
+    exec_status.l1_count += traffic[2];
 
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
+    compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    exec_status.compute_duration += compute_duration;
+    accumul_compute_duration += compute_duration;
 
-    // Softmax //
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      time_ns compute_duration = 0;
-      time_ns memory_duration = 0;
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+    accumul_memory_duration += memory_duration;
+    
+    accumul_len += n;
+  }
+  
+  exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
 
-      seq = seq_list.at(seq_idx);
 
-      m = seq->num_process_token;
-      n = seq->current_len + seq->num_process_token;
+  // Scale + mask + Softmax //
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    time_ns compute_duration = 0;
+    time_ns memory_duration = 0;
 
-      flops = 7.0 * m * n * num_heads; // scale + mask + softmax
-      total_flops += flops;
+    seq = seq_list.at(seq_idx);
 
-      memory_size = (2.0 * m * n * num_heads) *input->precision_byte;
+    m = seq->num_process_token;
+    n = seq->current_len + seq->num_process_token;
 
-      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+    flops = 7.0 * m * n * num_heads; // scale + mask + softmax
+    total_flops += flops;
 
-      if(use_ramulator){
-        memory_duration = 0;
-        
-        ExecStatus temp;
+    memory_size = (2.0 * m * n * num_heads) * input->precision_byte;
+    total_memory_size += memory_size;
+    compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
 
-        // read input
-        input->setShape({m, n * num_heads});
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-        exec_status += temp;
-        memory_duration += temp.memory_duration;
+    exec_status.total_duration += std::max(compute_duration, memory_duration);;
+  }
 
-        // store output
-        temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-        exec_status += temp;
-        memory_duration += temp.memory_duration;
-      }
-      else{
-        ExecStatus temp;
+  // Context //
+  accumul_len = 0;
+  accumul_compute_duration = 0;
+  accumul_memory_duration = 0;
+  for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
+    seq = seq_list.at(seq_idx);
 
-        // read input
-        input->setShape({m, n * num_heads});
-        temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-        exec_status += temp;
+    m = seq->num_process_token;
+    k = seq->current_len + seq->num_process_token;
+    n = kv_lora_rank;
 
-        // store output
-        temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
-        exec_status += temp;
-      }
+    // Determine tile sizes
+    TileConfig tiles = getOptimalTiles(m * num_heads, n, k, input->precision_byte,
+                                       config.l1_cache_size,
+                                       config.l2_cache_size);
+    // Compute cache traffic
+    auto traffic = computeCacheTraffic(m * num_heads, n, k, input->precision_byte, tiles);
 
-      exec_status.total_duration += std::max(compute_duration, memory_duration);;
-    }
+    flops = 1.0 * m * k * n * 2.0 * num_heads;
+    total_flops += flops;
+    memory_size = (1.0 * m * k * num_heads + 1.0 * k * n +
+      1.0 * m * n * num_heads) * input->precision_byte;
+    total_memory_size += memory_size;
 
-    // Context //
-    accumul_len = 0;
-    accumul_compute_duration = 0;
-    accumul_memory_duration = 0;
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
+    exec_status.read_count += traffic[0];
+    exec_status.l2_count += traffic[1];
+    exec_status.l1_count += traffic[2];
 
-      m = seq->num_process_token;
-      k = seq->current_len + seq->num_process_token;
-      n = head_dim;
+    compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
+    exec_status.compute_duration += compute_duration;
+    accumul_compute_duration += compute_duration;
 
-      for (int head_idx = 0; head_idx < num_heads; head_idx++) {
-        flops = m * k * n * 2.0; 
-        total_flops += flops;
+    memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+    accumul_memory_duration += memory_duration;
+    accumul_len += k;
+  }
 
-        memory_size = (m * k + k * n + m * n) * input->precision_byte;
-        total_memory_size += memory_size;
+  exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
 
-        compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-        exec_status.compute_duration += compute_duration;
-        accumul_compute_duration += compute_duration;
+  // exec_status.total_duration = total_duration;
 
-        memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-        accumul_memory_duration += memory_duration;
-      }
-      accumul_len += k;
-    }
+  exec_status.flops = total_flops;
+  exec_status.memory_size = total_memory_size;
 
-    if (use_ramulator) {
-      accumul_memory_duration = 0;
-      ExecStatus temp;
+  input->setShape(orig_shape); // restore orig shape of input
 
-      input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-      
-      v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, v_cache);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      input->setShape({num_seq, num_heads * head_dim});
-      temp = issueRamulator(device, LayerType::MLA_GEN, ProcessorType::PIM,
-                            DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
-      
-      input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      v_cache->setShape({accumul_len, head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kRead, v_cache);
-      exec_status += temp;
-
-      input->setShape({num_seq, num_heads * head_dim});
-      temp = getIdealMemoryStatus(device, ProcessorType::PIM, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
-  }
-
-  exec_status.compute_util = 1000.0 * 1000.0 * 1000.0 * total_flops /
-  compute_peak_flops / exec_status.total_duration;
-  exec_status.memory_util = 1000.0 * 1000.0 * 1000.0 * total_memory_size /
-  memory_bandwidth / exec_status.total_duration;
-
-  exec_status.flops = total_flops;
-  exec_status.memory_size = total_memory_size;
-  input->setShape(orig_shape); // restore orig shape of input
-  return exec_status;
-};
-
-ExecStatus AbsorbMLAGenExecutionGPU(Device_Ptr device,
-  std::vector<Tensor_Ptr> tensor,
-  BatchedSequence::Ptr sequences_metadata,
-  LayerInfo layer_info, bool use_ramulator) {
-
-  Tensor_Ptr input = tensor.at(0);
-
-  auto config = device->config;
-  hw_metric compute_peak_flops = config.compute_peak_flops;
-  hw_metric memory_bandwidth = config.memory_bandwidth;
-
-  int head_dim = layer_info.head_dim;
-  int num_heads = layer_info.num_heads;
-  int num_kv_heads = layer_info.num_kv_heads;
-  int qk_rope_head_dim = layer_info.qk_rope_head_dim;
-  int kv_lora_rank = layer_info.kv_lora_rank;
-  bool use_flash_mla = layer_info.use_flash_mla;
-
-  time_ns time = 0;
-
-  int m, n, k;
-  double flops, memory_size;
-  double total_flops = 0;
-  double total_memory_size = 0;
-
-  time_ns compute_duration;
-  time_ns memory_duration;
-  time_ns total_duration = 0;
-
-  ExecStatus exec_status;
-  if (sequences_metadata->get_gen_process_token() == 0) {
-    return exec_status;
-  }
-
-  std::vector<Sequence::Ptr> seq_list = sequences_metadata->get_gen();
-  Sequence::Ptr seq;
-
-  std::vector<int> orig_shape = input->shape;
-
-  int num_seq = seq_list.size();
-
-  int accumul_len = 0; // num_seq x seqLen
-  time_ns accumul_compute_duration = 0;
-  time_ns accumul_memory_duration = 0;
-
-  if(use_flash_mla){
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = kv_lora_rank + qk_rope_head_dim;
-      n = seq->current_len + seq->num_process_token;
-
-      accumul_len += n;
-
-      flops = 2.0 * num_heads * (kv_lora_rank + qk_rope_head_dim) * n + // score
-              2.0 * num_heads * (kv_lora_rank) * n; // context
-      total_flops += flops;
-
-
-      memory_size = 1.0 * (num_heads * (kv_lora_rank + qk_rope_head_dim) + // query
-                    1.0 * n * (kv_lora_rank + qk_rope_head_dim) + // latent kv and pe cache
-                    1.0 * num_heads * kv_lora_rank) * // output 
-                    input->precision_byte;
-      total_memory_size += memory_size;
-
-      accumul_compute_duration += flops / compute_peak_flops * 1000 * 1000 * 1000;
-      exec_status.compute_duration += accumul_compute_duration;
-
-      accumul_memory_duration +=
-          memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-      exec_status.memory_duration += accumul_memory_duration;
-    }
-
-    if(use_ramulator) {
-      ExecStatus temp;
-
-      // read query
-      input->setShape({num_seq * num_heads, (kv_lora_rank + qk_rope_head_dim)});
-      temp += issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-
-      // read latent kv and pe cache
-      input->setShape({accumul_len, (kv_lora_rank + qk_rope_head_dim)});
-      temp += issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-                            DRAMRequestType::kRead, PIMOperandType::kDRAM, input);                                          
-
-      // write output
-      input->setShape({num_seq, num_heads * kv_lora_rank});
-      temp += issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-                            DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);                        
-
-      exec_status += temp;
-      accumul_memory_duration = temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
-
-      input->setShape({num_seq * num_heads, (kv_lora_rank + qk_rope_head_dim)});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      input->setShape({accumul_len, (kv_lora_rank + qk_rope_head_dim)});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      input->setShape({num_seq, num_heads * kv_lora_rank});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
-
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
-  }
-  else{
-    // Scoring for NoPE//
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = kv_lora_rank;
-      n = seq->current_len + seq->num_process_token;
-
-      flops = 1.0 * m * k * n * 2.0 * num_heads;
-      total_flops += flops;
-
-      memory_size = (1.0 * m * k * num_heads + 1.0 * k * n + 1.0 * m * n * num_heads) * input->precision_byte;
-      total_memory_size += memory_size;
-
-      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-      exec_status.compute_duration += compute_duration;
-      accumul_compute_duration += compute_duration;
-
-      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-      accumul_memory_duration += memory_duration;
-      accumul_len += n;
-    }
-
-    if (use_ramulator) {
-      accumul_memory_duration = 0;
-
-      ExecStatus temp;
-
-      // read input
-      input->setShape({num_seq, (1 * kv_lora_rank) * num_heads});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      // read compressed_KV
-      input->setShape({kv_lora_rank, accumul_len});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      // store intermediate value
-      input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
-
-      // read input
-      input->setShape({num_seq, (1 * kv_lora_rank) * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // read compressed_KV
-      input->setShape({kv_lora_rank, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // store intermediate value
-      input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
-
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
-
-    // Scoring for RoPE//
-    accumul_len = 0;
-    accumul_compute_duration = 0;
-    accumul_memory_duration = 0;
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = qk_rope_head_dim;
-      n = seq->current_len + seq->num_process_token;
-
-      flops = 1.0 * m * k * n * 2.0 * num_heads;
-      total_flops += flops;
-
-      memory_size = (1.0 * m * k * num_heads + 1.0 * k * n + 1.0 * m * n * num_heads) * input->precision_byte;
-      total_memory_size += memory_size;
-
-      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-      exec_status.compute_duration += compute_duration;
-      accumul_compute_duration += compute_duration;
-
-      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-      accumul_memory_duration += memory_duration;
-      
-      accumul_len += n;
-    }
-
-    if (use_ramulator) {
-      accumul_memory_duration = 0;
-
-      ExecStatus temp;
-
-      // read input
-      input->setShape({num_seq, qk_rope_head_dim * num_heads});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      // read latent_PE
-      input->setShape({qk_rope_head_dim, accumul_len});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      // store intermediate value
-      input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration = temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
-
-      // read input
-      input->setShape({num_seq, qk_rope_head_dim * num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // read latent_PE
-      input->setShape({qk_rope_head_dim, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // store intermediate value
-      input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
-    
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
-
-
-    // Scale + mask + Softmax //
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      time_ns compute_duration = 0;
-      time_ns memory_duration = 0;
-
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      n = seq->current_len + seq->num_process_token;
-
-      flops = 7.0 * m * n * num_heads; // scale + mask + softmax
-      total_flops += flops;
-
-      memory_size = (2.0 * m * n * num_heads) * input->precision_byte;
-      total_memory_size += memory_size;
-      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-
-      if(use_ramulator){
-        memory_duration = 0;
-        
-        ExecStatus temp;
-
-        // read input
-        input->setShape({m, n * num_heads});
-        temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-                DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-        exec_status += temp;
-        memory_duration += temp.memory_duration;
-
-        // store output
-        temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-                DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-        exec_status += temp;
-        memory_duration += temp.memory_duration;
-      }
-      else{
-        ExecStatus temp;
-
-        // read input
-        input->setShape({m, n * num_heads});
-        temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-        exec_status += temp;
-
-        // store output
-        temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
-        exec_status += temp;
-      }
-
-      exec_status.total_duration += std::max(compute_duration, memory_duration);;
-    }
-
-    // Context //
-    accumul_len = 0;
-    accumul_compute_duration = 0;
-    accumul_memory_duration = 0;
-    for (int seq_idx = 0; seq_idx < num_seq; seq_idx++) {
-      seq = seq_list.at(seq_idx);
-
-      m = seq->num_process_token;
-      k = seq->current_len + seq->num_process_token;
-      n = kv_lora_rank;
-
-      flops = 1.0 * m * k * n * 2.0 * num_heads;
-      total_flops += flops;
-      memory_size = (1.0 * m * k * num_heads + 1.0 * k * n +
-        1.0 * m * n * num_heads) * input->precision_byte;
-      total_memory_size += memory_size;
-
-      compute_duration = flops / compute_peak_flops * 1000 * 1000 * 1000;
-      exec_status.compute_duration += compute_duration;
-      accumul_compute_duration += compute_duration;
-
-      memory_duration = memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-      accumul_memory_duration += memory_duration;
-      accumul_len += k;
-    }
-
-    if (use_ramulator) {
-      accumul_memory_duration = 0;
-
-      ExecStatus temp;
-
-      // read score output
-      input->setShape({num_heads, accumul_len});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-      // read compressed_kv
-      input->setShape({accumul_len, kv_lora_rank});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-
-
-      // store context out
-      input->setShape({num_seq * kv_lora_rank, num_heads});
-      temp = issueRamulator(device, LayerType::ABSORBED_MLA_GEN, ProcessorType::GPU,
-              DRAMRequestType::kWrite, PIMOperandType::kDRAM, input);
-      exec_status += temp;
-      accumul_memory_duration += temp.memory_duration;
-    }
-    else{
-      ExecStatus temp;
-
-      // read score output
-      input->setShape({num_heads, accumul_len});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // read compressed_kv
-      input->setShape({accumul_len, kv_lora_rank});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-      exec_status += temp;
-
-      // store context out
-      input->setShape({num_seq * kv_lora_rank, num_heads});
-      temp = getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, input);
-      exec_status += temp;
-    }
-
-    exec_status.total_duration += std::max(accumul_compute_duration, accumul_memory_duration);
-  }
-
-  // exec_status.total_duration = total_duration;
-
-  exec_status.compute_util = 1000.0 * 1000.0 * 1000.0 * total_flops /
-  compute_peak_flops / exec_status.total_duration;
-  exec_status.memory_util = 1000.0 * 1000.0 * 1000.0 * total_memory_size /
-  memory_bandwidth / exec_status.total_duration;
-
-  exec_status.flops = total_flops;
-  exec_status.memory_size = total_memory_size;
-
-  input->setShape(orig_shape); // restore orig shape of input
   return exec_status;
 };
 
+//// not used
 ExecStatus AbsorbMLAGenExecutionLogic(Device_Ptr device,
   std::vector<Tensor_Ptr> tensor,
   BatchedSequence::Ptr sequences_metadata,
@@ -2270,6 +1905,7 @@ ExecStatus AbsorbMLAGenExecutionLogic(Device_Ptr device,
   return exec_status;
 };
 
+//// not used
 ExecStatus AbsorbMLAGenExecutionPIM(Device_Ptr device,
   std::vector<Tensor_Ptr> tensor,
   BatchedSequence::Ptr sequences_metadata,
diff --git src/hardware/cluster.cpp src/hardware/cluster.cpp
index e2de805..ae9191b 100644
--- src/hardware/cluster.cpp
+++ src/hardware/cluster.cpp
@@ -218,7 +218,6 @@ bool Cluster::checkMemorySize() {
                  
   std::cout << "Total: " << size / 1024.0 / 1024 / 1024 << "GB" << std::endl;
   if (size > config.memory_capacity) {
-    out_of_memory = true;
     if (config.exit_out_of_memory) {
       return true;
     } else if (config.mem_cap_limit == true){
@@ -349,53 +348,42 @@ bool Cluster::check_module_graph_remain() {
 
 void Cluster::exportToCSV(std::ofstream &csv, std::vector<Stat> &stat_list) {
   for (auto temp : stat_list) {
-    csv << std::to_string(temp.iter_info) << "," << std::to_string(temp.split)
-        << "," << temp.type << "," << std::to_string(temp.time) << ","
-        << std::to_string(temp.latency) << ","
-        << std::to_string(temp.queueing_delay) << ","
-        << std::to_string(temp.arrival_time) << ","
-        << std::to_string(temp.seq_queue_size) << ","
-        << std::to_string(temp.input_len) << ","
-        << std::to_string(temp.output_len) << ","
-        << std::to_string(temp.num_sum_iter) << ","
-        << std::to_string(temp.is_mixed) << ","
+    csv << std::to_string(temp.iter_info) << ","
+        << temp.type << ","
         << std::to_string(temp.batchsize) << ","
         << std::to_string(temp.process_token) << ","
         << std::to_string(temp.sum_seq) << "," << std::to_string(temp.gen_seq)
         << "," << std::to_string(temp.average_seq_len) << ","
-        << std::to_string(temp.sum_attention_opb) << ","
-        << std::to_string(temp.qkv_gen) << "," 
-        << std::to_string(temp.q_down_proj) << "," 
-        << std::to_string(temp.kv_down_proj) << ","
-        << std::to_string(temp.kr_proj) << ","
-        << std::to_string(temp.q_up_proj) << ","
-        << std::to_string(temp.qr_proj) << ","
-        << std::to_string(temp.kv_up_proj) << ","
-        << std::to_string(temp.tr_k_up_proj) << ","
-        << std::to_string(temp.v_up_proj) << ","
+        << "->" << ","
+        << std::to_string(temp.decoders - temp.moe_decoders) << "," // non-moe decoders
+        << std::to_string(temp.moe_decoders) << "," 
+        << std::to_string(temp.qkv_gen) << "," // MHA, GQA
+        << std::to_string(temp.q_down_proj) << "," // MLA of deepseek
+        << std::to_string(temp.kv_down_proj) << "," // MLA of deepseek
+        << std::to_string(temp.kr_proj) << "," // MLA of deepseek
+        << std::to_string(temp.q_up_proj) << "," // MLA of deepseek
+        << std::to_string(temp.qr_proj) << "," // MLA of deepseek
+        << std::to_string(temp.kv_up_proj) << "," // MLA of deepseek
+        << std::to_string(temp.tr_k_up_proj) << "," // MLA of deepseek
+        << std::to_string(temp.v_up_proj) << "," // MLA of deepseek
         << std::to_string(temp.atten_sum)
-        << "," << std::to_string(temp.atten_gen) << ","
+        << "," << std::to_string(temp.atten_gen) << "," // Score, softmax, and Context
         << std::to_string(temp.o_proj) << "," << std::to_string(temp.ffn) << ","
-        << std::to_string(temp.expert_ffn) << ","
-        << std::to_string(temp.communication) << ","
-        << std::to_string(temp.rope) << ","
-        << std::to_string(temp.layernorm) << ","
-        << std::to_string(temp.residual) << ","
-        << std::to_string(temp.act_energy) << ","
-        << std::to_string(temp.read_energy) << ","
-        << std::to_string(temp.write_energy) << ","
-        << std::to_string(temp.all_act_energy) << ","
-        << std::to_string(temp.all_read_energy) << ","
-        << std::to_string(temp.all_write_energy) << ","
-        << std::to_string(temp.mac_energy) << ","
-        << std::to_string(temp.total_energy) << ","
+        << std::to_string(temp.shared_expert_ffn) << ","
+        << std::to_string(temp.expert_ffn) << "," // gated expert ffn
+        << std::to_string(temp.communication) << "," // inter-GPU
+        << "->" << ","
         << std::to_string(temp.FC_DRAM_energy) << ","
         << std::to_string(temp.FC_COMP_energy) << ","
         << std::to_string(temp.Attn_DRAM_energy) << ","
         << std::to_string(temp.Attn_COMP_energy) << ","
-        << std::to_string(temp.MoE_DRAM_energy) << ","
-        << std::to_string(temp.MoE_COMP_energy) << ","
-        << std::to_string(temp.isOOM) << std::endl;
+        << std::to_string(temp.FFN_DRAM_energy) << "," // non-MoE FFN
+        << std::to_string(temp.FFN_COMP_energy) << "," // non-MoE FFN
+        << std::to_string(temp.shared_MoE_HBM_energy) << ","
+        << std::to_string(temp.shared_MoE_COMP_energy) << ","
+        << std::to_string(temp.MoE_HBM_energy) << "," // gated expert
+        << std::to_string(temp.MoE_OFFLOAD_energy) << "," // gated expert
+        << std::to_string(temp.MoE_COMP_energy) << std::endl; // gated expert
   }
   stat_list.resize(0);
 }
@@ -404,14 +392,10 @@ std::vector<Stat> Cluster::runIteration(int iter, std::string file_name) {
   std::ofstream csv;
   csv.open(file_name);
 
-  csv << "iter_info,split,type,time,latency,queueing_delay,arrival_time,seq_queue_"
-         "size,"
-         "input_len,output_len,num_sum_iter,mixed,batchsize,numtoken,num_sum_"
-         "seq,num_gen_seq,seqlen,sum_attention_opb,qkvgen,q_down_proj,kv_down_proj,kr_proj,"
+  csv << "iter_info,type,batchsize,numtoken,num_sum_"
+         "seq,num_gen_seq,seqlen(Lin+Lout),##Latency,non_MoE_decoders,MoE_decoders,qkvgen,q_down_proj,kv_down_proj,kr_proj,"
          "q_up_proj,qr_proj,kv_up_proj,tr_k_up_proj,v_up_proj,atten_sum,atten_gen,"
-         "o_proj,ffn,expert_ffn,communication,rope,layernorm,residual,act_energy,read_energy,write_"
-         "energy,all_act_energy,all_read_energy,all_write_energy,mac_energy,"
-         "total_energy,fc_dram,fc_comp,attn_dram,attn_comp,moe_dram,moe_comp,OOM"
+         "o_proj,ffn,shared_expert_ffn,expert_ffn,communication,##Energy,fc_dram,fc_comp,attn_dram,attn_comp,ffn_dram,ffn_comp,shared_moe_hbm,shared_moe_comp,moe_hbm,moe_offload,moe_comp"
       << std::endl;
 
   std::vector<Stat> stat_list;
@@ -420,7 +404,7 @@ std::vector<Stat> Cluster::runIteration(int iter, std::string file_name) {
   scheduler->fillRunningQueue();
 
   // hitting
-  scheduler->hittingQueue(10000);
+  //scheduler->hittingQueue(10000);
 
   if (config.disagg_system) {
     stat_list = runIterationSumGenSplit(iter, csv);
@@ -428,7 +412,7 @@ std::vector<Stat> Cluster::runIteration(int iter, std::string file_name) {
     stat_list = runIterationMixed(iter, csv);
   }
 
-  std::cout << "Total: " << std::to_string(scheduler->total_time) << std::endl;
+  //std::cout << "Total: " << std::to_string(scheduler->total_time) << std::endl;
   std::cout << file_name << std::endl;
   csv.close();
 
@@ -663,26 +647,36 @@ void Cluster::setStat(Stat &stat) {
 void Cluster::setTimeBreakDown(Stat &stat) {
   TimeBoard &timeboard = get_device(0)->top_module_graph->timeboard;
 
-  if(scheduler->model_config.qk_nope_head_dim == 0){
-    std::vector<TimeStamp *> QKV_gen;    // GPU
-    std::vector<TimeStamp *> AttnSum;    // GPU
-    std::vector<TimeStamp *> AttnGen;    // PIM or Logic
-    std::vector<TimeStamp *> O_proj;     // GPU
-    std::vector<TimeStamp *> FFN;        // PIM or Logic
-    std::vector<TimeStamp *> ExpertFFN;  // PIM or Logic
-    std::vector<TimeStamp *> Comm;       // PIM or Logic
+  if(scheduler->model_config.qk_nope_head_dim == 0){ // model using MHA, GQA
+    std::vector<TimeStamp *> Decoders; // non-MoE decoder
+    std::vector<TimeStamp *> MoEDecoders;
+    std::vector<TimeStamp *> QKV_gen;    
+    std::vector<TimeStamp *> AttnSum;    
+    std::vector<TimeStamp *> AttnGen;    
+    std::vector<TimeStamp *> O_proj;     
+    std::vector<TimeStamp *> FFN;        
+    std::vector<TimeStamp *> SharedExpertFFN;
+    std::vector<TimeStamp *> CommInSharedExpertFFN; // AllReduce after shared expert ffn
+    std::vector<TimeStamp *> ExpertFFN; // gated expert ffn + shared expert ffn
+    std::vector<TimeStamp *> Comm;       
     std::vector<TimeStamp *> CommInExpertFFN;
 
     std::vector<TimeStamp *> RoPE;
     std::vector<TimeStamp *> LayerNorm;
     std::vector<TimeStamp *> Residual;
 
+    //// The timeboard gathers records of stamps whose strings **include** a specified substring and,
+    //// after resolving their inclusion hierarchy, logs the corresponding values for each layer in stat.
+    timeboard.find_stamp("decoder_", Decoders);
+    timeboard.find_stamp("MoE_decoder_", MoEDecoders);
     timeboard.find_stamp("attn_qkv_proj", QKV_gen);
     timeboard.find_stamp("AttentionSum", AttnSum);
     timeboard.find_stamp("AttentionGen", AttnGen);
     timeboard.find_stamp("attn_o_proj", O_proj);
     timeboard.find_stamp("feedforward", FFN);
-    timeboard.find_stamp("expertFFN", ExpertFFN);
+    timeboard.find_stamp("shared_expert_FFN", SharedExpertFFN);
+    timeboard.find_stamp("shared_moe_all_reduce", CommInSharedExpertFFN);
+    timeboard.find_stamp("expertFFN", ExpertFFN); // gated expert ffn + shared expert ffn
     timeboard.find_stamp("moe_scatter", CommInExpertFFN);
     timeboard.find_stamp("moe_all_reduce_for_e_tp", CommInExpertFFN);
     timeboard.find_stamp("moe_all_reduce_for_gather", CommInExpertFFN);
@@ -700,12 +694,16 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     timeboard.find_stamp("residual_1", Residual);
     timeboard.find_stamp("residual_2", Residual);
 
+    time_ns decoders = 0;
+    time_ns moe_decoders = 0;
     time_ns qkv_gen = 0;
     time_ns atten_sum = 0;
     time_ns atten_gen = 0;
     time_ns o_proj = 0;
     time_ns ffn = 0;
+    time_ns shared_expert_ffn = 0;
     time_ns expert_ffn = 0;
+    time_ns comm_in_shared_expert_ffn = 0;
     time_ns comm_in_expert_ffn = 0;
     time_ns communication = 0;
 
@@ -715,11 +713,27 @@ void Cluster::setTimeBreakDown(Stat &stat) {
 
     energy_nJ FC_DRAM = 0;
     energy_nJ FC_COMP = 0;
-    energy_nJ MoE_DRAM = 0;
+    energy_nJ FFN_DRAM = 0;
+    energy_nJ FFN_COMP = 0;
+    energy_nJ Shared_MoE_HBM = 0;
+    energy_nJ Shared_MoE_OFFLOAD = 0;
+    energy_nJ Shared_MoE_COMP = 0;
+    energy_nJ MoE_HBM = 0;
+    energy_nJ MoE_OFFLOAD = 0;
     energy_nJ MoE_COMP = 0;
     energy_nJ Attn_DRAM = 0;
     energy_nJ Attn_COMP = 0;
 
+    for (auto stamp : Decoders) {
+      decoders += stamp->get_duration();
+    }
+    stat.decoders = decoders;
+
+    for (auto stamp : MoEDecoders) {
+      moe_decoders += stamp->get_duration();
+    }
+    stat.moe_decoders = moe_decoders;
+
     for (auto stamp : QKV_gen) {
       qkv_gen += stamp->get_duration();
       FC_DRAM += stamp->getDramEnergy() * num_total_device;
@@ -750,33 +764,49 @@ void Cluster::setTimeBreakDown(Stat &stat) {
 
     for (auto stamp : FFN) {
       ffn += stamp->get_duration();
-      FC_DRAM += stamp->getDramEnergy() * num_total_device;
-      FC_COMP += stamp->getCompEnergy() * num_total_device;
+      FFN_DRAM += stamp->getDramEnergy() * num_total_device;
+      FFN_COMP += stamp->getCompEnergy() * num_total_device;
     }
     stat.ffn = ffn;
 
+    for (auto stamp : SharedExpertFFN) {
+      shared_expert_ffn += stamp->get_duration();
+      Shared_MoE_HBM += stamp->getMoeHbmEnergy() * num_total_device;
+      Shared_MoE_OFFLOAD += stamp->getMoeOffloadEnergy() * num_total_device;
+      Shared_MoE_COMP += stamp->getCompEnergy() * num_total_device;
+    }
+    for (auto stamp : CommInSharedExpertFFN) {
+      comm_in_shared_expert_ffn += stamp->get_duration();
+    }
+    shared_expert_ffn -= comm_in_shared_expert_ffn; // comm_in_shared_expert_ffn will be aggregated into stat.communication 
+    stat.shared_expert_ffn = shared_expert_ffn;
+    stat.shared_MoE_HBM_energy = Shared_MoE_HBM;
+    stat.shared_MoE_COMP_energy = Shared_MoE_COMP;
+
     for (auto stamp : ExpertFFN) {
       expert_ffn += stamp->get_duration();
     }
-
     // expertFFN may have different energy by device
     for(int device_id = 0; device_id < num_total_device; device_id ++){
       TimeBoard &timeboard_temp = get_device(device_id)->top_module_graph->timeboard;
-      std::vector<TimeStamp *> ExpertFFN_temp;  // PIM or Logic
+      std::vector<TimeStamp *> ExpertFFN_temp;
       timeboard_temp.find_stamp("expertFFN", ExpertFFN_temp);
       for (auto stamp : ExpertFFN_temp) {
-        MoE_DRAM += stamp->getDramEnergy();
+        MoE_HBM += stamp->getMoeHbmEnergy();
+        MoE_OFFLOAD += stamp->getMoeOffloadEnergy();
         MoE_COMP += stamp->getCompEnergy();
       }
     }
-
+    // MoE - shared_MoE = gated_MoE
+    MoE_HBM -= Shared_MoE_HBM;
+    MoE_OFFLOAD -= Shared_MoE_OFFLOAD;
+    MoE_COMP -= Shared_MoE_COMP;
     for (auto stamp : CommInExpertFFN) {
       comm_in_expert_ffn += stamp->get_duration();
     }
+    stat.expert_ffn = expert_ffn - shared_expert_ffn - comm_in_shared_expert_ffn - comm_in_expert_ffn; // to measure gated-expertFFN-only, exclude shared-expertFFN and communication
 
-    stat.expert_ffn = expert_ffn - comm_in_expert_ffn;
-
-    for (auto stamp : Comm) {
+    for (auto stamp : Comm) { // collects all communication in Decoder
       communication += stamp->get_duration();
     }
     stat.communication = communication;
@@ -800,8 +830,11 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     stat.FC_COMP_energy = FC_COMP;
     stat.Attn_DRAM_energy = Attn_DRAM;
     stat.Attn_COMP_energy = Attn_COMP;
-    stat.MoE_DRAM_energy = MoE_DRAM;
-    stat.MoE_COMP_energy = MoE_COMP;
+    stat.FFN_DRAM_energy = FFN_DRAM;
+    stat.FFN_COMP_energy = FFN_COMP;
+    stat.MoE_HBM_energy = MoE_HBM; // gated_MoE
+    stat.MoE_OFFLOAD_energy = MoE_OFFLOAD; // gated_MoE
+    stat.MoE_COMP_energy = MoE_COMP; // gated_MoE
     
     double opb = 0;
     for (auto stamp : AttnSum) {
@@ -813,8 +846,9 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     }
     stat.sum_attention_opb = opb;
   }
-  else{ // if Use MLA
-    std::vector<TimeStamp *> Decoders;    
+  else{ // model using MLA of deepseek
+    std::vector<TimeStamp *> Decoders; // non-MoE decoder
+    std::vector<TimeStamp *> MoEDecoders;
     std::vector<TimeStamp *> Q_down;    
     std::vector<TimeStamp *> KV_down;    
     std::vector<TimeStamp *> KR_proj;    
@@ -836,12 +870,16 @@ void Cluster::setTimeBreakDown(Stat &stat) {
 
     std::vector<TimeStamp *> O_proj;     
 
-    std::vector<TimeStamp *> FFN;        
-    std::vector<TimeStamp *> ExpertFFN;  
+    std::vector<TimeStamp *> FFN;
+    std::vector<TimeStamp *> SharedExpertFFN;        
+    std::vector<TimeStamp *> CommInSharedExpertFFN; // AllReduce after shared expert ffn
+    std::vector<TimeStamp *> ExpertFFN; // gated expert ffn + shared expert ffn
     std::vector<TimeStamp *> Comm;       
     std::vector<TimeStamp *> CommInExpertFFN;
     std::vector<TimeStamp *> Test;
 
+    //// The timeboard gathers records of stamps whose strings **include** a specified substring and,
+    //// after resolving their inclusion hierarchy, logs the corresponding values for each layer in stat.
     timeboard.find_stamp("attn_q_down_proj", Q_down);
     timeboard.find_stamp("attn_kv_down_proj", KV_down);
     timeboard.find_stamp("attn_kr_proj", KR_proj);
@@ -860,7 +898,9 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     timeboard.find_stamp("attn_o_proj", O_proj);
 
     timeboard.find_stamp("feedforward", FFN);
-    timeboard.find_stamp("expertFFN", ExpertFFN);
+    timeboard.find_stamp("shared_expert_FFN", SharedExpertFFN);
+    timeboard.find_stamp("shared_moe_all_reduce", CommInSharedExpertFFN);
+    timeboard.find_stamp("expertFFN", ExpertFFN); // gated expert ffn + shared expert ffn
     timeboard.find_stamp("moe_scatter", CommInExpertFFN);
     timeboard.find_stamp("moe_all_reduce_for_e_tp", CommInExpertFFN);
     timeboard.find_stamp("moe_all_reduce_for_gather", CommInExpertFFN);
@@ -881,7 +921,10 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     timeboard.find_stamp("residual_2", Residual);
 
     timeboard.find_stamp("decoder_", Decoders);
+    timeboard.find_stamp("MoE_decoder_", MoEDecoders);
 
+    time_ns decoders = 0;
+    time_ns moe_decoders = 0;    
     time_ns q_down_proj = 0;
     time_ns kv_down_proj = 0;
     time_ns kr_proj = 0;
@@ -899,7 +942,9 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     time_ns atten_gen = 0;
     time_ns o_proj = 0;
     time_ns ffn = 0;
+    time_ns shared_expert_ffn = 0;
     time_ns expert_ffn = 0;
+    time_ns comm_in_shared_expert_ffn = 0;
     time_ns comm_in_expert_ffn = 0;
     time_ns communication = 0;
     
@@ -909,11 +954,27 @@ void Cluster::setTimeBreakDown(Stat &stat) {
 
     energy_nJ FC_DRAM = 0;
     energy_nJ FC_COMP = 0;
-    energy_nJ MoE_DRAM = 0;
+    energy_nJ FFN_DRAM = 0;
+    energy_nJ FFN_COMP = 0;
+    energy_nJ Shared_MoE_HBM = 0;
+    energy_nJ Shared_MoE_OFFLOAD = 0;
+    energy_nJ Shared_MoE_COMP = 0;
+    energy_nJ MoE_HBM = 0;
+    energy_nJ MoE_OFFLOAD = 0;
     energy_nJ MoE_COMP = 0;
     energy_nJ Attn_DRAM = 0;
     energy_nJ Attn_COMP = 0;
 
+    for (auto stamp : Decoders) {
+      decoders += stamp->get_duration();
+    }
+    stat.decoders = decoders;
+
+    for (auto stamp : MoEDecoders) {
+      moe_decoders += stamp->get_duration();
+    }
+    stat.moe_decoders = moe_decoders;
+
     for (auto stamp : Q_down){
       q_down_proj += stamp->get_duration();
       Attn_DRAM += stamp->getDramEnergy() * num_total_device;
@@ -993,33 +1054,49 @@ void Cluster::setTimeBreakDown(Stat &stat) {
 
     for (auto stamp : FFN) {
       ffn += stamp->get_duration();
-      FC_DRAM += stamp->getDramEnergy() * num_total_device;
-      FC_COMP += stamp->getCompEnergy() * num_total_device;
+      FFN_DRAM += stamp->getDramEnergy() * num_total_device;
+      FFN_COMP += stamp->getCompEnergy() * num_total_device;
     }
     stat.ffn = ffn;
 
+    for (auto stamp : SharedExpertFFN) {
+      shared_expert_ffn += stamp->get_duration();
+      Shared_MoE_HBM += stamp->getMoeHbmEnergy() * num_total_device;
+      Shared_MoE_OFFLOAD += stamp->getMoeOffloadEnergy() * num_total_device;
+      Shared_MoE_COMP += stamp->getCompEnergy() * num_total_device;
+    }
+    for (auto stamp : CommInSharedExpertFFN) {
+      comm_in_shared_expert_ffn += stamp->get_duration();
+    }
+    shared_expert_ffn -= comm_in_shared_expert_ffn; // comm_in_shared_expert_ffn will be aggregated into stat.communication 
+    stat.shared_expert_ffn = shared_expert_ffn;
+    stat.shared_MoE_HBM_energy = Shared_MoE_HBM;
+    stat.shared_MoE_COMP_energy = Shared_MoE_COMP;
+
     for (auto stamp : ExpertFFN) {
       expert_ffn += stamp->get_duration();
     }
-
     // expertFFN may have different energy by device
     for(int device_id = 0; device_id < num_total_device; device_id ++){
       TimeBoard &timeboard_temp = get_device(device_id)->top_module_graph->timeboard;
-      std::vector<TimeStamp *> ExpertFFN_temp;  // PIM or Logic
+      std::vector<TimeStamp *> ExpertFFN_temp;
       timeboard_temp.find_stamp("expertFFN", ExpertFFN_temp);
       for (auto stamp : ExpertFFN_temp) {
-        MoE_DRAM += stamp->getDramEnergy();
+        MoE_HBM += stamp->getMoeHbmEnergy();
+        MoE_OFFLOAD += stamp->getMoeOffloadEnergy();
         MoE_COMP += stamp->getCompEnergy();
       }
     }
-
+    // MoE - shared-MoE = gated-MoE
+    MoE_HBM -= Shared_MoE_HBM;
+    MoE_OFFLOAD -= Shared_MoE_OFFLOAD;
+    MoE_COMP -= Shared_MoE_COMP;
     for (auto stamp : CommInExpertFFN) {
       comm_in_expert_ffn += stamp->get_duration();
     }
+    stat.expert_ffn = expert_ffn - shared_expert_ffn - comm_in_shared_expert_ffn - comm_in_expert_ffn; // to measure gated-expertFFN-only, exclude shared-expertFFN and communication
 
-    stat.expert_ffn = expert_ffn - comm_in_expert_ffn;
-
-    for (auto stamp : Comm) {
+    for (auto stamp : Comm) { // collects all communication in Decoder
       communication += stamp->get_duration();
     }
     stat.communication = communication;
@@ -1043,9 +1120,11 @@ void Cluster::setTimeBreakDown(Stat &stat) {
     stat.FC_COMP_energy = FC_COMP;
     stat.Attn_DRAM_energy = Attn_DRAM;
     stat.Attn_COMP_energy = Attn_COMP;
-    stat.MoE_DRAM_energy = MoE_DRAM;
-    stat.MoE_COMP_energy = MoE_COMP;
-    stat.isOOM = out_of_memory;
+    stat.FFN_DRAM_energy = FFN_DRAM;
+    stat.FFN_COMP_energy = FFN_COMP;
+    stat.MoE_HBM_energy = MoE_HBM; // gated_MoE
+    stat.MoE_OFFLOAD_energy = MoE_OFFLOAD; // gated_MoE
+    stat.MoE_COMP_energy = MoE_COMP; // gated_MoE
     
     double opb = 0;
     for (auto stamp : AttnSum) {
diff --git src/hardware/hardware_config.h src/hardware/hardware_config.h
index 2b1885b..ab1ab57 100644
--- src/hardware/hardware_config.h
+++ src/hardware/hardware_config.h
@@ -1,4 +1,6 @@
 #pragma once
+
+#include <yaml-cpp/yaml.h>
 #include <string>
 #include <vector>
 
@@ -24,7 +26,7 @@ class SystemConfig {
                  hw_metric device_ict_latency = 3.0 * 1000,
                  hw_metric device_ict_bandwidth = 450.0 * 1000 * 1000 * 1000, 
                  hw_metric compute_peak_flops = 989.4 * 1000 * 1000 * 1000 * 1000,
-                 hw_metric memory_bandwidth = 3.352 * 1000 * 1000 * 1000 * 1000,
+                 hw_metric base_memory_bandwidth = 3.352 * 1000 * 1000 * 1000 * 1000,
                  hw_metric memory_capacity = 80.0 * 1024 * 1024 * 1024,\
                  int logic_x = 4,
                  hw_metric logic_op_b = 8,                 
@@ -60,6 +62,7 @@ class SystemConfig {
         device_ict_latency(device_ict_latency),
         device_ict_bandwidth(device_ict_bandwidth),
         compute_peak_flops(compute_peak_flops),
+        base_memory_bandwidth(base_memory_bandwidth),
         memory_bandwidth(memory_bandwidth),
         memory_capacity(memory_capacity),
         logic_x(logic_x),
@@ -86,12 +89,18 @@ class SystemConfig {
         use_inject_rate(use_inject_rate),
         request_per_second(request_per_second),
         num_cube(num_cube),
-        num_logic_cube(num_logic_cube){
+        num_logic_cube(num_logic_cube),
+        l1_cache_size(256.0 * 1024),
+        l2_cache_size(50.0 * 1024 * 1024) {
+          parseOffloadConfig();
+
+          memory_bandwidth = base_memory_bandwidth - offload_bandwidth;
+
           logic_memory_bandwidth = memory_bandwidth * logic_x;
           pim_memory_bandwidth = memory_bandwidth * pim_x;
-        };
+        }
 
-    SystemConfig& operator=(const SystemConfig& rhs) = default;
+  SystemConfig& operator=(const SystemConfig& rhs) = default;
 
   std::string gpu_gen;
 
@@ -108,17 +117,20 @@ class SystemConfig {
   hw_metric device_ict_bandwidth;  // B/s
 
   // Device specification
+  hw_metric l1_cache_size;
+  hw_metric l2_cache_size;
   hw_metric compute_peak_flops;
-  hw_metric memory_bandwidth;
-
+  hw_metric memory_bandwidth; // HBM read/write bandwidth - SSD read bandwidth
+  hw_metric base_memory_bandwidth; // HBM read/write bandwidth
   hw_metric memory_capacity;
+  hw_metric offload_bandwidth;
 
-  // Logic specification
+  // not used
   int logic_x;
   hw_metric logic_memory_bandwidth = memory_bandwidth * logic_x;
   hw_metric logic_op_b;
 
-  // PIM specifiaction
+  // not used
   int pim_x;
   hw_metric pim_memory_bandwidth = memory_bandwidth * pim_x;
   hw_metric pim_op_b;
@@ -132,21 +144,17 @@ class SystemConfig {
 
   bool communication_hiding = false;
 
-  bool disagg_system = true;
+  bool disagg_system = false;
   bool use_low_unit_moe_only = false;
   bool use_ramulator = false;
   
   bool exit_out_of_memory = false;
   bool mem_cap_limit = false;
 
-  bool use_flash_mla = true; 
-  bool use_flash_attention = true; 
-  bool reuse_kv_cache = true;
+  bool use_flash_mla = false; 
+  bool use_flash_attention = false; 
+  bool reuse_kv_cache = false;
   hw_metric kv_cache_reuse_rate; 
-  // this rate includes, 
-  // 1) how long does prompt share tokens with cached KV 
-  // 2) does prompt share tokens with cached KV
-  // because we select rate between [0, kv_cache_reuse_rate * 2), kv_cache_reuse_rate must be max 0.5
 
   bool prefill_mode = false; 
   bool decode_mode = false;
@@ -156,7 +164,26 @@ class SystemConfig {
 
   int num_cube; //8: for HBM3E (B100), 5 for HBM3 (H100)
   int num_logic_cube;
-  // Device
+
+  private:
+    void parseOffloadConfig() {
+      YAML::Node config = YAML::LoadFile("config.yaml");
+
+      bool offload_cpu = config["system"]["offload"]["offload_expert_to_cpu_memory"].as<bool>();
+      bool offload_ssd = config["system"]["offload"]["offload_expert_to_ssd"].as<bool>();
+
+      if (offload_cpu || offload_ssd) {
+        int gen = config["system"]["offload"]["offload_bandwidth"].as<int>();
+        if (gen == 4) {
+          offload_bandwidth = 450.0 * 1000 * 1000 * 1000;  // NVLink 4.0
+        } else if (gen == 5) {
+          offload_bandwidth = 900.0 * 1000 * 1000 * 1000;  // NVLink 5.0
+        }
+      }
+      else {
+        offload_bandwidth = 0.0;
+      }
+    }  
 };
 
 
@@ -169,7 +196,7 @@ static SystemConfig A100 = SystemConfig(
                  3.0 * 1000,                        // device_ict_latency, nvlink 3
                  150.0 * 1000 * 1000 * 1000,        // device_ict_bandwidth
                  312.0 * 1000 * 1000 * 1000 * 1000, // compute_peak_flops, FP16
-                 2.039 * 1000 * 1000 * 1000 * 1000, // memory_bandwidth
+                 2.039 * 1000 * 1000 * 1000 * 1000, // base_memory_bandwidth
                  80.0 * 1024 * 1024 * 1024,         // memory_capacity 
                  4,                                 // logic_x 
                  8,                                 // logic_op_b                 
@@ -207,7 +234,7 @@ static SystemConfig H100 = SystemConfig(
                  0.8 * 1000,                        // device_ict_latency
                  450.0 * 1000 * 1000 * 1000,        // device_ict_bandwidth, nvlink 4
                  989.4 * 1000 * 1000 * 1000 * 1000, // compute_peak_flops, FP16
-                 3.352 * 1000 * 1000 * 1000 * 1000, // memory_bandwidth
+                 3.352 * 1000 * 1000 * 1000 * 1000, // base_memory_bandwidth
                  80.0 * 1024 * 1024 * 1024,         // memory_capacity 
                  4,                                 // logic_x 
                  8,                                 // logic_op_b                 
@@ -245,7 +272,7 @@ static SystemConfig B100 = SystemConfig(
                   0.8 * 1000,                        // device_ict_latency, nvlink 5.0
                   900.0 * 1000 * 1000 * 1000,        // device_ict_bandwidth
                   1750.0 * 1000 * 1000 * 1000 * 1000,// compute_peak_flops, FP16
-                  8.000 * 1000 * 1000 * 1000 * 1000, // memory_bandwidth
+                  8.000 * 1000 * 1000 * 1000 * 1000, // base_memory_bandwidth
                   192.0 * 1024 * 1024 * 1024,        // memory_capacity 
                   4,                                 // logic_x 
                   8,                                 // logic_op_b                 
@@ -283,7 +310,7 @@ static SystemConfig B200 = SystemConfig(
                  0.8 * 1000,                        // device_ict_latency, nvlink 5.0
                  900.0 * 1000 * 1000 * 1000,        // device_ict_bandwidth
                  2250.0 * 1000 * 1000 * 1000 * 1000,// compute_peak_flops, FP16
-                 8.000 * 1000 * 1000 * 1000 * 1000, // memory_bandwidth
+                 8.000 * 1000 * 1000 * 1000 * 1000, // base_memory_bandwidth
                  192.0 * 1024 * 1024 * 1024,        // memory_capacity 
                  4,                                 // logic_x 
                  8,                                 // logic_op_b                 
diff --git src/hardware/linear_impl.cpp src/hardware/linear_impl.cpp
index c8f4309..9be0704 100644
--- src/hardware/linear_impl.cpp
+++ src/hardware/linear_impl.cpp
@@ -1,10 +1,18 @@
+#include <yaml-cpp/yaml.h>
 #include <memory>
 
+#include <algorithm>
+#include <cmath>
+#include <limits>
+#include <vector>
+#include <array>
+
 #include "common/type.h"
 #include "dram/dram_interface.h"
 #include "dram/dram_request.h"
 #include "hardware/layer_impl.h"
 #include "module/tensor.h"
+#include "hardware/tile.h"
 
 namespace llm_system {
 class DRAMRequest;
@@ -13,12 +21,14 @@ class Tensor;
 using Tensor_Ptr = std::shared_ptr<Tensor>;
 using DRAMRequest_Ptr = std::shared_ptr<DRAMRequest>;
 
+//// In this branch, we use only GPU-related code, neither LOGIC nor PIM
 ExecStatus LinearExecutionGPU(Device_Ptr device, Tensor_Ptr input,
                               Tensor_Ptr weight, Tensor_Ptr output,
                               bool use_ramulator) {
   auto config = device->config;
   hw_metric compute_peak_flops = config.compute_peak_flops;
   hw_metric memory_bandwidth = config.memory_bandwidth;
+  hw_metric offload_bandwidth = config.offload_bandwidth;
 
   double m = input->shape[0];
   double k = input->shape[1];
@@ -27,53 +37,79 @@ ExecStatus LinearExecutionGPU(Device_Ptr device, Tensor_Ptr input,
   hw_metric total_flops = 2.0 * m * k * n;
   hw_metric total_memory_size = (m * k + k * n + m * n) * weight->precision_byte;
 
-  time_ns compute_duration =
-      total_flops / compute_peak_flops * 1000 * 1000 * 1000;
-  time_ns memory_duration =
-      total_memory_size / memory_bandwidth * 1000 * 1000 * 1000;
-
   ExecStatus exec_status;
   if (input->getSize() == 0) {
     return exec_status;
   }
-
-  exec_status.compute_duration = compute_duration;
   
-  if (use_ramulator) {
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, weight);
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kWrite, PIMOperandType::kDRAM, output);
-  } else {
+  ///////////////////////////////////////////
+  ///////////////// latency /////////////////
+  YAML::Node set_config = YAML::LoadFile("config.yaml");
+  time_ns compute_duration =
+      total_flops / compute_peak_flops * 1000 * 1000 * 1000;
+  // MoE with offloading
+  if (set_config["system"]["offload"]["offload_expert_to_cpu_memory"].as<bool>() ||
+      set_config["system"]["offload"]["offload_expert_to_ssd"].as<bool>()) {
+      if (set_config["system"]["offload"]["collects_gpu_compute"].as<bool>()) { // collects GPU-compute latency
+        time_ns memory_duration =
+          total_memory_size / memory_bandwidth * 1000 * 1000 * 1000;
+        exec_status.memory_duration = memory_duration;
+        exec_status.compute_duration = compute_duration;
+        exec_status.total_duration =
+          std::max(exec_status.compute_duration, exec_status.memory_duration);
+      } else { // collects offloaded-weight copying latency
+        time_ns memory_duration =
+          (k * n) * weight->precision_byte / offload_bandwidth * 1000 * 1000 * 1000;
+        exec_status.total_duration = memory_duration;
+      }
+  }
+  // MoE without offloading (baseline)
+  else {
+    time_ns memory_duration =
+      total_memory_size / memory_bandwidth * 1000 * 1000 * 1000;
     exec_status.memory_duration = memory_duration;
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
-
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, weight);
-    
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, output);
+    exec_status.compute_duration = compute_duration;
+    exec_status.total_duration =
+      std::max(exec_status.compute_duration, exec_status.memory_duration);
   }
 
-  exec_status.total_duration =
-      std::max(exec_status.compute_duration, exec_status.memory_duration);
 
-  exec_status.compute_util = 1000.0 * 1000.0 * 1000.0 * total_flops /
-                             compute_peak_flops / exec_status.total_duration;
-  exec_status.memory_util = 1000.0 * 1000.0 * 1000.0 * total_memory_size /
-                            memory_bandwidth / exec_status.total_duration;
+  //////////////////////////////////////////
+  ///////////////// energy /////////////////  
+
+  // Determine tile sizes
+  TileConfig tiles = getOptimalTiles(m, n, k, weight->precision_byte,
+                                     config.l1_cache_size,
+                                     config.l2_cache_size);
+  // Compute cache traffic
+  auto traffic = computeCacheTraffic(m, n, k, weight->precision_byte, tiles);
+
+  exec_status.read_count = traffic[0];
+  exec_status.l2_count = traffic[1];
+  exec_status.l1_count = traffic[2];
+
+  if (set_config["system"]["offload"]["offload_expert_to_cpu_memory"].as<bool>() ||
+      set_config["system"]["offload"]["offload_expert_to_ssd"].as<bool>()) {
+    // MoE with offloading
+    if (set_config["system"]["offload"]["collects_gpu_compute"].as<bool>()) {
+      // collect GPU-compute
+      exec_status.moe_hbm_read_count = traffic[0];
+    } else {
+      // collect offlaoded-weight copying
+      exec_status.moe_offload_read_count = k * n * weight->precision_byte;      
+    }
+  } else {
+    // MoE without offloading
+    exec_status.moe_hbm_read_count = traffic[0];
+  }
 
   exec_status.flops = total_flops;
   exec_status.memory_size = total_memory_size;
 
-  exec_status.opb = total_flops / total_memory_size;
-
   return exec_status;
 };
 
+//// not used
 ExecStatus LinearExecutionLogic(Device_Ptr device, Tensor_Ptr input,
                                 Tensor_Ptr weight, Tensor_Ptr output,
                                 bool use_ramulator) {
@@ -138,6 +174,7 @@ ExecStatus LinearExecutionLogic(Device_Ptr device, Tensor_Ptr input,
   return exec_status;
 };
 
+//// not used
 ExecStatus LinearExecutionPIM(Device_Ptr device, Tensor_Ptr input,
                               Tensor_Ptr weight, Tensor_Ptr output,
                               bool use_ramulator) {
@@ -201,6 +238,7 @@ ExecStatus LinearExecutionPIM(Device_Ptr device, Tensor_Ptr input,
   return exec_status;
 };
 
+//// In this branch, we use only GPU-related code, neither LOGIC nor PIM
 ExecStatus BatchedLinearExecutionGPU(Device_Ptr device, Tensor_Ptr input,
                               Tensor_Ptr weight, Tensor_Ptr output,
                               bool use_ramulator, bool duplicated_input) {
@@ -221,70 +259,42 @@ ExecStatus BatchedLinearExecutionGPU(Device_Ptr device, Tensor_Ptr input,
   int k = input->shape[2];
   int n = weight->shape[2];
 
+  // Determine tile sizes
+  TileConfig tiles = getOptimalTiles(m, n, k, weight->precision_byte,
+                                     config.l1_cache_size,
+                                     config.l2_cache_size);
+  // Compute cache traffic
+  auto traffic = computeCacheTraffic(m, n, k, weight->precision_byte, tiles);
+
+
   hw_metric total_flops = 2.0 * m * k * n * 1.0 * num_heads;
-  
   hw_metric total_memory_size;
-  if(duplicated_input){
-    total_memory_size = 1.0 * (m * k + k * n * 1.0  * num_heads + m * n * 1.0  * num_heads) * weight->precision_byte;
-  }
-  else{
-    total_memory_size = 1.0 * (m * k + k * n + m * n) * 1.0  * num_heads * weight->precision_byte;
-  }
-
-  time_ns compute_duration =
-      total_flops / compute_peak_flops * 1000 * 1000 * 1000;
-  time_ns memory_duration =
-      total_memory_size / memory_bandwidth * 1000 * 1000 * 1000;
 
   ExecStatus exec_status;
   if (input->getSize() == 0) {
     return exec_status;
   }
 
-  exec_status.compute_duration = compute_duration;
+  total_memory_size = 1.0 * (m * k + k * n + m * n) * 1.0  * num_heads * weight->precision_byte;
 
-  if (use_ramulator) {
-    if(duplicated_input){
-      input->setShape({m, k});
-    }
-    else{
-      input->setShape({m, k * num_heads});
-    }
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, input);
-    
-    weight->setShape({k * num_heads, n});
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kRead, PIMOperandType::kDRAM, weight);
+  exec_status.read_count = traffic[0] * num_heads;
+  exec_status.l2_count = traffic[1] * num_heads;
+  exec_status.l1_count = traffic[2] * num_heads;
 
-    output->setShape({m, n * num_heads});
-    exec_status +=
-        issueRamulator(device, LayerType::LINEAR, ProcessorType::GPU,
-                       DRAMRequestType::kWrite, PIMOperandType::kDRAM, output);
-  } else {
-    exec_status.memory_duration = memory_duration;
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, input);
+  time_ns compute_duration =
+      total_flops / compute_peak_flops * 1000 * 1000 * 1000;
+  time_ns memory_duration =
+      total_memory_size / memory_bandwidth * 1000 * 1000 * 1000;
 
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kRead, weight);
-    
-    exec_status += getIdealMemoryStatus(device, ProcessorType::GPU, DRAMRequestType::kWrite, output);
-  }
+  exec_status.compute_duration = compute_duration;
+  exec_status.memory_duration = memory_duration;
 
   exec_status.total_duration =
       std::max(exec_status.compute_duration, exec_status.memory_duration);
 
-  exec_status.compute_util = 1000.0 * 1000.0 * 1000.0 * total_flops /
-                             compute_peak_flops / exec_status.total_duration;
-  exec_status.memory_util = 1000.0 * 1000.0 * 1000.0 * total_memory_size /
-                            memory_bandwidth / exec_status.total_duration;
-
   exec_status.flops = total_flops;
   exec_status.memory_size = total_memory_size;
 
-  exec_status.opb = total_flops / total_memory_size;
-
   input->setShape(input_orig_shape);
   weight->setShape(weight_orig_shape);
   output->setShape(output_orig_shape);
@@ -292,6 +302,7 @@ ExecStatus BatchedLinearExecutionGPU(Device_Ptr device, Tensor_Ptr input,
   return exec_status;
 };
 
+//// not used
 ExecStatus BatchedLinearExecutionLogic(Device_Ptr device, Tensor_Ptr input,
                                 Tensor_Ptr weight, Tensor_Ptr output,
                                 bool use_ramulator, bool duplicated_input) {
@@ -355,6 +366,7 @@ ExecStatus BatchedLinearExecutionLogic(Device_Ptr device, Tensor_Ptr input,
   return exec_status;
 };
 
+//// not used
 ExecStatus BatchedLinearExecutionPIM(Device_Ptr device, Tensor_Ptr input,
                               Tensor_Ptr weight, Tensor_Ptr output,
                               bool use_ramulator, bool duplicated_input) {
diff --git src/hardware/stat.h src/hardware/stat.h
index af0a0a7..fc44fb2 100644
--- src/hardware/stat.h
+++ src/hardware/stat.h
@@ -24,11 +24,14 @@ struct Stat {
   double sum_attention_opb = 0.0;
   int end_token = 0;
 
+  time_ns decoders = 0;
+  time_ns moe_decoders = 0;
   time_ns qkv_gen = 0;
   time_ns atten_sum = 0;
   time_ns atten_gen = 0;
   time_ns o_proj = 0;
   time_ns ffn = 0;
+  time_ns shared_expert_ffn = 0;
   time_ns expert_ffn = 0;
   time_ns communication = 0;
 
@@ -63,9 +66,15 @@ struct Stat {
   energy_nJ FC_COMP_energy = 0;
   energy_nJ Attn_DRAM_energy = 0;
   energy_nJ Attn_COMP_energy = 0;
-  energy_nJ MoE_DRAM_energy = 0;
-  energy_nJ MoE_COMP_energy = 0;
+  energy_nJ FFN_DRAM_energy = 0;
+  energy_nJ FFN_COMP_energy = 0;
+  energy_nJ shared_MoE_HBM_energy = 0;
+  energy_nJ shared_MoE_COMP_energy = 0;
+  energy_nJ MoE_HBM_energy = 0;
+  energy_nJ MoE_OFFLOAD_energy = 0;
+  energy_nJ MoE_COMP_energy = 0;  
 
+  
   bool isOOM = false;
 
   int is_mixed = 0; // wheter it is mixed stage or not
diff --git src/hardware/tile.h src/hardware/tile.h
new file mode 100644
index 0000000..e8a5965
--- /dev/null
+++ src/hardware/tile.h
@@ -0,0 +1,105 @@
+#pragma once
+#include <vector>
+#include <array>
+#include <cmath>
+#include <limits>
+#include <algorithm>
+
+struct TileConfig {
+  int l1_tm;
+  int l1_tn;
+  int l1_tk;
+  int l2_tm;
+  int l2_tn;
+  int l2_tk;
+};
+
+// Determine optimal L1/L2 tile sizes
+inline TileConfig getOptimalTiles(double m, double n, double k,
+                           int dbyte,
+                           double l1_cache_size,
+                           double l2_cache_size) {
+    std::vector<int> trange = {8, 16, 32, 64, 128, 192, 256, 320, 384, 448, 512};
+    int l1_tk = 32;
+    double min_cost = std::numeric_limits<double>::max();
+    int opt_l1_tm = trange[0], opt_l1_tn = trange[0];
+
+    // L1 tile search
+    for (int tm : trange) {
+        for (int tn : trange) {
+            int tm_eff = std::min<int>(m, tm);
+            int tn_eff = std::min<int>(n, tn);
+            double required = (double)(tm_eff + tn_eff) * l1_tk * dbyte
+                              + (double)tm_eff * tn_eff * dbyte;
+            if (required > l1_cache_size) continue;
+            // L2 access estimate
+            double l2_access = (std::ceil(n / (double)tn_eff) * m * k
+                              + std::ceil(m / (double)tm_eff) * n * k
+                              + m * n) * dbyte;
+            double cost = l2_access;
+            if (cost < min_cost) {
+                min_cost = cost;
+                opt_l1_tm = tm_eff;
+                opt_l1_tn = tn_eff;
+            }
+        }
+    }
+
+    // L2 tile search
+    int l2_tk = std::max(1, (int)(k / 64));
+    double min_access = std::numeric_limits<double>::max();
+    int opt_l2_tm = opt_l1_tm, opt_l2_tn = opt_l1_tn;
+
+    std::vector<int> tm2_cands;
+    int max_i = std::max<int>(1, (int)(m / opt_l1_tm));
+    for (int i = 1; i <= max_i; ++i) {
+        int tm2 = std::min<int>(m, opt_l1_tm * i);
+        tm2_cands.push_back(tm2);
+    }
+    tm2_cands.push_back((int)m);
+
+    std::vector<int> tn2_cands;
+    int max_j = std::max<int>(1, (int)(n / opt_l1_tn));
+    for (int j = 1; j <= max_j; ++j) {
+        int tn2 = std::min<int>((int)n, opt_l1_tn * j);
+        tn2_cands.push_back(tn2);
+    }
+    tn2_cands.push_back(n);
+
+    for (int tm2 : tm2_cands) {
+        for (int tn2 : tn2_cands) {
+            double required2 = (double)(tm2 + tn2) * l2_tk * dbyte + (double)tm2 * tn2 * dbyte;
+            if (required2 > l2_cache_size && !(tm2 == opt_l1_tm && tn2 == opt_l1_tn)) continue;
+            double access = (std::ceil(m / (double)tm2) * n * k
+                           + std::ceil(n / (double)tn2) * m * k
+                           + m * n) * dbyte;
+            if (access < min_access) {
+                min_access = access;
+                opt_l2_tm = tm2;
+                opt_l2_tn = tn2;
+            }
+        }
+    }
+
+    return {opt_l1_tm, opt_l1_tn, l1_tk, opt_l2_tm, opt_l2_tn, l2_tk};
+}
+
+// Compute DRAM, L2, and L1 traffic (in bytes)
+inline std::array<double, 3> computeCacheTraffic(double m, double n, double k,
+                                          int dbyte,
+                                          const TileConfig &tiles) {
+    // DRAM traffic using L2 tile
+    double dram = (std::ceil(m / (double)tiles.l2_tm) * n * k
+                 + std::ceil(n / (double)tiles.l2_tn) * m * k
+                 + m * n) * dbyte;
+    // L2 traffic using L1 tile
+    double l2  = (std::ceil(m / (double)tiles.l1_tm) * n * k
+                 + std::ceil(n / (double)tiles.l1_tn) * m * k
+                 + m * n) * dbyte;
+    // L1 traffic using small register tiles
+    const int reg_tm = 16, reg_tn = 16;
+    double l1 = (std::ceil(m / (double)reg_tm) * n * k
+               + std::ceil(n / (double)reg_tn) * m * k
+               + m * n) * dbyte;
+    return {dram, l2, l1};
+}
\ No newline at end of file
diff --git src/model/CMakeLists.txt src/model/CMakeLists.txt
index c4a4ffc..a19fad7 100644
--- src/model/CMakeLists.txt
+++ src/model/CMakeLists.txt
@@ -5,3 +5,16 @@ target_sources(
     
     llm.cpp   
 )
+
+add_executable(model-test test.cpp)
+
+target_link_libraries(
+    model-test
+    PRIVATE llm_system
+)
+
+set_target_properties(model-test PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
+
+target_link_libraries(model PRIVATE hardware)
diff --git src/model/llm.cpp src/model/llm.cpp
index 8def059..d8fb61b 100644
--- src/model/llm.cpp
+++ src/model/llm.cpp
@@ -19,7 +19,7 @@ LLM::LLM(const ModelConfig& model_config, Cluster::Ptr cluster,
                                            model_config, device_list, device);
   add_module(embedding_layer);
 
-  if ((model_config.model_name != "deepseekV3")) {
+  if ((model_config.model_name != "deepseekR1")) {
     for (int layer = 0; layer < model_config.num_layers; layer++) {
       if ((model_config.expert_freq != 0) && (layer % model_config.expert_freq == 0)) {
         // MoE decoder;
@@ -39,7 +39,7 @@ LLM::LLM(const ModelConfig& model_config, Cluster::Ptr cluster,
         add_module(decoder);
       }
     }
-  } else if ((model_config.model_name == "deepseekV3")) {
+  } else if ((model_config.model_name == "deepseekR1")) {
     for (int layer = 0; layer < model_config.first_k_dense; layer++) {
       auto decoder =
           Decoder::Create(module_map_name, "decoder_" + std::to_string(layer),
@@ -70,7 +70,7 @@ Tensor::Ptr LLM::forward(const Tensor::Ptr input,
   Module::Ptr embedding = get_module("Embedding_layer");
   temp = (*embedding)(input, sequences_metadata);
 
-  if ((model_config.model_name != "deepseekV3")){
+  if ((model_config.model_name != "deepseekR1")){
     for (int layer = 0; layer < model_config.num_layers; layer++) {
       if ((model_config.expert_freq != 0) && (layer % model_config.expert_freq == 0)){
         decoder = get_module("MoE_decoder_" + std::to_string(layer));
@@ -79,7 +79,7 @@ Tensor::Ptr LLM::forward(const Tensor::Ptr input,
       }
       out = (*decoder)(temp, sequences_metadata);
     }
-  } else if ((model_config.model_name == "deepseekV3")) {
+  } else if ((model_config.model_name == "deepseekR1")) {
     for (int layer = 0; layer < model_config.first_k_dense; layer++) {
       decoder = get_module("decoder_" + std::to_string(layer));
       out = (*decoder)(temp, sequences_metadata);
diff --git src/model/model_config.h src/model/model_config.h
index 21f2eb8..f9ddc01 100644
--- src/model/model_config.h
+++ src/model/model_config.h
@@ -85,38 +85,33 @@ class ModelConfig {
   int output_len;
 };
 
-static ModelConfig mixtral = ModelConfig(4096, 128, 32, 32, 8, 32768, 14336,
-                                         14336, 1, 2, 8, 0, 1, 2, 3, 0, 0, 0, 0, 0, 32000, false, false, "mixtral");
+static ModelConfig mixtral =
+    ModelConfig(4096, 128, 32, 32, 8, 32768, 14336, 14336, 1, 2, 8, 0, 1, 2, 3, 0, 0, 0, 0, 0, 32000, false, false, "mixtral");
 
-static ModelConfig openMoE = ModelConfig(
-    3072, 128, 32, 24, 24, 2048, 12288, 12288, 2, 2, 32, 0, 4, 2, 3, 0, 0, 0, 0, 0, 32000, false, false, "openMoE");
+static ModelConfig openMoE =
+    ModelConfig(3072, 128, 32, 24, 24, 2048, 12288, 12288, 2, 2, 32, 0, 4, 2, 3, 0, 0, 0, 0, 0, 32000, false, false, "openMoE");
 
 static ModelConfig llama7bMoE =
-    ModelConfig(4096, 128, 32, 32, 32, 4096, 11008, 688, 1, 2, 16, 0, 1, 2, 3, 0, 0, 0, 0, 0, 32000, false, false,
-                "llama7bMoE");
+    ModelConfig(4096, 128, 32, 32, 32, 4096, 11008, 688, 1, 2, 16, 0, 1, 2, 3, 0, 0, 0, 0, 0, 32000, false, false, "llama7bMoE");
 
-static ModelConfig grok1 = ModelConfig(6144, 128, 64, 48, 8, 8192, 32768, 32768,
-                                       1, 2, 8, 0, 1, 2, 3, 0, 0, 0, 0, 0, 131072, false, false, "grok1");
+static ModelConfig grok1 =
+    ModelConfig(6144, 128, 64, 48, 8, 8192, 32768, 32768, 1, 2, 8, 0, 1, 2, 3, 0, 0, 0, 0, 0, 131072, false, false, "grok1");
 
-static ModelConfig glam = ModelConfig(4096, 128, 32, 32, 32, 8192, 16384, 16384,
-                                      1, 2, 64, 0, 2, 2, 2, 0, 0, 0, 0, 0, 256000, false, false, "glam");
+static ModelConfig glam =
+    ModelConfig(4096, 128, 32, 32, 32, 8192, 16384, 16384, 1, 2, 64, 0, 2, 2, 2, 0, 0, 0, 0, 0, 256000, false, false, "glam");
 
-static ModelConfig deepseekV3 =
-    ModelConfig(7168, 128, 60, 128, 128, 131072, 18432, 2048, 1, 1, 256, 1, 1, 8,
-                3, 3, 1536, 512, 128, 64, 129280, true, true,"deepseekV3"); // n_layer = 60 (not consider MTP module)
+static ModelConfig deepseekR1 =
+    ModelConfig(7168, 128, 60, 128, 128, 131072, 18432, 2048, 1, 2, 256, 1, 1, 8, 3, 3, 1536, 512, 128, 64, 129280, true, true,"deepseekR1"); // n_layer = 60 (not consider MTP module)
+
+static ModelConfig llama3_70B =
+    ModelConfig(8192, 128, 80, 64, 8, 131072, 28672, 0, 1, 2, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 128256, false, false, "llama3_70B");
 
 static ModelConfig llama3_405B =
-    ModelConfig(16384, 128, 126, 128, 8, 131072, 53248, 53248, 1, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 128256, false, false,
-                "llama3_405B");
+    ModelConfig(16384, 128, 126, 128, 8, 131072, 53248, 53248, 1, 1, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 128256, false, false, "llama3_405B");
 
 static ModelConfig llama4_scout = // 16 Expert 
-    ModelConfig(5120, 128, 48, 40, 8, 10485760, 16384, 8192, 1, 2, 16, 1, 1, 1,
-                3, 0, 0, 0, 0, 0, 202048, false, false,"llama4_scout");
+    ModelConfig(5120, 128, 48, 40, 8, 10485760, 16384, 8192, 1, 2, 16, 1, 1, 1, 3, 0, 0, 0, 0, 0, 202048, false, false,"llama4_scout");
       
 static ModelConfig llama4_maverick = // 128 Expert 
-                ModelConfig(5120, 128, 48, 40, 8, 1048576, 16384, 8192, 1, 2, 128, 1, 2, 1,
-                            3, 0, 0, 0, 0, 0, 202048, false, false,"llama4_maverick");
-
-// if model_config.q_lora_rank != 0 -> MLA로
-
+    ModelConfig(5120, 128, 48, 40, 8, 1048576, 16384, 8192, 1, 2, 128, 1, 2, 1, 3, 0, 0, 0, 0, 0, 202048, false, false,"llama4_maverick");
 }  // namespace llm_system
\ No newline at end of file
diff --git src/module/CMakeLists.txt src/module/CMakeLists.txt
index 5c64b05..3be23a2 100644
--- src/module/CMakeLists.txt
+++ src/module/CMakeLists.txt
@@ -1,5 +1,7 @@
 add_library(module OBJECT)
 
+target_link_libraries(module PRIVATE yaml-cpp::yaml-cpp)
+
 target_compile_definitions(module PRIVATE DEBUG)
 
 target_sources(
@@ -24,4 +26,15 @@ target_sources(
     residual.cpp
     expert.cpp
     lm_head.cpp
-)
\ No newline at end of file
+)
+
+add_executable(timeboard-test test.cpp)
+
+target_link_libraries(
+    timeboard-test
+    PRIVATE llm_system
+)
+
+set_target_properties(timeboard-test PROPERTIES
+    RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+)
diff --git src/module/communication.cpp src/module/communication.cpp
index 155454e..4053e58 100644
--- src/module/communication.cpp
+++ src/module/communication.cpp
@@ -273,11 +273,6 @@ TensorVec MoEGather::forward(const TensorVec input_vec,
   intra_node_comm_size /= device->model_config.ne_tp_dg; // receive only (1 / tp_degree) tokens, and then all reduce
   inter_node_comm_size /= device->model_config.ne_tp_dg; // receive only (1 / tp_degree) tokens, and then all reduce
 
-  // FP8 dispatch && BF16 combine
-  if((device->model_config.model_name == "deepseekV3") && device->model_config.precision_byte == 1){
-    intra_node_comm_size *= 2;
-    inter_node_comm_size *= 2;
-  }
 
   if(intra_node_comm_size == 0 && inter_node_comm_size == 0){
     return input_vec;
diff --git src/module/expert.cpp src/module/expert.cpp
index 8119196..83e3809 100644
--- src/module/expert.cpp
+++ src/module/expert.cpp
@@ -90,12 +90,12 @@ ExpertFFN::ExpertFFN(std::string& prefix, std::string& name,
     if (model_config.ffn_way == 2) {
         auto shared_expert_ffn = FeedForward2Way::Create(
             module_map_name, "shared_expert_FFN_" + std::to_string(shared_expert_idx),
-            model_config, scheduler, non_moe_device_list, device, false, true, true); // Shared Expert use TP degree of non-moe
+            model_config, scheduler, non_moe_device_list, device, true, true, false); // Shared Expert use TP degree of non-moe
         add_module(shared_expert_ffn);
     } else if (model_config.ffn_way == 3) {
         auto shared_expert_ffn = FeedForward3Way::Create(
             module_map_name, "shared_expert_FFN_" + std::to_string(shared_expert_idx),
-            model_config, scheduler, non_moe_device_list, device, false, true, true); // Shared Expert use TP degree of non-moe
+            model_config, scheduler, non_moe_device_list, device, true, true, false); // Shared Expert use TP degree of non-moe
         add_module(shared_expert_ffn);
     }
   }
diff --git src/module/layer.cpp src/module/layer.cpp
index 891abaa..c0580a7 100644
--- src/module/layer.cpp
+++ src/module/layer.cpp
@@ -276,7 +276,7 @@ Tensor::Ptr MultiLatentAttention::forward(const Tensor::Ptr input,
     attn_input.push_back(query);
     attn_input.push_back(key_value);
     attn_input.push_back(q_rope_out);
-    attn_input.push_back(k_rope_out); // 실제로는 RoPE의 output이 여기 input으로 들어가야함. 
+    attn_input.push_back(k_rope_out);
 
     TensorVec attn_out = (*multi_latent_attention)(attn_input, sequences_metadata);
     Tensor::Ptr o_proj = (*attn_o_proj)(attn_out.at(0), sequences_metadata);
@@ -318,9 +318,9 @@ FeedForward2Way::FeedForward2Way(std::string& prefix, std::string& name,
       model_config.hidden_dim, device_list, device);
   add_module(ffn_down_proj);
 
-  if (perform_all_reduce) {
+  if (perform_all_reduce) { // only true when executing shared-expertFFN
     auto all_reduce =
-        AllReduce::Create(module_map_name, "all_reduce", device_list, device);
+        AllReduce::Create(module_map_name, "shared_moe_all_reduce", device_list, device);
     add_module(all_reduce);
   }
 }
@@ -336,8 +336,8 @@ Tensor::Ptr FeedForward2Way::forward(const Tensor::Ptr input,
   Tensor::Ptr ffn_down = (*ffn_down_proj)(act_out, sequences_metadata);
   Tensor::Ptr result;
 
-  if (perform_all_reduce) {
-    Module::Ptr all_reduce = get_module("all_reduce");
+  if (perform_all_reduce) { // only true when executing shared-expertFFN
+    Module::Ptr all_reduce = get_module("shared_moe_all_reduce");
     result = (*all_reduce)(ffn_down, sequences_metadata);
   } else {
     result = ffn_down;
@@ -384,9 +384,9 @@ FeedForward3Way::FeedForward3Way(std::string& prefix, std::string& name,
                                           intermediate_dim, device_list, device);
     add_module(w3);
 
-    if (perform_all_reduce) {
+    if (perform_all_reduce) { // only true when executing shared-expertFFN
       auto all_reduce =
-          AllReduce::Create(module_map_name, "all_reduce", device_list, device);
+          AllReduce::Create(module_map_name, "shared_moe_all_reduce", device_list, device);
       add_module(all_reduce);
     }
   }
@@ -426,8 +426,8 @@ Tensor::Ptr FeedForward3Way::forward(const Tensor::Ptr input,
   Tensor::Ptr ffn_out = (*down_proj)(up_proj_out, sequences_metadata);
 
   Tensor::Ptr result;
-  if (perform_all_reduce) {
-    Module::Ptr all_reduce = get_module("all_reduce");
+  if (perform_all_reduce) { // only true when executing shared-expertFFN
+    Module::Ptr all_reduce = get_module("shared_moe_all_reduce");
     result = (*all_reduce)(ffn_out, sequences_metadata);
   } else {
     result = ffn_out;
diff --git src/module/module_graph.cpp src/module/module_graph.cpp
index cc43672..11cd4d6 100644
--- src/module/module_graph.cpp
+++ src/module/module_graph.cpp
@@ -1,3 +1,4 @@
+#include <yaml-cpp/yaml.h>
 #include "module/module_graph.h"
 
 namespace llm_system {
@@ -255,48 +256,41 @@ void TopModuleGraph::set_pop_status() {
       exec_status.processor_type == ProcessorType::LOGIC ||
       exec_status.processor_type == ProcessorType::GPU) {
     int processor_type = (int)exec_status.processor_type;
-    status.act_energy +=
-        exec_status.act_count * dram_powers[processor_type].kACT_energy_j_;
-    status.read_energy +=
-        exec_status.read_count * dram_powers[processor_type].kREAD_energy_j_;
-    status.write_energy +=
-        exec_status.write_count * dram_powers[processor_type].kWRITE_energy_j_;
 
-    status.all_act_energy += exec_status.all_act_count *
-                             dram_powers[processor_type].kALL_ACT_energy_j_;
-    status.all_read_energy += exec_status.all_read_count *
-                              dram_powers[processor_type].kALL_READ_energy_j_;
-    status.all_write_energy += exec_status.all_write_count *
-                               dram_powers[processor_type].kALL_WRITE_energy_j_;
+    float L1_ENERGY = 0.16 * 0.001; // (nJ/bit)
+    float L2_ENERGY = 0.3 * 0.001; // (nJ/bit)
+
+    // device memory energy
+    float HBM_READ_ENERGY = 4.2 * 0.001;  // HBM3 (nJ/bit)
+
+    // data copying energy from CPU-memory/SSD to HBM
+    YAML::Node config = YAML::LoadFile("config.yaml");
+    float OFFLOAD_READ_ENERGY = 0.0;
+    if(config["system"]["offload"]["offload_expert_to_cpu_memory"].as<bool>()){
+      OFFLOAD_READ_ENERGY = 9.2 * 0.001;  // DDR5 read (nJ/bit)
+      OFFLOAD_READ_ENERGY += HBM_READ_ENERGY;  // and HBM write
+    } else if(config["system"]["offload"]["offload_expert_to_ssd"].as<bool>()){
+      OFFLOAD_READ_ENERGY = 102.4 * 0.001;  // M.2 NVMe SSD read (nJ/bit)
+      float scale = config["system"]["offload"]["ssd_energy_scale"].as<float>();
+      OFFLOAD_READ_ENERGY *= scale;
+      OFFLOAD_READ_ENERGY += HBM_READ_ENERGY;  // and HBM write
+    }
 
+    // memory/storage access energy
+    status.read_energy +=
+        exec_status.read_count * 8 * HBM_READ_ENERGY; // 8 for Byte->bit
+    status.moe_hbm_read_energy +=
+        exec_status.moe_hbm_read_count * 8 * HBM_READ_ENERGY;
+    status.moe_offload_read_energy +=
+        exec_status.moe_offload_read_count * 8 * OFFLOAD_READ_ENERGY;
+
+    // GPU-compute energy
     status.mac_energy +=
         exec_status.flops * dram_powers[processor_type].kMAC_energy_j_;
-    ;  // 2flops per operation, energy per operation, pJ to nJ
+    status.cache_energy += (exec_status.l1_count * 8 * L1_ENERGY
+        + exec_status.l2_count * 8 * L2_ENERGY);
   }
 
-  // if (!exec_status.parallel_execution) {
-  //   // status.device_time = std::max(status.device_time,
-  //   //                               std::max(status.low_time,
-  //   //                               status.high_time));
-  //   // status.low_time = status.device_time;
-  //   // status.high_time = status.device_time;
-  //   status.parallel_execution = false;
-  // } else {
-  //   status.parallel_execution = true;
-  // }
-
-  // if (exec_status.processor_type == ProcessorType::LOGIC ||
-  //     exec_status.processor_type == ProcessorType::PIM) {
-  //   status.low_time += exec_status.total_duration;
-  //   status.device_time = status.low_time;
-  // } else if (exec_status.processor_type == ProcessorType::GPU) {
-  //   status.high_time += exec_status.total_duration;
-  //   status.device_time = status.high_time;
-  // } else {
-  //   status.low_time = status.device_time;
-  //   status.high_time = status.device_time;
-  // }
-
   status.compute_util = exec_status.compute_util;
   status.memory_util = exec_status.memory_util;
   status.processor_type = exec_status.processor_type;
diff --git src/module/status.h src/module/status.h
index 21d6b2d..9aa7fc9 100644
--- src/module/status.h
+++ src/module/status.h
@@ -38,6 +38,11 @@ struct ExecStatus {
   counter_t all_write_count = 0;
   counter_t ref_count = 0;
 
+  counter_t l1_count = 0;
+  counter_t l2_count = 0;
+  counter_t moe_hbm_read_count = 0;
+  counter_t moe_offload_read_count = 0; 
+
   bool parallel_execution = false;
 
   ExecStatus& operator+=(const ExecStatus& rhs) {
@@ -54,6 +59,11 @@ struct ExecStatus {
     all_write_count += rhs.all_write_count;
     ref_count += rhs.ref_count;
 
+    l1_count += rhs.l1_count;
+    l2_count += rhs.l2_count;
+    moe_hbm_read_count += rhs.moe_hbm_read_count;
+    moe_offload_read_count += rhs.moe_offload_read_count;
+
     return *this;
   }
 };
@@ -95,7 +105,11 @@ class StatusBoard {
   energy_nJ all_read_energy_load;
   energy_nJ all_write_energy_load;
 
+  energy_nJ moe_hbm_read_energy;
+  energy_nJ moe_offload_read_energy;
+
   energy_nJ mac_energy;
+  energy_nJ cache_energy;
 
   util compute_util;
   util memory_util;
diff --git src/module/timeboard.cpp src/module/timeboard.cpp
index 2b78b70..eea8cbb 100644
--- src/module/timeboard.cpp
+++ src/module/timeboard.cpp
@@ -54,7 +54,7 @@ void TimeStamp::set_status(const StatusBoard& output_status) {
     status.output_tensor_shape = output_status.tensor->shape;
   }
 
-  status.act_energy = output_status.act_energy - status.act_energy; // get layer's energy
+  status.act_energy = output_status.act_energy - status.act_energy;
   status.read_energy = output_status.read_energy - status.read_energy;
   status.write_energy = output_status.write_energy - status.write_energy;
 
@@ -62,7 +62,11 @@ void TimeStamp::set_status(const StatusBoard& output_status) {
   status.all_read_energy = output_status.all_read_energy - status.all_read_energy;
   status.all_write_energy = output_status.all_write_energy - status.all_write_energy;
   
+  status.moe_hbm_read_energy = output_status.moe_hbm_read_energy - status.moe_hbm_read_energy;
+  status.moe_offload_read_energy = output_status.moe_offload_read_energy - status.moe_offload_read_energy;
+
   status.mac_energy = output_status.mac_energy - status.mac_energy;
+  status.cache_energy = output_status.cache_energy - status.cache_energy;
 
   status.memory_util = output_status.memory_util;
   status.compute_util = output_status.compute_util;
@@ -94,8 +98,8 @@ void TimeStamp::print() {
     std::cout << name;
     print_time();
     print_tensor();
-    print_energy();
-    print_util();
+    //print_energy();
+    //print_util();
     std::cout << std::endl;
   }
 
@@ -119,7 +123,7 @@ void TimeStamp::exportGantt(std::string filepath, int device_id) {
     print_time();
     print_tensor();
     // print_energy();
-    print_util();
+    //print_util();
     std::cout << std::endl;
   }
 
diff --git src/module/timeboard.h src/module/timeboard.h
index 819ec59..fcbf372 100644
--- src/module/timeboard.h
+++ src/module/timeboard.h
@@ -34,17 +34,25 @@ class TimeStamp {
   }
 
   energy_nJ getDramEnergy(){
-    return status.act_energy + status.read_energy + status.write_energy +
-    status.all_act_energy + status.all_read_energy + status.all_write_energy;
+    return status.read_energy; // includes both DRAM-read/write
   }
 
-  energy_nJ getDramEnergyForLoad(){
-    return status.act_energy_load + status.read_energy_load + status.write_energy_load +
-    status.all_act_energy_load + status.all_read_energy_load + status.all_write_energy_load;
+  energy_nJ getMoeHbmEnergy(){
+    return status.moe_hbm_read_energy;
+  }
+
+  energy_nJ getMoeOffloadEnergy(){
+    return status.moe_offload_read_energy;
   }
 
   energy_nJ getCompEnergy(){
-    return status.mac_energy;
+    return status.mac_energy + status.cache_energy;  // GPU-compute energy
+  }
+
+  // not used
+  energy_nJ getDramEnergyForLoad(){
+    return status.act_energy_load + status.read_energy_load + status.write_energy_load +
+    status.all_act_energy_load + status.all_read_energy_load + status.all_write_energy_load;
   }
 
   double getOpb() { return status.opb; }
diff --git src/scheduler/CMakeLists.txt src/scheduler/CMakeLists.txt
index dcfb59d..f8c8f2a 100644
--- src/scheduler/CMakeLists.txt
+++ src/scheduler/CMakeLists.txt
@@ -5,4 +5,16 @@ target_sources(
 
     scheduler.cpp
     sequence.cpp
-)
\ No newline at end of file
+)
+
+target_link_libraries(scheduler PRIVATE hardware)
+# add_executable(scheduler-test test.cpp)
+
+# target_link_libraries(
+#     scheduler-test
+#     PRIVATE llm_system
+# )
+
+# set_target_properties(scheduler-test PROPERTIES
+#     RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}
+# )
diff --git src/scheduler/scheduler.cpp src/scheduler/scheduler.cpp
index 9c560f3..577732b 100644
--- src/scheduler/scheduler.cpp
+++ src/scheduler/scheduler.cpp
@@ -56,10 +56,10 @@ void Scheduler::pushDummySeq(int input_len, int output_len) {
     delta -= 1;
   }
 
-  // to give sequence some randomness, you can insert delat value in to length by uncommenting below
-  // delta = 0;
-  // input_len = input_len - delta;
-  // output_len = output_len + delta;
+  // to give sequence some randomness, you can insert non-zero delat value in to length by uncommenting below
+  delta = 0;
+  input_len = input_len - delta;
+  output_len = output_len + delta;
 
   if (output_len == 0) {
     return;
diff --git util/gantt.ipynb util/gantt.ipynb
new file mode 100644
index 0000000..ca54ee4
--- /dev/null
+++ util/gantt.ipynb
@@ -0,0 +1,230 @@
+{
+ "cells": [
+  {
+   "cell_type": "code",
+   "execution_count": 1,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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",
+      "text/plain": [
+       "<Figure size 2000x800 with 1 Axes>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "import matplotlib.pyplot as plt\n",
+    "import re\n",
+    "import os\n",
+    "from collections import defaultdict\n",
+    "\n",
+    "\n",
+    "# Define input log files for each device\n",
+    "\n",
+    "gantt_directory = \"../build/gantt\"\n",
+    "\n",
+    "# Define allowed operations (including wildcard match for expert_FFN)\n",
+    "allowed_operations = {\n",
+    "    \"attn_q_down_proj\",\n",
+    "    \"attn_kv_down_proj\",\n",
+    "    \"attn_kr_proj\",\n",
+    "    \"attn_q_up_proj\",\n",
+    "    \"attn_qr_proj\",\n",
+    "    \"attn_kv_up_proj\",\n",
+    "    \"attn_qkv_proj\",\n",
+    "    \"AttentionSum\",\n",
+    "    \"AttentionGen\",\n",
+    "    \"gate_fn\",\n",
+    "    \"moe_scatter\",\n",
+    "    \"attn_o_proj\",\n",
+    "    \"all_reduce\",\n",
+    "    \"gate_proj\",\n",
+    "    \"activation\",\n",
+    "    \"up_proj\",\n",
+    "    \"down_proj\",\n",
+    "    \"moe_gather\",\n",
+    "    \"moe_all_reduce\"\n",
+    "}\n",
+    "\n",
+    "# Store parsed tasks per device\n",
+    "device_data = defaultdict(list)\n",
+    "\n",
+    "def parse_log_file(file_path, device_name):\n",
+    "    with open(file_path, 'r') as f:\n",
+    "        lines = f.readlines()\n",
+    "\n",
+    "    parsed_tasks = []\n",
+    "    base_indent = None\n",
+    "    skip_indent = None\n",
+    "\n",
+    "    for i, line in enumerate(lines):\n",
+    "        line = line.replace('\\t', '    ')  # Convert tabs to spaces\n",
+    "        indent_level = len(line) - len(line.lstrip(' '))\n",
+    "\n",
+    "        if base_indent is None:\n",
+    "            base_indent = indent_level\n",
+    "\n",
+    "        relative_indent = indent_level - base_indent\n",
+    "        stripped_line = line.strip()\n",
+    "\n",
+    "        match = re.match(r'^(.*?)\\|\\s*([\\d.]+)us\\s*\\|\\s*([\\d.]+)\\s*-\\s*([\\d.]+)', stripped_line)\n",
+    "        if not match:\n",
+    "            continue\n",
+    "\n",
+    "        operation = match.group(1).strip()\n",
+    "        duration = float(match.group(2))\n",
+    "        start_time = float(match.group(3))\n",
+    "        end_time = float(match.group(4))\n",
+    "        \n",
+    "        if duration <= 0:\n",
+    "            continue\n",
+    "\n",
+    "        if operation == \"Linear\":\n",
+    "            continue\n",
+    "        \n",
+    "        if operation not in allowed_operations and \"expert_FFN\" not in operation:\n",
+    "            continue\n",
+    "\n",
+    "        if skip_indent is not None:\n",
+    "            if relative_indent > skip_indent:\n",
+    "                continue\n",
+    "            else:\n",
+    "                skip_indent = None\n",
+    "                \n",
+    "        if \"expert_FFN\" in operation:\n",
+    "            skip_indent = relative_indent\n",
+    "\n",
+    "        task = {\n",
+    "            \"operation\": operation,\n",
+    "            \"start_time\": start_time,\n",
+    "            \"end_time\": end_time,\n",
+    "            \"duration\": duration,\n",
+    "            \"indent\": relative_indent\n",
+    "        }\n",
+    "        parsed_tasks.append(task)\n",
+    "\n",
+    "    \n",
+    "    #merge experts\n",
+    "    first_expert = True\n",
+    "    expert_task = None\n",
+    "    end_time = 0\n",
+    "    for idx, task in enumerate(parsed_tasks):\n",
+    "        if \"expert_FFN\" in task[\"operation\"]:\n",
+    "            if first_expert:\n",
+    "                expert_task = {\n",
+    "                    \"operation\": \"MoE\",\n",
+    "                    \"start_time\": task[\"start_time\"],\n",
+    "                    \"end_time\": task[\"end_time\"],\n",
+    "                    \"duration\": task[\"duration\"],\n",
+    "                    \"indent\": task[\"indent\"]\n",
+    "                }\n",
+    "                first_expert = False\n",
+    "            else:\n",
+    "                end_time = task[\"end_time\"]\n",
+    "        elif not first_expert:\n",
+    "            \n",
+    "            expert_task[\"end_time\"] = end_time\n",
+    "            expert_task[\"duration\"] = end_time - expert_task[\"start_time\"]\n",
+    "            device_data[device_name].append(expert_task)\n",
+    "            expert_task = None\n",
+    "            first_expert = True\n",
+    "            \n",
+    "            device_data[device_name].append(task)\n",
+    "        else:\n",
+    "            device_data[device_name].append(task)\n",
+    "            \n",
+    "limit_num_device = 4\n",
+    "current_device_idx = 0\n",
+    "for filename in os.listdir(gantt_directory):\n",
+    "    current_device_idx = current_device_idx +1\n",
+    "    if current_device_idx == limit_num_device:\n",
+    "        break\n",
+    "    file_path = os.path.join(gantt_directory, filename)\n",
+    "    if os.path.isfile(file_path):\n",
+    "        device_name = os.path.splitext(filename)[0]\n",
+    "        \n",
+    "        parse_log_file(file_path, device_name)\n",
+    "\n",
+    "\n",
+    "# Sort device names for proper ordering (device_0, device_1, ...)\n",
+    "sorted_devices = sorted(device_data.keys(), key=lambda x: int(re.search(r'\\d+', x).group()))\n",
+    "\n",
+    "# Plot Gantt Chart for all devices (each device has separate y position with extra spacing)\n",
+    "fig, ax = plt.subplots(figsize=(20, 8))\n",
+    "\n",
+    "colors = [\n",
+    "    'tab:blue', 'tab:orange', 'tab:green', 'tab:red',\n",
+    "    'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray',\n",
+    "    'tab:olive' #, 'tab:cyan'\n",
+    "]\n",
+    "color_map = {}\n",
+    "\n",
+    "device_spacing = 1.5  # Increase spacing between devices for better visibility\n",
+    "\n",
+    "for device_index, device in enumerate(sorted_devices):\n",
+    "    tasks = device_data[device]\n",
+    "    y_position = device_index * device_spacing  # Spread devices vertically\n",
+    "\n",
+    "    for task in tasks:\n",
+    "        operation = task['operation']\n",
+    "        start = task['start_time']\n",
+    "        duration = task['duration']\n",
+    "\n",
+    "        if duration <= 0:\n",
+    "            continue\n",
+    "\n",
+    "        if operation not in color_map:\n",
+    "            color_map[operation] = colors[len(color_map) % len(colors)]\n",
+    "\n",
+    "        ax.barh(y_position, duration, left=start, color=color_map[operation])\n",
+    "\n",
+    "        ax.text(start + duration / 2, y_position, operation,\n",
+    "                ha='center', va='center', fontsize=8, rotation=90)\n",
+    "\n",
+    "# Set y-axis ticks and labels to each sorted device\n",
+    "yticks = [i * device_spacing for i in range(len(sorted_devices))]\n",
+    "ax.set_yticks(yticks)\n",
+    "ax.set_yticklabels(sorted_devices)\n",
+    "\n",
+    "ax.set_xlabel(\"Time (us)\")\n",
+    "ax.set_title(\"Device-wise Gantt Chart (Filtered Operations Only, expert_FFN excluded)\")\n",
+    "\n",
+    "plt.grid(True, linestyle='--', alpha=0.6)\n",
+    "plt.tight_layout()\n",
+    "plt.show()\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  }
+ ],
+ "metadata": {
+  "kernelspec": {
+   "display_name": "CXL",
+   "language": "python",
+   "name": "python3"
+  },
+  "language_info": {
+   "codemirror_mode": {
+    "name": "ipython",
+    "version": 3
+   },
+   "file_extension": ".py",
+   "mimetype": "text/x-python",
+   "name": "python",
+   "nbconvert_exporter": "python",
+   "pygments_lexer": "ipython3",
+   "version": "3.9.18"
+  }
+ },
+ "nbformat": 4,
+ "nbformat_minor": 2
+}
